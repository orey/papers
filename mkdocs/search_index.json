{
    "docs": [
        {
            "location": "/",
            "text": "Olivier Rey's Papers\n\n\nPapers\n\n\n\n\nThe Five Levels of Conceptual Maturity for IT Teams\n\n\nThe V2 Vortex\n - New version November 2017\n\n\nThe Various Stages of Digital Transformation\n\n\n\n\nRecent\n\n\n\n\nConsiderations about Rest and Web Services\n\n\nGraphQL And Classic Web Services\n\n\n\n\nMore to come soon...\n\n\n\n\nSite licensed under the terms of the \nGNU FDL V3\n.",
            "title": "Home"
        },
        {
            "location": "/#olivier-reys-papers",
            "text": "",
            "title": "Olivier Rey's Papers"
        },
        {
            "location": "/#papers",
            "text": "The Five Levels of Conceptual Maturity for IT Teams  The V2 Vortex  - New version November 2017  The Various Stages of Digital Transformation",
            "title": "Papers"
        },
        {
            "location": "/#recent",
            "text": "Considerations about Rest and Web Services  GraphQL And Classic Web Services   More to come soon...   Site licensed under the terms of the  GNU FDL V3 .",
            "title": "Recent"
        },
        {
            "location": "/articles/five-levels/",
            "text": "The Five Levels of Conceptual Maturity for IT Teams\n\n\n\n\nIT is a strange activity because it is an industry that has troubles getting out of a craft industry approach. Constantly moving to new exciting technologies and constantly influenced by massive technology centered marketing, IT people stay durably too technical and have recurring troubles to achieve IT projects.\n\n\nHistory of the Model\n\n\nYears ago, I was consulting a CEO and was trying to explain to him why his teams were failing big IT projects. A lot of executives today are digitally aware in their daily business practices but, when it comes to organizing new IT projects inside their company, they frequently discover that there is a huge gap between what they wanted to achieve and what is really done. Money is not the problem, and many big companies invest massive amounts of money in IT projects without getting the most of it. The fact is, when you invest too much in an IT project, your chances of success decrease drastically.\n\n\nI came to the CEO with this maturity model. I don't proclaim it to be perfect, far from it: it's just a representation of the problem. But the model is quite useful to explain and diagnose the capability of teams to perform IT projects and the levels of complexity attached to those projects.\n\n\nA Model of Conceptual Maturity\n\n\nIT is an abstract discipline manipulating abstract concepts. In IT, it is always possible to \"make the code work\", even if the cost and the delay to do that are unacceptable.\n\n\nThe problem of IT is that you can pay the price of bad choices years (even decades) after you did the investment - and you pay each year, possibly more and more year after year for the exact same modification. Bad IT choices imply increasing the technical debt and jeopardize the capability of achieving future projects (at least you will continuously increase their cost). In that sense, IT is quite near from traditional industry.\n\n\nThe fact is, in tradition industry, people are specialized in jobs: not every automotive worker will design the brakes or the engine. In IT, the situation is blurrier. Most IT people don't know what they are capable of doing, because specialization of jobs has not yet reached an industry agreement, and because it seems always possible to \"make the code work\" (despite of any consideration about quality/evolutions/money/timing/etc.).\n\n\nHowever, in IT, there are several levels of concerns that require different skills and different conceptual approaches. Over my 20 years in the IT business, I identified five levels in this conceptual world of software systems creation. Generally, in their professional life, people climb the rungs of the ladder in the same order, which makes the model a \"kind of\" maturity model.\n\n\nThe Levels of Conceptual Maturity\n\n\nI will describe briefly the most important aspects of the various levels. I don't pretend to be exhaustive and I use more detailed materials and samples when I play the consultant for executives.\n\n\nLevel 1: Development\n\n\nMost IT people know how to code, how to produce programs. For that purpose, IT people use programming languages (like Java, Cobol, C++, C, etc.). Behind the coding level, we can add the mastering of the development environment: development standards, debugging, versionning and branching management, build management, testing management, etc.. All those skills are crucial to good developments. Considering all that, if we rated people from 0 to 1 on this topic, I would say that most developers in the market are between 0.5 and 0.8 in development.\n\n\nLevel 2: Design\n\n\nIT uses object orientation programming (OOP) commonly for more than two decades. However, OOP is conceptually complicated. To be explained to the non-IT people, design is the activity of building a set of programs structure and knowing where to put the code inside the program structure. Depending on how the programs are internally structured, you will face more or less troubles in the future maintenance and evolutions. Design activity uses lots of design patterns (level 2) which are generally badly mastered by IT teams because IT teams think too often at the \"code level \" (level 1). With years, and with Agile methodologies, IT people were pushed back to level 1. Design is complicated. IT people have to think before they code (very hard for developers); they have to forecast what will be the maintenance. Few people are really capable of good designs.\n\n\nLevel 3: Application Architecture\n\n\nArchitecting an application today is not easy, because they are a lot of trendy \"off-the-shelf\" components, a lot of competing \"sexy\" technologies. The hype around technologies is extreme and IT people can change frameworks very quickly to follow the last trend. Architecting an application is the skill to choose what components (reusable or not) will be used into an application. It enables the application to be developed correctly by several people, to isolate \"modules\", to manage customizable dependencies inside the IT systems. This is not an easy job. The objective is not just to realize software but to be able to maintain them on the long run and whatever their functional perimeter (that can become quite large with years). In order to build good application architecture, you have to think about \"component borders\" and be able to estimate the pros and cons of using a trendy component in the market. That's quite a hard job.\n\n\nLevel 4: IT Architecture\n\n\nIT is everywhere and companies, even SMEs, have quite quickly a lot of IT systems. Being able to make those systems communicate together without being too hardly coupled, being able to make the IT systems as a whole evolving in a reasonable timing for reasonable costs, being able to communicate with the outside of the company in a safe manner, all that is part of IT architecture. In IT architecture, you also manage the positioning of features inside softwares: when a function is implemented in the wrong software, you can loose millions of dollars. In IT architecture problems are also the integration problems: a good IT architecture helps decreasing the QA costs and improving the TTM of the projects. Good IT architects are very rare in the market. It seems a pity because, for large companies, IT systems are one of the biggest problems, for instance in M&A phases. To add to the complexity, the IT market sells a lot of \"off-the-shelf solutions\" in that area, solutions that often bring confusion in IT systems, when put in place by non level 4 people.\n\n\nLevel 5: Enterprise Architecture\n\n\nThe term \"Enterprise Architecture\" (EA) is at the center of a lot of discussions because the industry is not clear about what an enterprise architect is supposed to do. I will provide my own pragmatic vision (which can be challenged). An enterprise architect must primarily ensure that company strategic intentions are concretely declined into the IT systems through auditable projects and programs. Secondly he/she must ensure that the changes brought to the softwares can be managed by end users, that software changes are not destroying efficient business processes but improving them. This is quite a work that requires a lot of IT skills but also a good business understanding and enhanced communication skills.\n\n\nThe Usual Maturity Path\n\n\nOne common path for IT-related designers or architects is to go from level 1 to level 5. Each level is requiring, following my experience, in average, at least 2 years of real life practice. Most often, when you work at a certain level, you have to consider also the impacts of what you are doing in all other lowest levels (that's why enterprise architects that never coded often make me uncomfortable, because they cannot imagine the consequences of their decisions).\n\n\nMost people will find happiness in a certain layer and will excel in that layer because it fits their mind and taste and it is perfectly adapted to the level of complexity they like to manage. Without a proper project experience and without training and/or coaching, it is quite hard for people to change levels without having to work for months in real life at the concerned level.\n\n\nA Maturity Model Related to Complexity\n\n\nThe model describes the capability to solve IT problems at the proper level, to manage the complexity of the IT problem with the proper approach. For instance, you can take a level 4 problem with a level 1 approach but you have 70% of chances of never solving the problem completely and 95% of chances to spend millions of dollars fixing \"things that don't work\". The \"things that don't work\" were created by people because of their problem solving approach (level 1 instead of level 4). Using the proper approach would never have generated those costs and troubles.\n\n\nWhat is the Required Level of Maturity for my Project?\n\n\nRegarding this maturity model, executives should appreciate the level of maturity of their internal IT organization. They are then able to ask themselves the question: what's the adequate level of maturity required to achieve the project I want? Or, we can ask the question the other way round: what projects can I safely achieve considering the maturity of my internal IT organization? Those questions are crucial.\n\n\nWhen I look at failed projects, I often see people having a maturity level between 1 and 3 trying to master a multi-million dollars IT projects requiring a level 5. Integrating external consultants can do the job but not in all cases. If your business is complex and quite specific, it is preferable to bet on internal skills in the long term for IT people to understand your business in depth. Moreover, who chooses the casting of your external consultants? Your internal IT organization, and few organizations are able to hire consultants that know more than they do.\n\n\nThis is exactly what happened when big companies got out of the mainframe. Mainframe teams were mostly level 1, some of them were level 2 (level 3 is not relevant in old mainframe application design and level 4 is often just about producing files for the outside or consuming them). Giving such a team an IT program with distributed applications in various technologies to build is doomed to failure, whatever the money and whatever the quality and dedication of people. For sure, with time and the appropriate training, most people can climb rungs in the ladder. But without time and training, people will keep on reproducing failures. Suppose the management is not aware of the core problem and people get fired for project failure, you can end up taking big outsourcing decisions based on a situation that nobody really understood.\n\n\nIT Industry Should Target Maturity\n\n\nBecause it is only \"soft\"-ware and not hardware, the IT industry seems to like the level 1 so much that every serious attempt to raise to level 2 or 3 is pushed back to level 1 by the IT providers. IT providers love to focus their customers on the \"code level\": because they cannot see the bigger picture and buy their products even if they don't need them.\n\n\nIf the IT industry created models for IT project management (like PMP), the increasing complexity of existing systems and new projects require the appropriate level of maturity in the project designers/architects teams.\n\n\nThe problem today, in most companies, is not only to \"code\" nor only to \"manage projects\": it is to also to maintain the running systems and make the huge amount of legacy applications evolve without risks and conforming to the company strategic intentions. Most IT systems are a pile of geological layers of outdated technological hypes. The code of today becomes the legacy of tomorrow. If some industries (like aviation) forecast their investments for decades, IT should begin to do the same, to work on the adequate long-term skills that are required to ensure durable and quality IT services.\n\n\nIT industry should target maturity and maturity comes with the identification of our level of mastering of IT complexity.",
            "title": "The Five Levels of Conceptual Maturity for IT Teams"
        },
        {
            "location": "/articles/five-levels/#the-five-levels-of-conceptual-maturity-for-it-teams",
            "text": "IT is a strange activity because it is an industry that has troubles getting out of a craft industry approach. Constantly moving to new exciting technologies and constantly influenced by massive technology centered marketing, IT people stay durably too technical and have recurring troubles to achieve IT projects.",
            "title": "The Five Levels of Conceptual Maturity for IT Teams"
        },
        {
            "location": "/articles/five-levels/#history-of-the-model",
            "text": "Years ago, I was consulting a CEO and was trying to explain to him why his teams were failing big IT projects. A lot of executives today are digitally aware in their daily business practices but, when it comes to organizing new IT projects inside their company, they frequently discover that there is a huge gap between what they wanted to achieve and what is really done. Money is not the problem, and many big companies invest massive amounts of money in IT projects without getting the most of it. The fact is, when you invest too much in an IT project, your chances of success decrease drastically.  I came to the CEO with this maturity model. I don't proclaim it to be perfect, far from it: it's just a representation of the problem. But the model is quite useful to explain and diagnose the capability of teams to perform IT projects and the levels of complexity attached to those projects.",
            "title": "History of the Model"
        },
        {
            "location": "/articles/five-levels/#a-model-of-conceptual-maturity",
            "text": "IT is an abstract discipline manipulating abstract concepts. In IT, it is always possible to \"make the code work\", even if the cost and the delay to do that are unacceptable.  The problem of IT is that you can pay the price of bad choices years (even decades) after you did the investment - and you pay each year, possibly more and more year after year for the exact same modification. Bad IT choices imply increasing the technical debt and jeopardize the capability of achieving future projects (at least you will continuously increase their cost). In that sense, IT is quite near from traditional industry.  The fact is, in tradition industry, people are specialized in jobs: not every automotive worker will design the brakes or the engine. In IT, the situation is blurrier. Most IT people don't know what they are capable of doing, because specialization of jobs has not yet reached an industry agreement, and because it seems always possible to \"make the code work\" (despite of any consideration about quality/evolutions/money/timing/etc.).  However, in IT, there are several levels of concerns that require different skills and different conceptual approaches. Over my 20 years in the IT business, I identified five levels in this conceptual world of software systems creation. Generally, in their professional life, people climb the rungs of the ladder in the same order, which makes the model a \"kind of\" maturity model.",
            "title": "A Model of Conceptual Maturity"
        },
        {
            "location": "/articles/five-levels/#the-levels-of-conceptual-maturity",
            "text": "I will describe briefly the most important aspects of the various levels. I don't pretend to be exhaustive and I use more detailed materials and samples when I play the consultant for executives.",
            "title": "The Levels of Conceptual Maturity"
        },
        {
            "location": "/articles/five-levels/#level-1-development",
            "text": "Most IT people know how to code, how to produce programs. For that purpose, IT people use programming languages (like Java, Cobol, C++, C, etc.). Behind the coding level, we can add the mastering of the development environment: development standards, debugging, versionning and branching management, build management, testing management, etc.. All those skills are crucial to good developments. Considering all that, if we rated people from 0 to 1 on this topic, I would say that most developers in the market are between 0.5 and 0.8 in development.",
            "title": "Level 1: Development"
        },
        {
            "location": "/articles/five-levels/#level-2-design",
            "text": "IT uses object orientation programming (OOP) commonly for more than two decades. However, OOP is conceptually complicated. To be explained to the non-IT people, design is the activity of building a set of programs structure and knowing where to put the code inside the program structure. Depending on how the programs are internally structured, you will face more or less troubles in the future maintenance and evolutions. Design activity uses lots of design patterns (level 2) which are generally badly mastered by IT teams because IT teams think too often at the \"code level \" (level 1). With years, and with Agile methodologies, IT people were pushed back to level 1. Design is complicated. IT people have to think before they code (very hard for developers); they have to forecast what will be the maintenance. Few people are really capable of good designs.",
            "title": "Level 2: Design"
        },
        {
            "location": "/articles/five-levels/#level-3-application-architecture",
            "text": "Architecting an application today is not easy, because they are a lot of trendy \"off-the-shelf\" components, a lot of competing \"sexy\" technologies. The hype around technologies is extreme and IT people can change frameworks very quickly to follow the last trend. Architecting an application is the skill to choose what components (reusable or not) will be used into an application. It enables the application to be developed correctly by several people, to isolate \"modules\", to manage customizable dependencies inside the IT systems. This is not an easy job. The objective is not just to realize software but to be able to maintain them on the long run and whatever their functional perimeter (that can become quite large with years). In order to build good application architecture, you have to think about \"component borders\" and be able to estimate the pros and cons of using a trendy component in the market. That's quite a hard job.",
            "title": "Level 3: Application Architecture"
        },
        {
            "location": "/articles/five-levels/#level-4-it-architecture",
            "text": "IT is everywhere and companies, even SMEs, have quite quickly a lot of IT systems. Being able to make those systems communicate together without being too hardly coupled, being able to make the IT systems as a whole evolving in a reasonable timing for reasonable costs, being able to communicate with the outside of the company in a safe manner, all that is part of IT architecture. In IT architecture, you also manage the positioning of features inside softwares: when a function is implemented in the wrong software, you can loose millions of dollars. In IT architecture problems are also the integration problems: a good IT architecture helps decreasing the QA costs and improving the TTM of the projects. Good IT architects are very rare in the market. It seems a pity because, for large companies, IT systems are one of the biggest problems, for instance in M&A phases. To add to the complexity, the IT market sells a lot of \"off-the-shelf solutions\" in that area, solutions that often bring confusion in IT systems, when put in place by non level 4 people.",
            "title": "Level 4: IT Architecture"
        },
        {
            "location": "/articles/five-levels/#level-5-enterprise-architecture",
            "text": "The term \"Enterprise Architecture\" (EA) is at the center of a lot of discussions because the industry is not clear about what an enterprise architect is supposed to do. I will provide my own pragmatic vision (which can be challenged). An enterprise architect must primarily ensure that company strategic intentions are concretely declined into the IT systems through auditable projects and programs. Secondly he/she must ensure that the changes brought to the softwares can be managed by end users, that software changes are not destroying efficient business processes but improving them. This is quite a work that requires a lot of IT skills but also a good business understanding and enhanced communication skills.",
            "title": "Level 5: Enterprise Architecture"
        },
        {
            "location": "/articles/five-levels/#the-usual-maturity-path",
            "text": "One common path for IT-related designers or architects is to go from level 1 to level 5. Each level is requiring, following my experience, in average, at least 2 years of real life practice. Most often, when you work at a certain level, you have to consider also the impacts of what you are doing in all other lowest levels (that's why enterprise architects that never coded often make me uncomfortable, because they cannot imagine the consequences of their decisions).  Most people will find happiness in a certain layer and will excel in that layer because it fits their mind and taste and it is perfectly adapted to the level of complexity they like to manage. Without a proper project experience and without training and/or coaching, it is quite hard for people to change levels without having to work for months in real life at the concerned level.",
            "title": "The Usual Maturity Path"
        },
        {
            "location": "/articles/five-levels/#a-maturity-model-related-to-complexity",
            "text": "The model describes the capability to solve IT problems at the proper level, to manage the complexity of the IT problem with the proper approach. For instance, you can take a level 4 problem with a level 1 approach but you have 70% of chances of never solving the problem completely and 95% of chances to spend millions of dollars fixing \"things that don't work\". The \"things that don't work\" were created by people because of their problem solving approach (level 1 instead of level 4). Using the proper approach would never have generated those costs and troubles.",
            "title": "A Maturity Model Related to Complexity"
        },
        {
            "location": "/articles/five-levels/#what-is-the-required-level-of-maturity-for-my-project",
            "text": "Regarding this maturity model, executives should appreciate the level of maturity of their internal IT organization. They are then able to ask themselves the question: what's the adequate level of maturity required to achieve the project I want? Or, we can ask the question the other way round: what projects can I safely achieve considering the maturity of my internal IT organization? Those questions are crucial.  When I look at failed projects, I often see people having a maturity level between 1 and 3 trying to master a multi-million dollars IT projects requiring a level 5. Integrating external consultants can do the job but not in all cases. If your business is complex and quite specific, it is preferable to bet on internal skills in the long term for IT people to understand your business in depth. Moreover, who chooses the casting of your external consultants? Your internal IT organization, and few organizations are able to hire consultants that know more than they do.  This is exactly what happened when big companies got out of the mainframe. Mainframe teams were mostly level 1, some of them were level 2 (level 3 is not relevant in old mainframe application design and level 4 is often just about producing files for the outside or consuming them). Giving such a team an IT program with distributed applications in various technologies to build is doomed to failure, whatever the money and whatever the quality and dedication of people. For sure, with time and the appropriate training, most people can climb rungs in the ladder. But without time and training, people will keep on reproducing failures. Suppose the management is not aware of the core problem and people get fired for project failure, you can end up taking big outsourcing decisions based on a situation that nobody really understood.",
            "title": "What is the Required Level of Maturity for my Project?"
        },
        {
            "location": "/articles/five-levels/#it-industry-should-target-maturity",
            "text": "Because it is only \"soft\"-ware and not hardware, the IT industry seems to like the level 1 so much that every serious attempt to raise to level 2 or 3 is pushed back to level 1 by the IT providers. IT providers love to focus their customers on the \"code level\": because they cannot see the bigger picture and buy their products even if they don't need them.  If the IT industry created models for IT project management (like PMP), the increasing complexity of existing systems and new projects require the appropriate level of maturity in the project designers/architects teams.  The problem today, in most companies, is not only to \"code\" nor only to \"manage projects\": it is to also to maintain the running systems and make the huge amount of legacy applications evolve without risks and conforming to the company strategic intentions. Most IT systems are a pile of geological layers of outdated technological hypes. The code of today becomes the legacy of tomorrow. If some industries (like aviation) forecast their investments for decades, IT should begin to do the same, to work on the adequate long-term skills that are required to ensure durable and quality IT services.  IT industry should target maturity and maturity comes with the identification of our level of mastering of IT complexity.",
            "title": "IT Industry Should Target Maturity"
        },
        {
            "location": "/articles/the-v2-vortex/",
            "text": "The V2 Vortex\n\n\n\n\nA lot of companies enter, at a certain moment in their life, into the \"V2 Vortex\u201d. In this article, we're going to detail the pattern of the harsh replacement of the V1 of the application by the V2.\n\n\nThe Case of Software Companies\n\n\nFor a lot of software companies that enter into the V2 Vortex, the history is similar: the V1 of the product was created long ago, for instance in the late 90s or in the early 2000s. It was a success. The company found its market. It has customers. All is great except that in 15 years, technology changed. And so did the competition.\n\n\nIn those software companies, we can often see a lack of software culture, and often, business-oriented people surfing on a tool that developed and gathered success. The R&D team is generally composed of IT people that preferred implementing new features for customers rather than making the development process change progressively to modernize slowly but surely the product.\n\n\nThe problem looks like a pattern: after a large amount of time, even the best buildings get old when they are not maintained. With years, it becomes difficult to implement new features; evolutions are slower to much slower, and the software bugs are more and more complicated to fix [1].\n\n\nSoftware is a complex activity because when things go wrong, the more customers, the more troubles, the more time and money spent in support, the less money to invest and the less time to fix. This is the dark side of the software moon, the one that enables selling plenty of times a software that was developed once. Multipliers work both ways.\n\n\nSo, one day, the CEO has to face the facts. \"We need a V2\", he says. \u201cWe can invest, I have cash\u201d.\n\n\nThat's when the trouble begins.\n\n\nWho Really Does The Maths?\n\n\nDoes the company have the right skills? Enough people? The right money?\n\n\nAs a consultant, I see that CEOs frequently don't run the maths, or maybe their calculator is bugged, or perhaps they are too optimistic.\n\n\nNew trendy technologies make them dream their product can be rewritten in 2 years maximum (it's always 2 years). For sure, they think current technology is much more productive that the old one they keep on using. And new developers can do so many pretty web stuff. The argument I hear also a lot is that the current software was refactored several times and so its global costs is much lower that the R&D team multiplied by the age of the software  (generally around 10 to 20 years).\n\n\nBut, let's do the maths together: let's suppose the R&D development team was around 4 FTE during all those years. The gross product weight would be 60 men years (MY) for a team that worked 15 years on a product. Let's suppose 1/3 of the charge is not relevant and is corresponding to the refactorings. This leads to 40 useful MY.\n\n\nGlobally, a good developer in Europe costs around 100 k\u20ac a year (with charges). That means that the software company should have approximately 4 M\u20ac available to realize this project. This investment is generally quite risky.\n\n\n40 MY of effort is possible in two years provided you have a team of 30 people (taking ramp-ups into account). For sure, the CEO of the company (or the CTO) is used to manage only 4 R&D people. So he rarely has the (project) management skills for such a big project. Let's suppose he reduces the team and targets 10 people for 4 years, he still has to train them for them to learn the business and to understand the existing product in order to be productive.\n\n\nBut where to find the right people? In IT service companies? This could be a bad move: IT service developer usually don't know how to produce good software; but they make it fast.\n\n\nMost CEOs will choose one of those options:\n\n\n\n\nBe optimistic and start with his 4 developers using new technologies. Things will probably not progressed as expected.\n\n\nOutsource because a service company can convince the CEO that with 1M\u20ac, you can create 2M\u20ac of software. Generally, the budget is consumed but the software is not produced (and not working).\n\n\nBelieve software bandits or software magicians that will promise the CEO that with their code generation MDA approach, they can develop his V2 software in less than 2 years \u2013 provided he pays upfront.\n\n\nDevelop a new product with a limited scope, for new customers, hoping to renew the old company success while fearing about the double team and the double recurring costs. But wouldn't it be a company startup within the company?\n\n\nSell the company when it is still worth something and before it's too late.\n\n\n\n\nIf we do the maths, we can be worried.\n\n\nMost of those CEOs will potentially try successively several options and fail. We can say they entered into the \"V2 Vortex\", a place where many software company go to die. Like the elephant cemetery, they cannot go back. Soon, they'll run out of cash and be obliged to find a more radical solution.\n\n\nThe V2 Vortex in Big Companies\n\n\nIn big companies, unfortunately, things are quite similar. Perhaps the application is much bigger (like a mainframe); perhaps it was not developed over 15 years but over 30; perhaps the development team is 100 people instead of 4; but the fact is here: they definitely are into the V2 Vortex.\n\n\nDoing the maths is really frightening on their side. 30 years with 100 FTE leads to 3,000 MY of effort. Let's apply the 1/3 optimistic ratio: the application weights 2,000 MY... For sure, perhaps they don't have the cash problem. But they have tried to replace their systems 3 or 4 times, and all projects failed. Indeed, they never really understood why. This is too scary to look into it. They also tried several options and spent a lot of cash, and the amount spent is at the size of the ambition.\n\n\nThey, also, would like to get rid of the problem in 2 years, but that would mean 1,000 people in the project team... Nah, not possible.\n\n\nIT service companies are not a big help with this kind of problem. Those applications are core business ones and they are connected to dozens of systems inside and outside their company. It is quite hard to even grasp the full picture.\n\n\nThey are in the V2 Vortex, and despite the fact that they're much bigger than the small software company we spoke about, the people in charge risk their job if they fail.\n\n\nThere is No Magic\n\n\nI must say: there is no magic. I wish I could solve their problems with a lot of money but I cannot. However, there is a path - but they never like it.\n\n\n\u201cWhen you have applications with thousands of function points, you have to think about it as an asset\u201d, I say to them. Ok, it is old, they did not maintain it enough, it is bugged, they are locked in ancient design constraints and old technologies, yes, it is the same for everybody. But it is their asset.\n\n\nMost of time, they can still sell it to customers (one day, they won't), or the application is still massively used (because it is core business). It still solves business issues. It still runs everyday for tons of people that can hate it while not being able to live without it.\n\n\nSo, as they would do with an old unmaintained asset that they have to keep, they have to \nmodernize\n it. They cannot let it go. They have to do what they should have done, a long time ago. And it becomes urgent.\n\n\nRewrite by Parts and Refactor\n\n\nNew technologies brought new tools and new ways of managing legacy code. My advice is to upgrade to new tools, to new source management, to continuous build and integration, to pure local development workstations, to automated testing. The old code must absolutely be ported on recent compilers and OS.\n\n\nSurprisingly, after all the previous failures, their development process is often the same old unproductive non agile process.\n\n\nThe hardest thing to do is to rewrite the software by parts, which implies code refactoring.\n\n\nThe final users of the applications will have to cope with a hybrid product for years, with some modernized parts and some old ones, the time for the team in place to modernize it completely.\n\n\nEnterprise Architecture and Transition Architecture\n\n\nBuilding The Functional Map, the Map to Other Systems and the Transformation Steps\n\n\nModernizing a complex software must be planned through the use of enterprise architecture techniques. As the system can be big in terms of functionality and/or can be connected to a lot of other systems, getting a complete map of the functionality and interfaces seems the best starting point. Archimate is the language for the task [2].\n\n\nWith Archimate, it is possible to represent the software progressive evolutions and the impacts on the user's processes. This is particularly important because some application modifications will have a serious impact on processes whereas others won't.\n\n\nIn terms of software, it is hard to migrate the core of an old application right away. Generally, the peripheric functions will be the easiest to replace. Basically, those functions often use data that, in the database, are not lined to any other data (leaves of the tree of tables).\n\n\nBy migrating function by function, and refactoring continuously the interfaces between the new part of the system and the old part of the system, the old part will become thinner while the new part will grow.\n\n\nTransition Architecture\n\n\nTechnically, a transition architecture must be determined. The current one was supposed to be abandonned for a new one, but if we suppose the application will be composed by both technologies at the same time, it is crucial to have a generic way of moving functions from one environment to the other.\n\n\nIdentifying what process is creating the data and what processes are consuming the data are important knowledge to get in order to do a smooth migration. The migration should enable to transfer the control of data from the old system to the new one progressively.\n\n\nSoftware is Made of People\n\n\nThe HR can be a problem. Let's face it: it is not so easy to convert people that were used for years to work in a certain paradigm in another. Some people will adapt and some won't. Moreover, developing in that phase of the life of the application requires skills that are generally not present in the original team.\n\n\nRecruitment is generally getting on the critical path.\n\n\nBecause software is made of people. If people are closed, the software will be closed; if they are open, it will be open. If their people are messy, the software will be a mess; if they are structured, so will their software.\n\n\nSo, to create a great software, you need to hire great people. And, once you got them, you have to cherish them because bright people can be insecure. And you don't want to lose them.\n\n\nThis change is hard is many context:\n\n\n\n\nIn small companies, it can bring a lot of issues due to feelings towards people (generally the original CTO is part of the founders and should be replaced);\n\n\nIn big companies, the application manager, even if he took in charge for decades an application, must generally be changed in order for new motivated people to take over and do the work.\n\n\n\n\nTabula Rasa (Rewriting) Versus Transformation\n\n\nTransformation Is Not Popular\n\n\nIn the software industry, we  generally damage our software assets when maintaining them. Then comes a time when we must transform the big software.\n\n\nSoftware transformation and refactoring is not very popular, for a lot of reasons:\n\n\n\n\nIt requires a lot of work on the existing system, to really analyze what it does, how it was designed, and what are his good and bad points;\n\n\nIt requires to enter into the fucntional details of the business, which many software engineers are reluctant to do, because they are more interested by technology than by functional aspects;\n\n\nIt implies mastering two environments for a long period of time, and one of them may be very old and not very appealing on a r\u00e9sum\u00e9;\n\n\nIt implies thinking at the software and IT architecture levels (3 and 4), even in a single application, which is not possible for many people [3].\n\n\n\n\nFor all those reasons, most people and managers prefer the \"tabula rasa\" solution. But, if we do the maths, we see that it is very often not an option.\n\n\nThe Rewriting Path\n\n\nThe rewriting path is not impossible, and it is sometimes the only option. It can be a success (even if many rewriting projects fail) but the conditions are numerous for it to succeed:\n\n\n\n\nThe budget must be very big, and so must be the business case to get a ROI in a reasonable delay;\n\n\nThe rewriting project must take into account the migration from the old system to the new one, the earlier possible in the project;\n\n\nThe architecture of the V2 product should respect the semantics of the business, because may old systems do;\n\n\nThe team must be great and work closely with the old system one;\n\n\nEvolutions should be limited in the old system;\n\n\nThe architecture of the new system must enable parallel developments.\n\n\n\n\nConclusion\n\n\nAfter decades of projects and auditing, most of the companies I saw facing the V2 vortex should have considered the transformation path rather than the rewriting path.\n\n\n\n\nNotes\n\n\n[1] - We will come back on technical debt in other parts of this book.\n\n\n[2] - Archimate resources can be found here: http://www.opengroup.org/subjectareas/enterprise/archimate.\n\n\n[3] - Refer to \nThe Five Levels Of Conceptual Maturity for IT Teams\n.",
            "title": "The V2 Vortex"
        },
        {
            "location": "/articles/the-v2-vortex/#the-v2-vortex",
            "text": "A lot of companies enter, at a certain moment in their life, into the \"V2 Vortex\u201d. In this article, we're going to detail the pattern of the harsh replacement of the V1 of the application by the V2.",
            "title": "The V2 Vortex"
        },
        {
            "location": "/articles/the-v2-vortex/#the-case-of-software-companies",
            "text": "For a lot of software companies that enter into the V2 Vortex, the history is similar: the V1 of the product was created long ago, for instance in the late 90s or in the early 2000s. It was a success. The company found its market. It has customers. All is great except that in 15 years, technology changed. And so did the competition.  In those software companies, we can often see a lack of software culture, and often, business-oriented people surfing on a tool that developed and gathered success. The R&D team is generally composed of IT people that preferred implementing new features for customers rather than making the development process change progressively to modernize slowly but surely the product.  The problem looks like a pattern: after a large amount of time, even the best buildings get old when they are not maintained. With years, it becomes difficult to implement new features; evolutions are slower to much slower, and the software bugs are more and more complicated to fix [1].  Software is a complex activity because when things go wrong, the more customers, the more troubles, the more time and money spent in support, the less money to invest and the less time to fix. This is the dark side of the software moon, the one that enables selling plenty of times a software that was developed once. Multipliers work both ways.  So, one day, the CEO has to face the facts. \"We need a V2\", he says. \u201cWe can invest, I have cash\u201d.  That's when the trouble begins.",
            "title": "The Case of Software Companies"
        },
        {
            "location": "/articles/the-v2-vortex/#who-really-does-the-maths",
            "text": "Does the company have the right skills? Enough people? The right money?  As a consultant, I see that CEOs frequently don't run the maths, or maybe their calculator is bugged, or perhaps they are too optimistic.  New trendy technologies make them dream their product can be rewritten in 2 years maximum (it's always 2 years). For sure, they think current technology is much more productive that the old one they keep on using. And new developers can do so many pretty web stuff. The argument I hear also a lot is that the current software was refactored several times and so its global costs is much lower that the R&D team multiplied by the age of the software  (generally around 10 to 20 years).  But, let's do the maths together: let's suppose the R&D development team was around 4 FTE during all those years. The gross product weight would be 60 men years (MY) for a team that worked 15 years on a product. Let's suppose 1/3 of the charge is not relevant and is corresponding to the refactorings. This leads to 40 useful MY.  Globally, a good developer in Europe costs around 100 k\u20ac a year (with charges). That means that the software company should have approximately 4 M\u20ac available to realize this project. This investment is generally quite risky.  40 MY of effort is possible in two years provided you have a team of 30 people (taking ramp-ups into account). For sure, the CEO of the company (or the CTO) is used to manage only 4 R&D people. So he rarely has the (project) management skills for such a big project. Let's suppose he reduces the team and targets 10 people for 4 years, he still has to train them for them to learn the business and to understand the existing product in order to be productive.  But where to find the right people? In IT service companies? This could be a bad move: IT service developer usually don't know how to produce good software; but they make it fast.  Most CEOs will choose one of those options:   Be optimistic and start with his 4 developers using new technologies. Things will probably not progressed as expected.  Outsource because a service company can convince the CEO that with 1M\u20ac, you can create 2M\u20ac of software. Generally, the budget is consumed but the software is not produced (and not working).  Believe software bandits or software magicians that will promise the CEO that with their code generation MDA approach, they can develop his V2 software in less than 2 years \u2013 provided he pays upfront.  Develop a new product with a limited scope, for new customers, hoping to renew the old company success while fearing about the double team and the double recurring costs. But wouldn't it be a company startup within the company?  Sell the company when it is still worth something and before it's too late.   If we do the maths, we can be worried.  Most of those CEOs will potentially try successively several options and fail. We can say they entered into the \"V2 Vortex\", a place where many software company go to die. Like the elephant cemetery, they cannot go back. Soon, they'll run out of cash and be obliged to find a more radical solution.",
            "title": "Who Really Does The Maths?"
        },
        {
            "location": "/articles/the-v2-vortex/#the-v2-vortex-in-big-companies",
            "text": "In big companies, unfortunately, things are quite similar. Perhaps the application is much bigger (like a mainframe); perhaps it was not developed over 15 years but over 30; perhaps the development team is 100 people instead of 4; but the fact is here: they definitely are into the V2 Vortex.  Doing the maths is really frightening on their side. 30 years with 100 FTE leads to 3,000 MY of effort. Let's apply the 1/3 optimistic ratio: the application weights 2,000 MY... For sure, perhaps they don't have the cash problem. But they have tried to replace their systems 3 or 4 times, and all projects failed. Indeed, they never really understood why. This is too scary to look into it. They also tried several options and spent a lot of cash, and the amount spent is at the size of the ambition.  They, also, would like to get rid of the problem in 2 years, but that would mean 1,000 people in the project team... Nah, not possible.  IT service companies are not a big help with this kind of problem. Those applications are core business ones and they are connected to dozens of systems inside and outside their company. It is quite hard to even grasp the full picture.  They are in the V2 Vortex, and despite the fact that they're much bigger than the small software company we spoke about, the people in charge risk their job if they fail.",
            "title": "The V2 Vortex in Big Companies"
        },
        {
            "location": "/articles/the-v2-vortex/#there-is-no-magic",
            "text": "I must say: there is no magic. I wish I could solve their problems with a lot of money but I cannot. However, there is a path - but they never like it.  \u201cWhen you have applications with thousands of function points, you have to think about it as an asset\u201d, I say to them. Ok, it is old, they did not maintain it enough, it is bugged, they are locked in ancient design constraints and old technologies, yes, it is the same for everybody. But it is their asset.  Most of time, they can still sell it to customers (one day, they won't), or the application is still massively used (because it is core business). It still solves business issues. It still runs everyday for tons of people that can hate it while not being able to live without it.  So, as they would do with an old unmaintained asset that they have to keep, they have to  modernize  it. They cannot let it go. They have to do what they should have done, a long time ago. And it becomes urgent.",
            "title": "There is No Magic"
        },
        {
            "location": "/articles/the-v2-vortex/#rewrite-by-parts-and-refactor",
            "text": "New technologies brought new tools and new ways of managing legacy code. My advice is to upgrade to new tools, to new source management, to continuous build and integration, to pure local development workstations, to automated testing. The old code must absolutely be ported on recent compilers and OS.  Surprisingly, after all the previous failures, their development process is often the same old unproductive non agile process.  The hardest thing to do is to rewrite the software by parts, which implies code refactoring.  The final users of the applications will have to cope with a hybrid product for years, with some modernized parts and some old ones, the time for the team in place to modernize it completely.",
            "title": "Rewrite by Parts and Refactor"
        },
        {
            "location": "/articles/the-v2-vortex/#enterprise-architecture-and-transition-architecture",
            "text": "",
            "title": "Enterprise Architecture and Transition Architecture"
        },
        {
            "location": "/articles/the-v2-vortex/#building-the-functional-map-the-map-to-other-systems-and-the-transformation-steps",
            "text": "Modernizing a complex software must be planned through the use of enterprise architecture techniques. As the system can be big in terms of functionality and/or can be connected to a lot of other systems, getting a complete map of the functionality and interfaces seems the best starting point. Archimate is the language for the task [2].  With Archimate, it is possible to represent the software progressive evolutions and the impacts on the user's processes. This is particularly important because some application modifications will have a serious impact on processes whereas others won't.  In terms of software, it is hard to migrate the core of an old application right away. Generally, the peripheric functions will be the easiest to replace. Basically, those functions often use data that, in the database, are not lined to any other data (leaves of the tree of tables).  By migrating function by function, and refactoring continuously the interfaces between the new part of the system and the old part of the system, the old part will become thinner while the new part will grow.",
            "title": "Building The Functional Map, the Map to Other Systems and the Transformation Steps"
        },
        {
            "location": "/articles/the-v2-vortex/#transition-architecture",
            "text": "Technically, a transition architecture must be determined. The current one was supposed to be abandonned for a new one, but if we suppose the application will be composed by both technologies at the same time, it is crucial to have a generic way of moving functions from one environment to the other.  Identifying what process is creating the data and what processes are consuming the data are important knowledge to get in order to do a smooth migration. The migration should enable to transfer the control of data from the old system to the new one progressively.",
            "title": "Transition Architecture"
        },
        {
            "location": "/articles/the-v2-vortex/#software-is-made-of-people",
            "text": "The HR can be a problem. Let's face it: it is not so easy to convert people that were used for years to work in a certain paradigm in another. Some people will adapt and some won't. Moreover, developing in that phase of the life of the application requires skills that are generally not present in the original team.  Recruitment is generally getting on the critical path.  Because software is made of people. If people are closed, the software will be closed; if they are open, it will be open. If their people are messy, the software will be a mess; if they are structured, so will their software.  So, to create a great software, you need to hire great people. And, once you got them, you have to cherish them because bright people can be insecure. And you don't want to lose them.  This change is hard is many context:   In small companies, it can bring a lot of issues due to feelings towards people (generally the original CTO is part of the founders and should be replaced);  In big companies, the application manager, even if he took in charge for decades an application, must generally be changed in order for new motivated people to take over and do the work.",
            "title": "Software is Made of People"
        },
        {
            "location": "/articles/the-v2-vortex/#tabula-rasa-40rewriting41-versus-transformation",
            "text": "",
            "title": "Tabula Rasa (Rewriting) Versus Transformation"
        },
        {
            "location": "/articles/the-v2-vortex/#transformation-is-not-popular",
            "text": "In the software industry, we  generally damage our software assets when maintaining them. Then comes a time when we must transform the big software.  Software transformation and refactoring is not very popular, for a lot of reasons:   It requires a lot of work on the existing system, to really analyze what it does, how it was designed, and what are his good and bad points;  It requires to enter into the fucntional details of the business, which many software engineers are reluctant to do, because they are more interested by technology than by functional aspects;  It implies mastering two environments for a long period of time, and one of them may be very old and not very appealing on a r\u00e9sum\u00e9;  It implies thinking at the software and IT architecture levels (3 and 4), even in a single application, which is not possible for many people [3].   For all those reasons, most people and managers prefer the \"tabula rasa\" solution. But, if we do the maths, we see that it is very often not an option.",
            "title": "Transformation Is Not Popular"
        },
        {
            "location": "/articles/the-v2-vortex/#the-rewriting-path",
            "text": "The rewriting path is not impossible, and it is sometimes the only option. It can be a success (even if many rewriting projects fail) but the conditions are numerous for it to succeed:   The budget must be very big, and so must be the business case to get a ROI in a reasonable delay;  The rewriting project must take into account the migration from the old system to the new one, the earlier possible in the project;  The architecture of the V2 product should respect the semantics of the business, because may old systems do;  The team must be great and work closely with the old system one;  Evolutions should be limited in the old system;  The architecture of the new system must enable parallel developments.",
            "title": "The Rewriting Path"
        },
        {
            "location": "/articles/the-v2-vortex/#conclusion",
            "text": "After decades of projects and auditing, most of the companies I saw facing the V2 vortex should have considered the transformation path rather than the rewriting path.",
            "title": "Conclusion"
        },
        {
            "location": "/articles/the-v2-vortex/#notes",
            "text": "[1] - We will come back on technical debt in other parts of this book.  [2] - Archimate resources can be found here: http://www.opengroup.org/subjectareas/enterprise/archimate.  [3] - Refer to  The Five Levels Of Conceptual Maturity for IT Teams .",
            "title": "Notes"
        },
        {
            "location": "/articles/various-stages/",
            "text": "The Various Stages of Digital Transformation\n\n\n\n\nPresentation done in Airbus Helicopters for the PMI France Chapter.\n\n\nClick to see the \nPrezi presentation\n.",
            "title": "The Various Stages of Digital Transformation"
        },
        {
            "location": "/articles/various-stages/#the-various-stages-of-digital-transformation",
            "text": "Presentation done in Airbus Helicopters for the PMI France Chapter.  Click to see the  Prezi presentation .",
            "title": "The Various Stages of Digital Transformation"
        },
        {
            "location": "/articles/about-rest/",
            "text": "Considerations About Rest And Web Services\n\n\n \n\n\nIt's been a very long time since I've been explaining this to a lot of people and maybe today I should try to put the full explanation on paper.\n\n\nA Bit Of History\n\n\nWhen the REST concept was published, sometimes around 2001, I was in a middleware team in a big software company. In that team, we were accustomed to use business services for more than a decade.\n\n\nIndeed, the term \"web service\" or \"SOA\" was not existing at the time. But, in many business domains such as airline commercial business, banking, insurance, etc., many people knew what was RPC and were using it extensively to communicate between systems.\n\n\nRPC means Remote Procedure Call. RPC concept was introduced to me with the DCE (\nDistributed Computing Environment\n). DCE was a very powerful standard that were never completely used, as far as I know, but it explained in great details the basis of interoperability between systems. For sure, the standard was only making a synthesis of ideas that were much older.\n\n\nWhat Is RPC?\n\n\nInteroperability Contract\n\n\nFundamentally RPC is, like it is said in its name, a remote procedure call.\n\n\nTo understand the concept, let's imagine 2 programs that want to communicate, first program being \nA\n and second being \nB\n. \nB\n will publish an API in order to be called. In most procedural programming languages (like C), if the two programs are located on the same machine, \nA\n can call the API of \nB\n (see the top of Figure 1).\n\n\nFigure 1: Local Procedure Call and Remote Procedure Call\n\n\nThe idea of interoperability in RPC is that, if \nB\n is located in a remote machine (or a remote process), \nB\n should not change when invoked by \nA\n. On the other side, \nA\n should not change in its invocation of \nB\n interface. So \nA\n will call a \nB\n interface locally to its system, this interface hiding a client stub that will wrap/serialize data in a certain format to be sent on the wire; on the machine hosting \nB\n, there will be a server stub unwraping/unserializing data to call locally the \nB\n interface.\n\n\nFor sure, in order to work in a complex network, the message sent from \nA\n will have to fond is route to the machine hosting \nB\n. We have here all the elements of the client-server architecture.\n\n\nThe Notion Of \"Verb\"\n\n\nWell, RPC is a bit more than that. Because, when calling a remote procedure (or function), semantically, we call a remote system asking it to do something (the B API) with the input data that we provide (the request). We expect data back (the response).\n\n\nThis is/was called a \"verb\". \nA\n says to \nB\n: \"perform \nB-interface\n contract with my input data and give me back the contract expected output data\".\n\n\nTrends passed on many parameters:\n\n\n\n\n\n\nThe protocols used changed,\n\n\n\n\n\n\nThe addressing schemes changed,\n\n\n\n\n\n\nThe format of the data changed (from many proprietary formats or Edifact, to XML to JSON).\n\n\n\n\n\n\nBut the fact is, in most businesses proposing a certain degree of business complexity, RPC is still there. Most often, verbs are expressing an \"action\" semantic and requests are proposing data trees, so as responses.\n\n\nNo Assumptions on the Technology Used\n\n\nWe must notice that RPC does not make any assumption on the technology used by the server (the one that implements the \nB\n interface).\n\n\nThe contract is limited to a verb, and two trees of data, one for the request and one for the response. We could note the contract as being: \nA\n uses \nresponse = B-interface(request)\n .\n\n\nThe Corba Failed Attempt\n\n\nPrinciple\n\n\nIn the 90s, the objet-oriented programming (OOP) being trendy, the intention behind \nCorba\n was born. The idea was to generalize the object notions to a distributed world. The logic consequence was to imagine a client-server protocol for objects to communicate between one another.\n\n\nThe principle is simple: an object \nA\n calls locally a method \nm(...)\n on an object \nB\n. If we imagine the \nB\n instance as being remote, the idea is the same than RPC: The method should have a client and server stub.\n\n\nFigure 1: Local Method Invocation and Remote Method Invocation\n\n\nThe fact is, despite the fact that it looks like RPC, this model is radically different from it:\n\n\n\n\nIt supposes that the remote system is object oriented;\n\n\nIt supposes that the remote system is stateful (the \nB\n instance must exist for its method to be called);\n\n\nIt supposes an addressing system to find the reference of the instance of B;\n\n\nThe contract of the method is not self sufficient, indeed, conceptually \nA\n asks for: \nresponse = state(B).m(request)\n which introduce some uncertainty on the call (because it depends on B state);\n\n\nThe contract is supposing the state of B should be managed differently from the request parameters, and so, it puts a different semantic weight on B that should be a \"first-class entity\" whereas the request parameters are considered as \"second class entities\".\n\n\n\n\nThis way of thinking distributed systems is leading to consider that the network offers \"objects that can provide services on themselves\", instead of providing simple services (i.e. distributed functions).\n\n\nIn 2017's language, we could say that CORBA is proposing a \"resource-oriented architecture\".\n\n\nThe Drawbacks of the ORB Approach\n\n\nAn ORB (Object Request Broker) is a middleware that enables to manage those distributed objects, their remote invocation and the required addressing schema.\n\n\nThe main drawback of the ORB approach is that a service can be, semantically, much more complex than calling one method of one object. Indeed, if we call a method on an object, we can imagine that the object will process this method on its state, or that it will process it considering its aggregation (\"facade\" pattern).\n\n\nFor services, we do not make any assumptions of the kind. We just call a function that is supposed to do something with the inbound parameters that we provide (inbound parameters that are generally some kind of tree of objects having or not links together).\n\n\nCertainly, we can \"twist\" Corba in order to make it look like RPC: we can use objects that do not contain state and that have \"methods\" that are indeed just \"procedures\".\n\n\nBut the addressing repository will have to manage object instance addresses instead of knowing the service location.\n\n\nAn Idea That Keeps Coming Back\n\n\nThis idea keeps coming back. Rest architecture can be seen as a reformulation of Corba principles:\n\n\n\n\nResources are benefiting from an addressing scheme (URI);\n\n\nResources have a name (class name), they are identified through their instance and they publish methods;\n\n\nInvoking a service is indeed a remote method invocation.\n\n\n\n\nMoreover, a strange design option is taken in the Rest specification: the presentation layer is binded on the underlying network protocol (http) in a very hard way. Indeed, the only verbs that seem to be invokable are CRUD verbs (CRUD standing for Create Retrieve Update Delete). \n\n\nWe can also see this ORB principle applied, in a weaker way, in WSDL. When SOAP web services were really in the spirit of RPC, WSDL standard groups transactions in a way that is sometimes near to the idea of a group of methods operating on a remote object.\n\n\nWe can note also in WSDL, a very strange hard biding on the underlying protocol (the WSDL schema integrates information from several OSI layers which is bad).\n\n\nRest was those last years recently put in front of the scene due to IT marketing and the creation of \nSwagger.io\n\n\nThe Core Problem of Resource Orientation\n\n\nResource orientation, like in an ORB, is semantically a very hard restriction of RPC. RPC is a way of calling a service that will transform a graph of data in another graph of data. In the ORB approach, we call a method on a object, which is very restrictive.\n\n\nThis can work if the semantics of the business is simple. If my business objects are modeled properly by one class, then maybe an ORB can work. If the business objects I have to model need several classes interacting together in a complex manner, then using an ORB will be a real pain. When a service can transform my graph of objects into another graph of objects, an ORB will force me to \"adapt\" my semantics to a pure technical object-oriented method call.\n\n\nFor sure, this is true for Rest, as it was true for Corba. For pure Rest, it is even worse: being able to consider that only the http verbs (network layer) should be used to express the semantics of the presentation layer, the semantics of the \"functional verbs\" seems even more restrictive than Corba.\n\n\nFor social networks like Twitter or Facebook, it seems to work. I can define a Rest API providing all the services offered by the platform. For a business application, we generally cannot use Rest because the constraints (RMI with CRUD verbs) are too strong.\n\n\nAgain, like in Corba, we can cheat: we can use \"almost Rest\" and have a request body with JSON. That turns the method invocation into an almost-service. We can also include in the request a service name, but that is pretending to implement Rest and doing JSON-RPC-like behind the scene.\n\n\nConclusion\n\n\nRest is very practical for the applications which semantics is simple and can be adapted to two constraints:\n\n\n\n\nA resource oriented API (pushing for services to be RMI);\n\n\nA verb semantic limited to a variation of CRUD.\n\n\n\n\nFor other applications, like business applications [1], we believe things never changed for decades. Before SOA, RPC services were existing. They were the same stuff than JSON-RPC like transactions today.\n\n\nService orientation is a much more general way of implementing service distribution than ORB concepts. In particular, service orientation does not presuppose that the remote server sees the data as the caller  does. The service signature is an agreed contract for both the client and the server to communicate together, but each of them can restructure data as they want, in an object oriented way or not.\n\n\nSo, my advice is not to force yourself to implement a Rest API to your application because it is trendy, but to do it only if your business semantics enables it.\n\n\nSee Also\n\n\n\n\nAbout GraphQL\n\n\n\n\nNotes\n\n\n[1] - In some businesses, like the airline one (standardized by \nIATA\n), services have big requests and big responses for decades because the business requires it.",
            "title": "Considerations About Rest And Web Services"
        },
        {
            "location": "/articles/about-rest/#considerations-about-rest-and-web-services",
            "text": "It's been a very long time since I've been explaining this to a lot of people and maybe today I should try to put the full explanation on paper.",
            "title": "Considerations About Rest And Web Services"
        },
        {
            "location": "/articles/about-rest/#a-bit-of-history",
            "text": "When the REST concept was published, sometimes around 2001, I was in a middleware team in a big software company. In that team, we were accustomed to use business services for more than a decade.  Indeed, the term \"web service\" or \"SOA\" was not existing at the time. But, in many business domains such as airline commercial business, banking, insurance, etc., many people knew what was RPC and were using it extensively to communicate between systems.  RPC means Remote Procedure Call. RPC concept was introduced to me with the DCE ( Distributed Computing Environment ). DCE was a very powerful standard that were never completely used, as far as I know, but it explained in great details the basis of interoperability between systems. For sure, the standard was only making a synthesis of ideas that were much older.",
            "title": "A Bit Of History"
        },
        {
            "location": "/articles/about-rest/#what-is-rpc",
            "text": "",
            "title": "What Is RPC?"
        },
        {
            "location": "/articles/about-rest/#interoperability-contract",
            "text": "Fundamentally RPC is, like it is said in its name, a remote procedure call.  To understand the concept, let's imagine 2 programs that want to communicate, first program being  A  and second being  B .  B  will publish an API in order to be called. In most procedural programming languages (like C), if the two programs are located on the same machine,  A  can call the API of  B  (see the top of Figure 1).  Figure 1: Local Procedure Call and Remote Procedure Call  The idea of interoperability in RPC is that, if  B  is located in a remote machine (or a remote process),  B  should not change when invoked by  A . On the other side,  A  should not change in its invocation of  B  interface. So  A  will call a  B  interface locally to its system, this interface hiding a client stub that will wrap/serialize data in a certain format to be sent on the wire; on the machine hosting  B , there will be a server stub unwraping/unserializing data to call locally the  B  interface.  For sure, in order to work in a complex network, the message sent from  A  will have to fond is route to the machine hosting  B . We have here all the elements of the client-server architecture.",
            "title": "Interoperability Contract"
        },
        {
            "location": "/articles/about-rest/#the-notion-of-verb",
            "text": "Well, RPC is a bit more than that. Because, when calling a remote procedure (or function), semantically, we call a remote system asking it to do something (the B API) with the input data that we provide (the request). We expect data back (the response).  This is/was called a \"verb\".  A  says to  B : \"perform  B-interface  contract with my input data and give me back the contract expected output data\".  Trends passed on many parameters:    The protocols used changed,    The addressing schemes changed,    The format of the data changed (from many proprietary formats or Edifact, to XML to JSON).    But the fact is, in most businesses proposing a certain degree of business complexity, RPC is still there. Most often, verbs are expressing an \"action\" semantic and requests are proposing data trees, so as responses.",
            "title": "The Notion Of \"Verb\""
        },
        {
            "location": "/articles/about-rest/#no-assumptions-on-the-technology-used",
            "text": "We must notice that RPC does not make any assumption on the technology used by the server (the one that implements the  B  interface).  The contract is limited to a verb, and two trees of data, one for the request and one for the response. We could note the contract as being:  A  uses  response = B-interface(request)  .",
            "title": "No Assumptions on the Technology Used"
        },
        {
            "location": "/articles/about-rest/#the-corba-failed-attempt",
            "text": "",
            "title": "The Corba Failed Attempt"
        },
        {
            "location": "/articles/about-rest/#principle",
            "text": "In the 90s, the objet-oriented programming (OOP) being trendy, the intention behind  Corba  was born. The idea was to generalize the object notions to a distributed world. The logic consequence was to imagine a client-server protocol for objects to communicate between one another.  The principle is simple: an object  A  calls locally a method  m(...)  on an object  B . If we imagine the  B  instance as being remote, the idea is the same than RPC: The method should have a client and server stub.  Figure 1: Local Method Invocation and Remote Method Invocation  The fact is, despite the fact that it looks like RPC, this model is radically different from it:   It supposes that the remote system is object oriented;  It supposes that the remote system is stateful (the  B  instance must exist for its method to be called);  It supposes an addressing system to find the reference of the instance of B;  The contract of the method is not self sufficient, indeed, conceptually  A  asks for:  response = state(B).m(request)  which introduce some uncertainty on the call (because it depends on B state);  The contract is supposing the state of B should be managed differently from the request parameters, and so, it puts a different semantic weight on B that should be a \"first-class entity\" whereas the request parameters are considered as \"second class entities\".   This way of thinking distributed systems is leading to consider that the network offers \"objects that can provide services on themselves\", instead of providing simple services (i.e. distributed functions).  In 2017's language, we could say that CORBA is proposing a \"resource-oriented architecture\".",
            "title": "Principle"
        },
        {
            "location": "/articles/about-rest/#the-drawbacks-of-the-orb-approach",
            "text": "An ORB (Object Request Broker) is a middleware that enables to manage those distributed objects, their remote invocation and the required addressing schema.  The main drawback of the ORB approach is that a service can be, semantically, much more complex than calling one method of one object. Indeed, if we call a method on an object, we can imagine that the object will process this method on its state, or that it will process it considering its aggregation (\"facade\" pattern).  For services, we do not make any assumptions of the kind. We just call a function that is supposed to do something with the inbound parameters that we provide (inbound parameters that are generally some kind of tree of objects having or not links together).  Certainly, we can \"twist\" Corba in order to make it look like RPC: we can use objects that do not contain state and that have \"methods\" that are indeed just \"procedures\".  But the addressing repository will have to manage object instance addresses instead of knowing the service location.",
            "title": "The Drawbacks of the ORB Approach"
        },
        {
            "location": "/articles/about-rest/#an-idea-that-keeps-coming-back",
            "text": "This idea keeps coming back. Rest architecture can be seen as a reformulation of Corba principles:   Resources are benefiting from an addressing scheme (URI);  Resources have a name (class name), they are identified through their instance and they publish methods;  Invoking a service is indeed a remote method invocation.   Moreover, a strange design option is taken in the Rest specification: the presentation layer is binded on the underlying network protocol (http) in a very hard way. Indeed, the only verbs that seem to be invokable are CRUD verbs (CRUD standing for Create Retrieve Update Delete).   We can also see this ORB principle applied, in a weaker way, in WSDL. When SOAP web services were really in the spirit of RPC, WSDL standard groups transactions in a way that is sometimes near to the idea of a group of methods operating on a remote object.  We can note also in WSDL, a very strange hard biding on the underlying protocol (the WSDL schema integrates information from several OSI layers which is bad).  Rest was those last years recently put in front of the scene due to IT marketing and the creation of  Swagger.io",
            "title": "An Idea That Keeps Coming Back"
        },
        {
            "location": "/articles/about-rest/#the-core-problem-of-resource-orientation",
            "text": "Resource orientation, like in an ORB, is semantically a very hard restriction of RPC. RPC is a way of calling a service that will transform a graph of data in another graph of data. In the ORB approach, we call a method on a object, which is very restrictive.  This can work if the semantics of the business is simple. If my business objects are modeled properly by one class, then maybe an ORB can work. If the business objects I have to model need several classes interacting together in a complex manner, then using an ORB will be a real pain. When a service can transform my graph of objects into another graph of objects, an ORB will force me to \"adapt\" my semantics to a pure technical object-oriented method call.  For sure, this is true for Rest, as it was true for Corba. For pure Rest, it is even worse: being able to consider that only the http verbs (network layer) should be used to express the semantics of the presentation layer, the semantics of the \"functional verbs\" seems even more restrictive than Corba.  For social networks like Twitter or Facebook, it seems to work. I can define a Rest API providing all the services offered by the platform. For a business application, we generally cannot use Rest because the constraints (RMI with CRUD verbs) are too strong.  Again, like in Corba, we can cheat: we can use \"almost Rest\" and have a request body with JSON. That turns the method invocation into an almost-service. We can also include in the request a service name, but that is pretending to implement Rest and doing JSON-RPC-like behind the scene.",
            "title": "The Core Problem of Resource Orientation"
        },
        {
            "location": "/articles/about-rest/#conclusion",
            "text": "Rest is very practical for the applications which semantics is simple and can be adapted to two constraints:   A resource oriented API (pushing for services to be RMI);  A verb semantic limited to a variation of CRUD.   For other applications, like business applications [1], we believe things never changed for decades. Before SOA, RPC services were existing. They were the same stuff than JSON-RPC like transactions today.  Service orientation is a much more general way of implementing service distribution than ORB concepts. In particular, service orientation does not presuppose that the remote server sees the data as the caller  does. The service signature is an agreed contract for both the client and the server to communicate together, but each of them can restructure data as they want, in an object oriented way or not.  So, my advice is not to force yourself to implement a Rest API to your application because it is trendy, but to do it only if your business semantics enables it.",
            "title": "Conclusion"
        },
        {
            "location": "/articles/about-rest/#see-also",
            "text": "About GraphQL",
            "title": "See Also"
        },
        {
            "location": "/articles/about-rest/#notes",
            "text": "[1] - In some businesses, like the airline one (standardized by  IATA ), services have big requests and big responses for decades because the business requires it.",
            "title": "Notes"
        },
        {
            "location": "/articles/graphql-web-services/",
            "text": "GraphQL And Classic Web Services\n\n\n \n\n\nFacebook produced the \nGraphQL specification\n in order to be an alternative to Rest.\n\n\nWe already explained \nwhat we thought of Rest services\n.\n\n\nGraphQL appears to us as a new way to do the same thing that what the industry is doing for decades.\n\n\nThis article is an analysis of GraphQL principles, their consequences and a comparison with traditional ways of doing web services.\n\n\nThe Basics of Services\n\n\nVarious Standards, Same Spirit\n\n\nThe underlying basis of services is RPC (Remote Procedure Call, \nsee the dedicated article on the subject\n). For decades, this concept is reinvented again and again with various flavors.\n\n\nApart from RPC protocols or proprietary protocols (like the \nTuxedo\n ones), \nEdifact\n was for a long time an industry standard in some business domains like travel, banking, medical and logistics. Edifact was replaced by \nXML\n then by \nJSON\n.\n\n\nEvery of those standards aimed to provide the same functionality: exchanging structured data between two different systems, probably implemented with different programming languages and/or paradigms, and having a client system benefiting from a service provided by a server system.\n\n\nBasic Requirements For a Service Oriented Language\n\n\nThe first requirement is to have a \"verb\", meaning the name of the \"transaction\" we expect the remote system to process.\n\n\nThe second requirement is to have a type system to be able to send structured data under the form of a tree of objects. This is generally achieved by proposing basic types (like string, integer, float, etc.) and the capability of defining fields that have those properties and that can be grouped into classes. Classes will generally have to indicate what fields are required and what fields are optional. This is generally done within the context of one particular transaction.\n\n\nOnce classes are defined, they can be grouped in larger structures or other classes to define groups of object structures than define larger entities, most of the time, better suitable to express the business semantics.\n\n\nOptionally, those definitions are expressed in \"grammars\" or \"schemas\". This is the case in RPC systems, in Edifact and in XML, and this is optional for JSON (even if the \nJSON schema\n specification is very usable). Note that, even if there is no explicit schema mechanism, the fact of defining objects and structures leads to the use of implicit schemas. The difference is really what part of the code is \"validating\" that the messages are well formed:\n\n\n\n\nWith an explicit schema definition, we can generate the validation code and create a kind of serializer/unserializer layer (wrapper/unwrapper or proxy/stub layer);\n\n\nWithout an explicit schema definition, the validation code is generally hand written.\n\n\n\n\nAll those requirements are defined in the \nOSI model\n in the \"presentation layer\".\n\n\nDon't forget the protocol\n\n\nThe last requirement is to have some kind of underlying protocol to:\n\n\n\n\nSend and receive messages between systems;\n\n\nCreate a naming convention that enables to use the name of the service (or name of the \"verb\");\n\n\nFind the network configuration that enable to route the request message to the proper entry point;\n\n\nPossibly associate the request and the response with identifiers;\n\n\nPossibly include information about the sender and its security credentials.\n\n\n\n\nOnce again, the \nOSI model\n defines a certain split of responsibilities between all those requirements. Quickly explained:\n\n\n\n\nThe transport layer (layer 4) is defining the addressing scheme and primitives that enables several systems to communicate together;\n\n\nThe session layer (layer 5) enables to manage the sessions and conversations between the client and the server; this layer manages the succession of names messages that the two peers will exchange;\n\n\nThe presentation layer (layer 6) manages the content of data, their structure and their values.\n\n\n\n\nIn the current JSON world, \nJSON-RPC\n is presenting a very basic protocol that can manage the basic single statement and request response messages.\n\n\nGraphQL Answers To Service Oriented Requirements\n\n\nGraphQL seems to us as a new flavor of the same old story. It brings some interesting new stuff (and we will come back on that), but the important point is that we can implement traditional services with this language.\n\n\nThe \"Verb\"\n\n\nEach GraphQL service is an named \nhttp\n entry point. The \nget\n method will be used to access the \nquery\n part of the service and the \npost\n method will access the \nmutation\n part of it.\n\n\nThis looks like Rest but this is rather different, because the idea seems not to access to a single resource (even if it is possible), but to perform a service.\n\n\nThe Type System\n\n\nGraphQL proposes an extended type system that proposes:\n\n\n\n\nBasic types,\n\n\nClasses,\n\n\nGroups of classes,\n\n\nFragments (kind of filters on classes),\n\n\nQueries.\n\n\n\n\nWith all the available semantics, it is very easy to implement all the existing web services that are used currently in the business.\n\n\nMoreover, it is possible to have a richer interface on existing services and a more adaptative way of performing transactions.\n\n\nThis seems rather promising for mobile or chatbot accesses for instance. Those two UIs manipulate a small amount of data and may require some server services to adapt their reply to their particular use.\n\n\nFor sure, we could argue saying: with standard web services in JSON, as a client, we can always extract the subset of the response that is interesting to us. The fact is this feature goes beyond the subset approach and enables to ask the server to adapt the contents of its response to the client.\n\n\nAnd The Protocol?\n\n\nIn the Web, the protocol is \nhttp\n and the approach seems inspired by both Rest and Ajax kind of calls.\n\n\nA Promising Standard\n\n\nAn Intermediate Between JSON-RPC and Rest?\n\n\nGraphQL seems to target the intermediate path between JSON-RPC (which is a simplified SOAP) and Rest. The fact of publishing a schema enables both approaches:\n\n\n\n\nPerform elaborated services (like before);\n\n\nAccess business objects (like the Rest or CRUD approach).\n\n\n\n\nGraphQL seems to propose a way to benefit from both approaches.\n\n\nThe fact of being able to perform complex queries (with a lot of flavours) also adds intelligence to the protocol, an intelligence driven by the client.\n\n\nBut What Impact On The Server Design?\n\n\nThis seems very interesting but the problem caused by this new protocol is located on the backend part.\n\n\nSome questions must be answered:\n\n\n\n\nWhat kind of models and/or application this protocol is targeting? All applications? Applications that should be able to disclose a part of their model?\n\n\nDo we know the extend of the server impacts in terms of design?\n\n\n\n\nModel Your Business Domain As A Graph\n\n\nWe are entering here into delicate topics.\n\n\nFor those how already performed some semantic modeling with \nRDF\n, \nRDFS\n or \nOWL\n, I think you're convinced about the fact that we can model a business domain with a graph.\n\n\nFor those who already used graph databases such as \nNeo\n or \nOrient\n in Big Data use cases or fraud recognition or social applications, may be convinced that a business model can be modeled as a graph.\n\n\nBut for the vast majority of IT people, this assertion is not obvious.\n\n\nWell, we'll come back on this particular topic in future articles, so we'll take for granted that a business domain can be modeled with a graph.\n\n\nGraph Data Means Graph Database\n\n\nWhat Facebook is proposing us is a protocol that enables to query the internal structure of the graph of data.\n\n\nSo, the first consequences seem to be:\n\n\n\n\nYour data are modeled as a graph,\n\n\nYou probably use a graph database of some sort.\n\n\n\n\nConsequently, your GraphQL schema is more or less the same thing as your graph database schema (when this sort of things exists such as in \nOrientDB\n).\n\n\nOpen Heart Data Model\n\n\nThe second consequence is also double:\n\n\n\n\nYou published your graph business model to your clients;\n\n\nYou use your database model as the presentation layer.\n\n\n\n\nThis second point is very problematic. Indeed, it explains why the protocol is proposing so much complexity in some of its part: because it is supposed to be, at the same time, a protocol and a graph query language (like \nCypher\n).\n\n\nIn a certain sense, it is supposed to be the new \nSQL*Net\n or \nODBC\n but for graphs.\n\n\nAnd this is where the approach is questionable.\n\n\nThe Myth Of The Single Representation Of Reality\n\n\nThere are many myths in IT, and this one is a big one. Many people believe that there is only one way of representing the reality-theirs, indeed. And the reality is, they are wrong.\n\n\nNote that, throughout the history of science, many great scientists had the same debate as the \nintuitionist debate of the beginning of the 20th Century\n.\n\n\nAny software designer that worked sufficiently in the domain should have seen several facts:\n\n\n\n\nWhatever the efforts and the design patterns used, you cannot predict how the business will evolve;\n\n\nTwo designer confronted to the same problem will produce two different designs.\n\n\n\n\nIndeed, design is very subjective and, for a lot of reasons we won't explain here, it is not a bad thing that there is no unique model for a particular business domain, on the contrary.\n\n\nWhen Client and Server Share Everything\n\n\nGraphQL is proposing a way for the customer to be very near from its server, so near that its data model is perfectly known, right from the client.\n\n\nThis can be very useful when you work with yourself, when you develop your own client and server in a kind of symbiosis. If you are on an Internet application, you also must know that you core model will be known from the outside, which can be a problems in terms of IP and in terms of security.\n\n\nSo:\n\n\n\n\nGraphQL is OK if your API is \ninternal\n;\n\n\nGraphQL seems not OK if your API is external.\n\n\n\n\nPublishing a GraphQL API\n\n\nWell, if you want to publish a GraphQL API, you have to consider several things:\n\n\n\n\nYou will impose to your client the graph model of your business domain, and maybe it is relevant and maybe not (see next part);\n\n\nYou will disclose a graph representation of your business model, which is not the case in JSON-RPC where you only disclose the \ninteroperability formats\n; This can represent a potential threat on your IT because your software design is very often at the heart of your business value;\n\n\nYou will have to have a complex security system, which is the security system of the graph exploration, and this will not be obvious to address;\n\n\nYou will be bound to implement the protocol complexity that can open more security issues in your software.\n\n\n\n\nFor sure, there are cases where all those arguments may be irrelevant:\n\n\n\n\nYou can work in a domain where the graph model has no value or is very well known (for instance the social media);\n\n\nYou can work for non profit organizations;\n\n\nYou can have a system that will not cause any loss of money, loss of IP or loss of value if hacked.\n\n\n\n\nThe Fundamental Principle Of Interoperability\n\n\nIn complement to the \narticle on REST\n, we will explain the fundamental principle of interoperability.\n\n\nThe context of interoperability is the following:\n\n\n\n\nSystems communicate when they have some interest in doing so: interoperability is business-oriented.\n\n\nTo establish system interconnection costs money and time, this in the project phase but also in the recurring phase: to realize those investments, the client and the server generally have a good business reason.\n\n\nWhen a server is publishing an API, it cannot imagine the list of clients he will have tomorrow, nor make assumptions on their programming language or even design approach.\n\n\n\n\nIn this context, the fundamental principle of interoperability is that the client and the server contracting the exchange \nshould define the common format that is proposing the less semantic constraints possible on both ends\n.\n\n\nBecause the client and the server don't have to commit on their internal structure and functional aspects, the interchange is described with a semantic that must be the simplest possible and that can enable the client and server to define different modeling of the same \"exchanged data model\".\n\n\n\n\nThe figure above shows the core problem of interoperability:\n\n\n\n\nThe client and the server have no reason to model the interchange data the same way. Indeed, they don't. The consequence is that they must adapt their internal formats (memory or database) to the network presentation.\n\n\nThe way they store the data is their problem, and their storage representation can be even quite different from their memory representation (that's why, generally, people use a data access layer to adapt their business objects to the database).\n\n\n\n\nVery commonly, in the industry, we have 3 representations of the data: the client's, the server's and the network presentation.\n\n\nThose adaptation are painful to develop, for sure, but they are the conditions of isolation, and who says isolation says separation of concerns.\n\n\nThis is because the client and the server are different software parts that there is no need to impose more constraints.\n\n\nIn this context, we do not recommend using GraphQL for an external business domain interface, but we can recommend it if the application architecture needs a kinf of ODBC or SQL*Net directly from the UI, and in a context where the API is not made to be exposed to unknown third parties.\n\n\nNote that we did not really enter into the core details of \"how do we really build a graph-oriented business application\"? This will come in later articles.\n\n\nA Correct Intuition?\n\n\nIf we try to step back one minute, we can say that the Facebook team may have had an intuition that, with the graph-oriented modeling approach, this fundamental principle or interoperability could be declared obsolete (and consequently graphs could be in the client, in the server, in the database, with exactly the same structure).\n\n\nIn some cases, that's true: if you master both ends of the wire, that's probably one very efficient way to do it. But it looks like ODBC or SQL*Net anyway.\n\n\nOne thing is sure, the graph-oriented modeling of business domains will revolution the IT world, but perhaps not the way the Facebook team imagined it. We'll come back on that in the future.\n\n\nConclusion\n\n\nGraphQL is a very interesting attempt to propose a middle term between REST and JSON-RPC, as the following diagram is showing it.\n\n\n\n\nHowever, this diagram is very misleading because the 3 inbound protocols have many different impacts on the server design, contrary to what's presented. Indeed, both REST and GraphQL imply a very specific programming model:\n\n\n\n\nREST is imposing a hard resource orientation that is unnatural to business applications (see \nhere\n),\n\n\nGraphQL proposes a graph-oriented ODBC-like protocol that will have the tendency to tie-up strongly the client and the server.\n\n\n\n\nThe conclusion is it seems to us that, so far, only RPC enables to design and deliver reliable and secure business applications, and to do it the way you want. RPC defines a contract that can lead to many various programming paradigms, which is not the case for REST or for GraphQL.\n\n\nThe GraphQL has, however, opened publicly the case of graph-orientation in the design of business applications.\n\n\nThis area is really a core game changer for the IT business and it will be a topic described and explained in the future in this site.\n\n\nSee also\n\n\n\n\nAbout Rest",
            "title": "GraphQL And Classic Web Services"
        },
        {
            "location": "/articles/graphql-web-services/#graphql-and-classic-web-services",
            "text": "Facebook produced the  GraphQL specification  in order to be an alternative to Rest.  We already explained  what we thought of Rest services .  GraphQL appears to us as a new way to do the same thing that what the industry is doing for decades.  This article is an analysis of GraphQL principles, their consequences and a comparison with traditional ways of doing web services.",
            "title": "GraphQL And Classic Web Services"
        },
        {
            "location": "/articles/graphql-web-services/#the-basics-of-services",
            "text": "",
            "title": "The Basics of Services"
        },
        {
            "location": "/articles/graphql-web-services/#various-standards-same-spirit",
            "text": "The underlying basis of services is RPC (Remote Procedure Call,  see the dedicated article on the subject ). For decades, this concept is reinvented again and again with various flavors.  Apart from RPC protocols or proprietary protocols (like the  Tuxedo  ones),  Edifact  was for a long time an industry standard in some business domains like travel, banking, medical and logistics. Edifact was replaced by  XML  then by  JSON .  Every of those standards aimed to provide the same functionality: exchanging structured data between two different systems, probably implemented with different programming languages and/or paradigms, and having a client system benefiting from a service provided by a server system.",
            "title": "Various Standards, Same Spirit"
        },
        {
            "location": "/articles/graphql-web-services/#basic-requirements-for-a-service-oriented-language",
            "text": "The first requirement is to have a \"verb\", meaning the name of the \"transaction\" we expect the remote system to process.  The second requirement is to have a type system to be able to send structured data under the form of a tree of objects. This is generally achieved by proposing basic types (like string, integer, float, etc.) and the capability of defining fields that have those properties and that can be grouped into classes. Classes will generally have to indicate what fields are required and what fields are optional. This is generally done within the context of one particular transaction.  Once classes are defined, they can be grouped in larger structures or other classes to define groups of object structures than define larger entities, most of the time, better suitable to express the business semantics.  Optionally, those definitions are expressed in \"grammars\" or \"schemas\". This is the case in RPC systems, in Edifact and in XML, and this is optional for JSON (even if the  JSON schema  specification is very usable). Note that, even if there is no explicit schema mechanism, the fact of defining objects and structures leads to the use of implicit schemas. The difference is really what part of the code is \"validating\" that the messages are well formed:   With an explicit schema definition, we can generate the validation code and create a kind of serializer/unserializer layer (wrapper/unwrapper or proxy/stub layer);  Without an explicit schema definition, the validation code is generally hand written.   All those requirements are defined in the  OSI model  in the \"presentation layer\".",
            "title": "Basic Requirements For a Service Oriented Language"
        },
        {
            "location": "/articles/graphql-web-services/#dont-forget-the-protocol",
            "text": "The last requirement is to have some kind of underlying protocol to:   Send and receive messages between systems;  Create a naming convention that enables to use the name of the service (or name of the \"verb\");  Find the network configuration that enable to route the request message to the proper entry point;  Possibly associate the request and the response with identifiers;  Possibly include information about the sender and its security credentials.   Once again, the  OSI model  defines a certain split of responsibilities between all those requirements. Quickly explained:   The transport layer (layer 4) is defining the addressing scheme and primitives that enables several systems to communicate together;  The session layer (layer 5) enables to manage the sessions and conversations between the client and the server; this layer manages the succession of names messages that the two peers will exchange;  The presentation layer (layer 6) manages the content of data, their structure and their values.   In the current JSON world,  JSON-RPC  is presenting a very basic protocol that can manage the basic single statement and request response messages.",
            "title": "Don't forget the protocol"
        },
        {
            "location": "/articles/graphql-web-services/#graphql-answers-to-service-oriented-requirements",
            "text": "GraphQL seems to us as a new flavor of the same old story. It brings some interesting new stuff (and we will come back on that), but the important point is that we can implement traditional services with this language.",
            "title": "GraphQL Answers To Service Oriented Requirements"
        },
        {
            "location": "/articles/graphql-web-services/#the-verb",
            "text": "Each GraphQL service is an named  http  entry point. The  get  method will be used to access the  query  part of the service and the  post  method will access the  mutation  part of it.  This looks like Rest but this is rather different, because the idea seems not to access to a single resource (even if it is possible), but to perform a service.",
            "title": "The \"Verb\""
        },
        {
            "location": "/articles/graphql-web-services/#the-type-system",
            "text": "GraphQL proposes an extended type system that proposes:   Basic types,  Classes,  Groups of classes,  Fragments (kind of filters on classes),  Queries.   With all the available semantics, it is very easy to implement all the existing web services that are used currently in the business.  Moreover, it is possible to have a richer interface on existing services and a more adaptative way of performing transactions.  This seems rather promising for mobile or chatbot accesses for instance. Those two UIs manipulate a small amount of data and may require some server services to adapt their reply to their particular use.  For sure, we could argue saying: with standard web services in JSON, as a client, we can always extract the subset of the response that is interesting to us. The fact is this feature goes beyond the subset approach and enables to ask the server to adapt the contents of its response to the client.",
            "title": "The Type System"
        },
        {
            "location": "/articles/graphql-web-services/#and-the-protocol",
            "text": "In the Web, the protocol is  http  and the approach seems inspired by both Rest and Ajax kind of calls.",
            "title": "And The Protocol?"
        },
        {
            "location": "/articles/graphql-web-services/#a-promising-standard",
            "text": "",
            "title": "A Promising Standard"
        },
        {
            "location": "/articles/graphql-web-services/#an-intermediate-between-json-rpc-and-rest",
            "text": "GraphQL seems to target the intermediate path between JSON-RPC (which is a simplified SOAP) and Rest. The fact of publishing a schema enables both approaches:   Perform elaborated services (like before);  Access business objects (like the Rest or CRUD approach).   GraphQL seems to propose a way to benefit from both approaches.  The fact of being able to perform complex queries (with a lot of flavours) also adds intelligence to the protocol, an intelligence driven by the client.",
            "title": "An Intermediate Between JSON-RPC and Rest?"
        },
        {
            "location": "/articles/graphql-web-services/#but-what-impact-on-the-server-design",
            "text": "This seems very interesting but the problem caused by this new protocol is located on the backend part.  Some questions must be answered:   What kind of models and/or application this protocol is targeting? All applications? Applications that should be able to disclose a part of their model?  Do we know the extend of the server impacts in terms of design?",
            "title": "But What Impact On The Server Design?"
        },
        {
            "location": "/articles/graphql-web-services/#model-your-business-domain-as-a-graph",
            "text": "We are entering here into delicate topics.  For those how already performed some semantic modeling with  RDF ,  RDFS  or  OWL , I think you're convinced about the fact that we can model a business domain with a graph.  For those who already used graph databases such as  Neo  or  Orient  in Big Data use cases or fraud recognition or social applications, may be convinced that a business model can be modeled as a graph.  But for the vast majority of IT people, this assertion is not obvious.  Well, we'll come back on this particular topic in future articles, so we'll take for granted that a business domain can be modeled with a graph.",
            "title": "Model Your Business Domain As A Graph"
        },
        {
            "location": "/articles/graphql-web-services/#graph-data-means-graph-database",
            "text": "What Facebook is proposing us is a protocol that enables to query the internal structure of the graph of data.  So, the first consequences seem to be:   Your data are modeled as a graph,  You probably use a graph database of some sort.   Consequently, your GraphQL schema is more or less the same thing as your graph database schema (when this sort of things exists such as in  OrientDB ).",
            "title": "Graph Data Means Graph Database"
        },
        {
            "location": "/articles/graphql-web-services/#open-heart-data-model",
            "text": "The second consequence is also double:   You published your graph business model to your clients;  You use your database model as the presentation layer.   This second point is very problematic. Indeed, it explains why the protocol is proposing so much complexity in some of its part: because it is supposed to be, at the same time, a protocol and a graph query language (like  Cypher ).  In a certain sense, it is supposed to be the new  SQL*Net  or  ODBC  but for graphs.  And this is where the approach is questionable.",
            "title": "Open Heart Data Model"
        },
        {
            "location": "/articles/graphql-web-services/#the-myth-of-the-single-representation-of-reality",
            "text": "There are many myths in IT, and this one is a big one. Many people believe that there is only one way of representing the reality-theirs, indeed. And the reality is, they are wrong.  Note that, throughout the history of science, many great scientists had the same debate as the  intuitionist debate of the beginning of the 20th Century .  Any software designer that worked sufficiently in the domain should have seen several facts:   Whatever the efforts and the design patterns used, you cannot predict how the business will evolve;  Two designer confronted to the same problem will produce two different designs.   Indeed, design is very subjective and, for a lot of reasons we won't explain here, it is not a bad thing that there is no unique model for a particular business domain, on the contrary.",
            "title": "The Myth Of The Single Representation Of Reality"
        },
        {
            "location": "/articles/graphql-web-services/#when-client-and-server-share-everything",
            "text": "GraphQL is proposing a way for the customer to be very near from its server, so near that its data model is perfectly known, right from the client.  This can be very useful when you work with yourself, when you develop your own client and server in a kind of symbiosis. If you are on an Internet application, you also must know that you core model will be known from the outside, which can be a problems in terms of IP and in terms of security.  So:   GraphQL is OK if your API is  internal ;  GraphQL seems not OK if your API is external.",
            "title": "When Client and Server Share Everything"
        },
        {
            "location": "/articles/graphql-web-services/#publishing-a-graphql-api",
            "text": "Well, if you want to publish a GraphQL API, you have to consider several things:   You will impose to your client the graph model of your business domain, and maybe it is relevant and maybe not (see next part);  You will disclose a graph representation of your business model, which is not the case in JSON-RPC where you only disclose the  interoperability formats ; This can represent a potential threat on your IT because your software design is very often at the heart of your business value;  You will have to have a complex security system, which is the security system of the graph exploration, and this will not be obvious to address;  You will be bound to implement the protocol complexity that can open more security issues in your software.   For sure, there are cases where all those arguments may be irrelevant:   You can work in a domain where the graph model has no value or is very well known (for instance the social media);  You can work for non profit organizations;  You can have a system that will not cause any loss of money, loss of IP or loss of value if hacked.",
            "title": "Publishing a GraphQL API"
        },
        {
            "location": "/articles/graphql-web-services/#the-fundamental-principle-of-interoperability",
            "text": "In complement to the  article on REST , we will explain the fundamental principle of interoperability.  The context of interoperability is the following:   Systems communicate when they have some interest in doing so: interoperability is business-oriented.  To establish system interconnection costs money and time, this in the project phase but also in the recurring phase: to realize those investments, the client and the server generally have a good business reason.  When a server is publishing an API, it cannot imagine the list of clients he will have tomorrow, nor make assumptions on their programming language or even design approach.   In this context, the fundamental principle of interoperability is that the client and the server contracting the exchange  should define the common format that is proposing the less semantic constraints possible on both ends .  Because the client and the server don't have to commit on their internal structure and functional aspects, the interchange is described with a semantic that must be the simplest possible and that can enable the client and server to define different modeling of the same \"exchanged data model\".   The figure above shows the core problem of interoperability:   The client and the server have no reason to model the interchange data the same way. Indeed, they don't. The consequence is that they must adapt their internal formats (memory or database) to the network presentation.  The way they store the data is their problem, and their storage representation can be even quite different from their memory representation (that's why, generally, people use a data access layer to adapt their business objects to the database).   Very commonly, in the industry, we have 3 representations of the data: the client's, the server's and the network presentation.  Those adaptation are painful to develop, for sure, but they are the conditions of isolation, and who says isolation says separation of concerns.  This is because the client and the server are different software parts that there is no need to impose more constraints.  In this context, we do not recommend using GraphQL for an external business domain interface, but we can recommend it if the application architecture needs a kinf of ODBC or SQL*Net directly from the UI, and in a context where the API is not made to be exposed to unknown third parties.  Note that we did not really enter into the core details of \"how do we really build a graph-oriented business application\"? This will come in later articles.",
            "title": "The Fundamental Principle Of Interoperability"
        },
        {
            "location": "/articles/graphql-web-services/#a-correct-intuition",
            "text": "If we try to step back one minute, we can say that the Facebook team may have had an intuition that, with the graph-oriented modeling approach, this fundamental principle or interoperability could be declared obsolete (and consequently graphs could be in the client, in the server, in the database, with exactly the same structure).  In some cases, that's true: if you master both ends of the wire, that's probably one very efficient way to do it. But it looks like ODBC or SQL*Net anyway.  One thing is sure, the graph-oriented modeling of business domains will revolution the IT world, but perhaps not the way the Facebook team imagined it. We'll come back on that in the future.",
            "title": "A Correct Intuition?"
        },
        {
            "location": "/articles/graphql-web-services/#conclusion",
            "text": "GraphQL is a very interesting attempt to propose a middle term between REST and JSON-RPC, as the following diagram is showing it.   However, this diagram is very misleading because the 3 inbound protocols have many different impacts on the server design, contrary to what's presented. Indeed, both REST and GraphQL imply a very specific programming model:   REST is imposing a hard resource orientation that is unnatural to business applications (see  here ),  GraphQL proposes a graph-oriented ODBC-like protocol that will have the tendency to tie-up strongly the client and the server.   The conclusion is it seems to us that, so far, only RPC enables to design and deliver reliable and secure business applications, and to do it the way you want. RPC defines a contract that can lead to many various programming paradigms, which is not the case for REST or for GraphQL.  The GraphQL has, however, opened publicly the case of graph-orientation in the design of business applications.  This area is really a core game changer for the IT business and it will be a topic described and explained in the future in this site.",
            "title": "Conclusion"
        },
        {
            "location": "/articles/graphql-web-services/#see-also",
            "text": "About Rest",
            "title": "See also"
        },
        {
            "location": "/about/about/",
            "text": "About The Author\n\n\nYeah, that's me.",
            "title": "Author"
        },
        {
            "location": "/about/about/#about-the-author",
            "text": "Yeah, that's me.",
            "title": "About The Author"
        },
        {
            "location": "/about/LICENSE/",
            "text": "This site is licensed under the terms and conditions of the GNU FDL V3 that can be found hereafter.\n\n\nGNU Free Documentation License\n\n\nVersion 1.3, 3 November 2008\n\n\nCopyright \u00a9 2000, 2001, 2002, 2007, 2008 Free Software Foundation, Inc. \nhttps://fsf.org/\n\n\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\n\n0. PREAMBLE\n\n\nThe purpose of this License is to make a manual, textbook, or other functional and useful document \"free\" in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or noncommercially. Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others.\n\n\nThis License is a kind of \"copyleft\", which means that derivative works of the document must themselves be free in the same sense. It complements the GNU General Public License, which is a copyleft license designed for free software.\n\n\nWe have designed this License in order to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does. But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book. We recommend this License principally for works whose purpose is instruction or reference.\n\n\n1. APPLICABILITY AND DEFINITIONS\n\n\nThis License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License. Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein. The \"Document\", below, refers to any such manual or work. Any member of the public is a licensee, and is addressed as \"you\". You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law.\n\n\nA \"Modified Version\" of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language.\n\n\nA \"Secondary Section\" is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document's overall subject (or to related matters) and contains nothing that could fall directly within that overall subject. (Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them.\n\n\nThe \"Invariant Sections\" are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License. If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant. The Document may contain zero Invariant Sections. If the Document does not identify any Invariant Sections then there are none.\n\n\nThe \"Cover Texts\" are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License. A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words.\n\n\nA \"Transparent\" copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters. A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent. An image format is not Transparent if used for any substantial amount of text. A copy that is not \"Transparent\" is called \"Opaque\".\n\n\nExamples of suitable formats for Transparent copies include plain ASCII without markup, Texinfo input format, LaTeX input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification. Examples of transparent image formats include PNG, XCF and JPG. Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only.\n\n\nThe \"Title Page\" means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page. For works in formats which do not have any title page as such, \"Title Page\" means the text near the most prominent appearance of the work's title, preceding the beginning of the body of the text.\n\n\nThe \"publisher\" means any person or entity that distributes copies of the Document to the public.\n\n\nA section \"Entitled XYZ\" means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language. (Here XYZ stands for a specific section name mentioned below, such as \"Acknowledgements\", \"Dedications\", \"Endorsements\", or \"History\".) To \"Preserve the Title\" of such a section when you modify the Document means that it remains a section \"Entitled XYZ\" according to this definition.\n\n\nThe Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document. These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License.\n\n\n2. VERBATIM COPYING\n\n\nYou may copy and distribute the Document in any medium, either commercially or noncommercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License. You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute. However, you may accept compensation in exchange for copies. If you distribute a large enough number of copies you must also follow the conditions in section 3.\n\n\nYou may also lend copies, under the same conditions stated above, and you may publicly display copies.\n\n\n3. COPYING IN QUANTITY\n\n\nIf you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document's license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover. Both covers must also clearly and legibly identify you as the publisher of these copies. The front cover must present the full title with all words of the title equally prominent and visible. You may add other material on the covers in addition. Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects.\n\n\nIf the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages.\n\n\nIf you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material. If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public.\n\n\nIt is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document.\n\n\n4. MODIFICATIONS\n\n\nYou may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it. In addition, you must do these things in the Modified Version:\n\n\nA. Use in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission.\nB. List on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement.\nC. State on the Title page the name of the publisher of the Modified Version, as the publisher.\nD. Preserve all the copyright notices of the Document.\nE. Add an appropriate copyright notice for your modifications adjacent to the other copyright notices.\nF. Include, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below.\nG. Preserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document's license notice.\nH. Include an unaltered copy of this License.\nI. Preserve the section Entitled \"History\", Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled \"History\" in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence.\nJ. Preserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the \"History\" section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission.\nK. For any section Entitled \"Acknowledgements\" or \"Dedications\", Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein.\nL. Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles.\nM. Delete any section Entitled \"Endorsements\". Such a section may not be included in the Modified Version.\nN. Do not retitle any existing section to be Entitled \"Endorsements\" or to conflict in title with any Invariant Section.\nO. Preserve any Warranty Disclaimers.\n\n\nIf the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant. To do this, add their titles to the list of Invariant Sections in the Modified Version's license notice. These titles must be distinct from any other section titles.\n\n\nYou may add a section Entitled \"Endorsements\", provided it contains nothing but endorsements of your Modified Version by various parties\u2014for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard.\n\n\nYou may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version. Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity. If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one.\n\n\nThe author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version.\n\n\n5. COMBINING DOCUMENTS\n\n\nYou may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers.\n\n\nThe combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy. If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number. Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work.\n\n\nIn the combination, you must combine any sections Entitled \"History\" in the various original documents, forming one section Entitled \"History\"; likewise combine any sections Entitled \"Acknowledgements\", and any sections Entitled \"Dedications\". You must delete all sections Entitled \"Endorsements\".\n\n\n6. COLLECTIONS OF DOCUMENTS\n\n\nYou may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects.\n\n\nYou may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document.\n\n\n7. AGGREGATION WITH INDEPENDENT WORKS\n\n\nA compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the copyright resulting from the compilation is not used to limit the legal rights of the compilation's users beyond what the individual works permit. When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document.\n\n\nIf the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document's Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form. Otherwise they must appear on printed covers that bracket the whole aggregate.\n\n\n8. TRANSLATION\n\n\nTranslation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4. Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections. You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers. In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail.\n\n\nIf a section in the Document is Entitled \"Acknowledgements\", \"Dedications\", or \"History\", the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title.\n\n\n9. TERMINATION\n\n\nYou may not copy, modify, sublicense, or distribute the Document except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense, or distribute it is void, and will automatically terminate your rights under this License.\n\n\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\n\n\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\n\n\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, receipt of a copy of some or all of the same material does not give you any rights to use it.\n\n\n10. FUTURE REVISIONS OF THIS LICENSE\n\n\nThe Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. See https://www.gnu.org/licenses/.\n\n\nEach version of the License is given a distinguishing version number. If the Document specifies that a particular numbered version of this License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation. If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation. If the Document specifies that a proxy can decide which future versions of this License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Document.\n\n\n11. RELICENSING\n\n\n\"Massive Multiauthor Collaboration Site\" (or \"MMC Site\") means any World Wide Web server that publishes copyrightable works and also provides prominent facilities for anybody to edit those works. A public wiki that anybody can edit is an example of such a server. A \"Massive Multiauthor Collaboration\" (or \"MMC\") contained in the site means any set of copyrightable works thus published on the MMC site.\n\n\n\"CC-BY-SA\" means the Creative Commons Attribution-Share Alike 3.0 license published by Creative Commons Corporation, a not-for-profit corporation with a principal place of business in San Francisco, California, as well as future copyleft versions of that license published by that same organization.\n\n\n\"Incorporate\" means to publish or republish a Document, in whole or in part, as part of another Document.\n\n\nAn MMC is \"eligible for relicensing\" if it is licensed under this License, and if all works that were first published under this License somewhere other than this MMC, and subsequently incorporated in whole or in part into the MMC, (1) had no cover texts or invariant sections, and (2) were thus incorporated prior to November 1, 2008.\n\n\nThe operator of an MMC Site may republish an MMC contained in the site under CC-BY-SA on the same site at any time before August 1, 2009, provided the MMC is eligible for relicensing.\nADDENDUM: How to use this License for your documents\n\n\nTo use this License in a document you have written, include a copy of the License in the document and put the following copyright and license notices just after the title page:",
            "title": "License"
        },
        {
            "location": "/about/LICENSE/#gnu-free-documentation-license",
            "text": "Version 1.3, 3 November 2008  Copyright \u00a9 2000, 2001, 2002, 2007, 2008 Free Software Foundation, Inc.  https://fsf.org/  Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.",
            "title": "GNU Free Documentation License"
        },
        {
            "location": "/about/LICENSE/#0-preamble",
            "text": "The purpose of this License is to make a manual, textbook, or other functional and useful document \"free\" in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or noncommercially. Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others.  This License is a kind of \"copyleft\", which means that derivative works of the document must themselves be free in the same sense. It complements the GNU General Public License, which is a copyleft license designed for free software.  We have designed this License in order to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does. But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book. We recommend this License principally for works whose purpose is instruction or reference.",
            "title": "0. PREAMBLE"
        },
        {
            "location": "/about/LICENSE/#1-applicability-and-definitions",
            "text": "This License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License. Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein. The \"Document\", below, refers to any such manual or work. Any member of the public is a licensee, and is addressed as \"you\". You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law.  A \"Modified Version\" of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language.  A \"Secondary Section\" is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document's overall subject (or to related matters) and contains nothing that could fall directly within that overall subject. (Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them.  The \"Invariant Sections\" are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License. If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant. The Document may contain zero Invariant Sections. If the Document does not identify any Invariant Sections then there are none.  The \"Cover Texts\" are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License. A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words.  A \"Transparent\" copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters. A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent. An image format is not Transparent if used for any substantial amount of text. A copy that is not \"Transparent\" is called \"Opaque\".  Examples of suitable formats for Transparent copies include plain ASCII without markup, Texinfo input format, LaTeX input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification. Examples of transparent image formats include PNG, XCF and JPG. Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only.  The \"Title Page\" means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page. For works in formats which do not have any title page as such, \"Title Page\" means the text near the most prominent appearance of the work's title, preceding the beginning of the body of the text.  The \"publisher\" means any person or entity that distributes copies of the Document to the public.  A section \"Entitled XYZ\" means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language. (Here XYZ stands for a specific section name mentioned below, such as \"Acknowledgements\", \"Dedications\", \"Endorsements\", or \"History\".) To \"Preserve the Title\" of such a section when you modify the Document means that it remains a section \"Entitled XYZ\" according to this definition.  The Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document. These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License.",
            "title": "1. APPLICABILITY AND DEFINITIONS"
        },
        {
            "location": "/about/LICENSE/#2-verbatim-copying",
            "text": "You may copy and distribute the Document in any medium, either commercially or noncommercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License. You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute. However, you may accept compensation in exchange for copies. If you distribute a large enough number of copies you must also follow the conditions in section 3.  You may also lend copies, under the same conditions stated above, and you may publicly display copies.",
            "title": "2. VERBATIM COPYING"
        },
        {
            "location": "/about/LICENSE/#3-copying-in-quantity",
            "text": "If you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document's license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover. Both covers must also clearly and legibly identify you as the publisher of these copies. The front cover must present the full title with all words of the title equally prominent and visible. You may add other material on the covers in addition. Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects.  If the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages.  If you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material. If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public.  It is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document.",
            "title": "3. COPYING IN QUANTITY"
        },
        {
            "location": "/about/LICENSE/#4-modifications",
            "text": "You may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it. In addition, you must do these things in the Modified Version:  A. Use in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission.\nB. List on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement.\nC. State on the Title page the name of the publisher of the Modified Version, as the publisher.\nD. Preserve all the copyright notices of the Document.\nE. Add an appropriate copyright notice for your modifications adjacent to the other copyright notices.\nF. Include, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below.\nG. Preserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document's license notice.\nH. Include an unaltered copy of this License.\nI. Preserve the section Entitled \"History\", Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled \"History\" in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence.\nJ. Preserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the \"History\" section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission.\nK. For any section Entitled \"Acknowledgements\" or \"Dedications\", Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein.\nL. Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles.\nM. Delete any section Entitled \"Endorsements\". Such a section may not be included in the Modified Version.\nN. Do not retitle any existing section to be Entitled \"Endorsements\" or to conflict in title with any Invariant Section.\nO. Preserve any Warranty Disclaimers.  If the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant. To do this, add their titles to the list of Invariant Sections in the Modified Version's license notice. These titles must be distinct from any other section titles.  You may add a section Entitled \"Endorsements\", provided it contains nothing but endorsements of your Modified Version by various parties\u2014for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard.  You may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version. Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity. If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one.  The author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version.",
            "title": "4. MODIFICATIONS"
        },
        {
            "location": "/about/LICENSE/#5-combining-documents",
            "text": "You may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers.  The combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy. If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number. Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work.  In the combination, you must combine any sections Entitled \"History\" in the various original documents, forming one section Entitled \"History\"; likewise combine any sections Entitled \"Acknowledgements\", and any sections Entitled \"Dedications\". You must delete all sections Entitled \"Endorsements\".",
            "title": "5. COMBINING DOCUMENTS"
        },
        {
            "location": "/about/LICENSE/#6-collections-of-documents",
            "text": "You may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects.  You may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document.",
            "title": "6. COLLECTIONS OF DOCUMENTS"
        },
        {
            "location": "/about/LICENSE/#7-aggregation-with-independent-works",
            "text": "A compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the copyright resulting from the compilation is not used to limit the legal rights of the compilation's users beyond what the individual works permit. When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document.  If the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document's Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form. Otherwise they must appear on printed covers that bracket the whole aggregate.",
            "title": "7. AGGREGATION WITH INDEPENDENT WORKS"
        },
        {
            "location": "/about/LICENSE/#8-translation",
            "text": "Translation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4. Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections. You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers. In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail.  If a section in the Document is Entitled \"Acknowledgements\", \"Dedications\", or \"History\", the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title.",
            "title": "8. TRANSLATION"
        },
        {
            "location": "/about/LICENSE/#9-termination",
            "text": "You may not copy, modify, sublicense, or distribute the Document except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense, or distribute it is void, and will automatically terminate your rights under this License.  However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.  Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.  Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, receipt of a copy of some or all of the same material does not give you any rights to use it.",
            "title": "9. TERMINATION"
        },
        {
            "location": "/about/LICENSE/#10-future-revisions-of-this-license",
            "text": "The Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. See https://www.gnu.org/licenses/.  Each version of the License is given a distinguishing version number. If the Document specifies that a particular numbered version of this License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation. If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation. If the Document specifies that a proxy can decide which future versions of this License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Document.",
            "title": "10. FUTURE REVISIONS OF THIS LICENSE"
        },
        {
            "location": "/about/LICENSE/#11-relicensing",
            "text": "\"Massive Multiauthor Collaboration Site\" (or \"MMC Site\") means any World Wide Web server that publishes copyrightable works and also provides prominent facilities for anybody to edit those works. A public wiki that anybody can edit is an example of such a server. A \"Massive Multiauthor Collaboration\" (or \"MMC\") contained in the site means any set of copyrightable works thus published on the MMC site.  \"CC-BY-SA\" means the Creative Commons Attribution-Share Alike 3.0 license published by Creative Commons Corporation, a not-for-profit corporation with a principal place of business in San Francisco, California, as well as future copyleft versions of that license published by that same organization.  \"Incorporate\" means to publish or republish a Document, in whole or in part, as part of another Document.  An MMC is \"eligible for relicensing\" if it is licensed under this License, and if all works that were first published under this License somewhere other than this MMC, and subsequently incorporated in whole or in part into the MMC, (1) had no cover texts or invariant sections, and (2) were thus incorporated prior to November 1, 2008.  The operator of an MMC Site may republish an MMC contained in the site under CC-BY-SA on the same site at any time before August 1, 2009, provided the MMC is eligible for relicensing.\nADDENDUM: How to use this License for your documents  To use this License in a document you have written, include a copy of the License in the document and put the following copyright and license notices just after the title page:",
            "title": "11. RELICENSING"
        }
    ]
}