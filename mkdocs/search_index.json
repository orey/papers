{
    "docs": [
        {
            "location": "/",
            "text": "Olivier Rey's Papers\n\n\nPapers\n\n\n\n\nThe Five Levels of Conceptual Maturity for IT Teams\n\n\nThe V2 Vortex\n - New version November 2017\n\n\nThe Various Stages of Digital Transformation\n\n\nConsiderations about Rest and Web Services\n\n\nGraphQL And Classic Web Services\n\n\nA Simple Meta-Model for Portfolio Management\n\n\nAn Introduction to The Archimate Revolution\n\n\n\n\nGraph-oriented programming\n\n\nThis is a new section with some interesting materials about graph-oriented programming, solving the technical debt issue and using graph transformations in software engineering.\n\n\n\n\nFirst article on graph-oriented programming\n\n\nConference at the STAF/ICGT 2018 in Toulouse",
            "title": "Home"
        },
        {
            "location": "/#olivier-reys-papers",
            "text": "",
            "title": "Olivier Rey's Papers"
        },
        {
            "location": "/#papers",
            "text": "The Five Levels of Conceptual Maturity for IT Teams  The V2 Vortex  - New version November 2017  The Various Stages of Digital Transformation  Considerations about Rest and Web Services  GraphQL And Classic Web Services  A Simple Meta-Model for Portfolio Management  An Introduction to The Archimate Revolution",
            "title": "Papers"
        },
        {
            "location": "/#graph-oriented-programming",
            "text": "This is a new section with some interesting materials about graph-oriented programming, solving the technical debt issue and using graph transformations in software engineering.   First article on graph-oriented programming  Conference at the STAF/ICGT 2018 in Toulouse",
            "title": "Graph-oriented programming"
        },
        {
            "location": "/articles/five-levels/",
            "text": "The Five Levels of Conceptual Maturity for IT Teams\n\n\n\n\nIT is a strange activity because it is an industry that has troubles getting out of a craft industry approach. Constantly moving to new exciting technologies and constantly influenced by massive technology centered marketing, IT people stay durably too technical and have recurring troubles to achieve IT projects.\n\n\nHistory of the Model\n\n\nYears ago, I was consulting a CEO and was trying to explain to him why his teams were failing big IT projects. A lot of executives today are digitally aware in their daily business practices but, when it comes to organizing new IT projects inside their company, they frequently discover that there is a huge gap between what they wanted to achieve and what is really done. Money is not the problem, and many big companies invest massive amounts of money in IT projects without getting the most of it. The fact is, when you invest too much in an IT project, your chances of success decrease drastically.\n\n\nI came to the CEO with this maturity model. I don't proclaim it to be perfect, far from it: it's just a representation of the problem. But the model is quite useful to explain and diagnose the capability of teams to perform IT projects and the levels of complexity attached to those projects.\n\n\nA Model of Conceptual Maturity\n\n\nIT is an abstract discipline manipulating abstract concepts. In IT, it is always possible to \"make the code work\", even if the cost and the delay to do that are unacceptable.\n\n\nThe problem of IT is that you can pay the price of bad choices years (even decades) after you did the investment - and you pay each year, possibly more and more year after year for the exact same modification. Bad IT choices imply increasing the technical debt and jeopardize the capability of achieving future projects (at least you will continuously increase their cost). In that sense, IT is quite near from traditional industry.\n\n\nThe fact is, in tradition industry, people are specialized in jobs: not every automotive worker will design the brakes or the engine. In IT, the situation is blurrier. Most IT people don't know what they are capable of doing, because specialization of jobs has not yet reached an industry agreement, and because it seems always possible to \"make the code work\" (despite of any consideration about quality/evolutions/money/timing/etc.).\n\n\nHowever, in IT, there are several levels of concerns that require different skills and different conceptual approaches. Over my 20 years in the IT business, I identified five levels in this conceptual world of software systems creation. Generally, in their professional life, people climb the rungs of the ladder in the same order, which makes the model a \"kind of\" maturity model.\n\n\nThe Levels of Conceptual Maturity\n\n\nI will describe briefly the most important aspects of the various levels. I don't pretend to be exhaustive and I use more detailed materials and samples when I play the consultant for executives.\n\n\nLevel 1: Development\n\n\nMost IT people know how to code, how to produce programs. For that purpose, IT people use programming languages (like Java, Cobol, C++, C, etc.). Behind the coding level, we can add the mastering of the development environment: development standards, debugging, versionning and branching management, build management, testing management, etc.. All those skills are crucial to good developments. Considering all that, if we rated people from 0 to 1 on this topic, I would say that most developers in the market are between 0.5 and 0.8 in development.\n\n\nLevel 2: Design\n\n\nIT uses object orientation programming (OOP) commonly for more than two decades. However, OOP is conceptually complicated. To be explained to the non-IT people, design is the activity of building a set of programs structure and knowing where to put the code inside the program structure. Depending on how the programs are internally structured, you will face more or less troubles in the future maintenance and evolutions. Design activity uses lots of design patterns (level 2) which are generally badly mastered by IT teams because IT teams think too often at the \"code level \" (level 1). With years, and with Agile methodologies, IT people were pushed back to level 1. Design is complicated. IT people have to think before they code (very hard for developers); they have to forecast what will be the maintenance. Few people are really capable of good designs.\n\n\nLevel 3: Application Architecture\n\n\nArchitecting an application today is not easy, because they are a lot of trendy \"off-the-shelf\" components, a lot of competing \"sexy\" technologies. The hype around technologies is extreme and IT people can change frameworks very quickly to follow the last trend. Architecting an application is the skill to choose what components (reusable or not) will be used into an application. It enables the application to be developed correctly by several people, to isolate \"modules\", to manage customizable dependencies inside the IT systems. This is not an easy job. The objective is not just to realize software but to be able to maintain them on the long run and whatever their functional perimeter (that can become quite large with years). In order to build good application architecture, you have to think about \"component borders\" and be able to estimate the pros and cons of using a trendy component in the market. That's quite a hard job.\n\n\nLevel 4: IT Architecture\n\n\nIT is everywhere and companies, even SMEs, have quite quickly a lot of IT systems. Being able to make those systems communicate together without being too hardly coupled, being able to make the IT systems as a whole evolving in a reasonable timing for reasonable costs, being able to communicate with the outside of the company in a safe manner, all that is part of IT architecture. In IT architecture, you also manage the positioning of features inside softwares: when a function is implemented in the wrong software, you can loose millions of dollars. In IT architecture problems are also the integration problems: a good IT architecture helps decreasing the QA costs and improving the TTM of the projects. Good IT architects are very rare in the market. It seems a pity because, for large companies, IT systems are one of the biggest problems, for instance in M&A phases. To add to the complexity, the IT market sells a lot of \"off-the-shelf solutions\" in that area, solutions that often bring confusion in IT systems, when put in place by non level 4 people.\n\n\nLevel 5: Enterprise Architecture\n\n\nThe term \"Enterprise Architecture\" (EA) is at the center of a lot of discussions because the industry is not clear about what an enterprise architect is supposed to do. I will provide my own pragmatic vision (which can be challenged). An enterprise architect must primarily ensure that company strategic intentions are concretely declined into the IT systems through auditable projects and programs. Secondly he/she must ensure that the changes brought to the softwares can be managed by end users, that software changes are not destroying efficient business processes but improving them. This is quite a work that requires a lot of IT skills but also a good business understanding and enhanced communication skills.\n\n\nThe Usual Maturity Path\n\n\nOne common path for IT-related designers or architects is to go from level 1 to level 5. Each level is requiring, following my experience, in average, at least 2 years of real life practice. Most often, when you work at a certain level, you have to consider also the impacts of what you are doing in all other lowest levels (that's why enterprise architects that never coded often make me uncomfortable, because they cannot imagine the consequences of their decisions).\n\n\nMost people will find happiness in a certain layer and will excel in that layer because it fits their mind and taste and it is perfectly adapted to the level of complexity they like to manage. Without a proper project experience and without training and/or coaching, it is quite hard for people to change levels without having to work for months in real life at the concerned level.\n\n\nA Maturity Model Related to Complexity\n\n\nThe model describes the capability to solve IT problems at the proper level, to manage the complexity of the IT problem with the proper approach. For instance, you can take a level 4 problem with a level 1 approach but you have 70% of chances of never solving the problem completely and 95% of chances to spend millions of dollars fixing \"things that don't work\". The \"things that don't work\" were created by people because of their problem solving approach (level 1 instead of level 4). Using the proper approach would never have generated those costs and troubles.\n\n\nWhat is the Required Level of Maturity for my Project?\n\n\nRegarding this maturity model, executives should appreciate the level of maturity of their internal IT organization. They are then able to ask themselves the question: what's the adequate level of maturity required to achieve the project I want? Or, we can ask the question the other way round: what projects can I safely achieve considering the maturity of my internal IT organization? Those questions are crucial.\n\n\nWhen I look at failed projects, I often see people having a maturity level between 1 and 3 trying to master a multi-million dollars IT projects requiring a level 5. Integrating external consultants can do the job but not in all cases. If your business is complex and quite specific, it is preferable to bet on internal skills in the long term for IT people to understand your business in depth. Moreover, who chooses the casting of your external consultants? Your internal IT organization, and few organizations are able to hire consultants that know more than they do.\n\n\nThis is exactly what happened when big companies got out of the mainframe. Mainframe teams were mostly level 1, some of them were level 2 (level 3 is not relevant in old mainframe application design and level 4 is often just about producing files for the outside or consuming them). Giving such a team an IT program with distributed applications in various technologies to build is doomed to failure, whatever the money and whatever the quality and dedication of people. For sure, with time and the appropriate training, most people can climb rungs in the ladder. But without time and training, people will keep on reproducing failures. Suppose the management is not aware of the core problem and people get fired for project failure, you can end up taking big outsourcing decisions based on a situation that nobody really understood.\n\n\nIT Industry Should Target Maturity\n\n\nBecause it is only \"soft\"-ware and not hardware, the IT industry seems to like the level 1 so much that every serious attempt to raise to level 2 or 3 is pushed back to level 1 by the IT providers. IT providers love to focus their customers on the \"code level\": because they cannot see the bigger picture and buy their products even if they don't need them.\n\n\nIf the IT industry created models for IT project management (like PMP), the increasing complexity of existing systems and new projects require the appropriate level of maturity in the project designers/architects teams.\n\n\nThe problem today, in most companies, is not only to \"code\" nor only to \"manage projects\": it is to also to maintain the running systems and make the huge amount of legacy applications evolve without risks and conforming to the company strategic intentions. Most IT systems are a pile of geological layers of outdated technological hypes. The code of today becomes the legacy of tomorrow. If some industries (like aviation) forecast their investments for decades, IT should begin to do the same, to work on the adequate long-term skills that are required to ensure durable and quality IT services.\n\n\nIT industry should target maturity and maturity comes with the identification of our level of mastering of IT complexity.\n\n\n(\nApril 2015\n)",
            "title": "The Five Levels of Conceptual Maturity for IT Teams"
        },
        {
            "location": "/articles/five-levels/#the-five-levels-of-conceptual-maturity-for-it-teams",
            "text": "IT is a strange activity because it is an industry that has troubles getting out of a craft industry approach. Constantly moving to new exciting technologies and constantly influenced by massive technology centered marketing, IT people stay durably too technical and have recurring troubles to achieve IT projects.",
            "title": "The Five Levels of Conceptual Maturity for IT Teams"
        },
        {
            "location": "/articles/five-levels/#history-of-the-model",
            "text": "Years ago, I was consulting a CEO and was trying to explain to him why his teams were failing big IT projects. A lot of executives today are digitally aware in their daily business practices but, when it comes to organizing new IT projects inside their company, they frequently discover that there is a huge gap between what they wanted to achieve and what is really done. Money is not the problem, and many big companies invest massive amounts of money in IT projects without getting the most of it. The fact is, when you invest too much in an IT project, your chances of success decrease drastically.  I came to the CEO with this maturity model. I don't proclaim it to be perfect, far from it: it's just a representation of the problem. But the model is quite useful to explain and diagnose the capability of teams to perform IT projects and the levels of complexity attached to those projects.",
            "title": "History of the Model"
        },
        {
            "location": "/articles/five-levels/#a-model-of-conceptual-maturity",
            "text": "IT is an abstract discipline manipulating abstract concepts. In IT, it is always possible to \"make the code work\", even if the cost and the delay to do that are unacceptable.  The problem of IT is that you can pay the price of bad choices years (even decades) after you did the investment - and you pay each year, possibly more and more year after year for the exact same modification. Bad IT choices imply increasing the technical debt and jeopardize the capability of achieving future projects (at least you will continuously increase their cost). In that sense, IT is quite near from traditional industry.  The fact is, in tradition industry, people are specialized in jobs: not every automotive worker will design the brakes or the engine. In IT, the situation is blurrier. Most IT people don't know what they are capable of doing, because specialization of jobs has not yet reached an industry agreement, and because it seems always possible to \"make the code work\" (despite of any consideration about quality/evolutions/money/timing/etc.).  However, in IT, there are several levels of concerns that require different skills and different conceptual approaches. Over my 20 years in the IT business, I identified five levels in this conceptual world of software systems creation. Generally, in their professional life, people climb the rungs of the ladder in the same order, which makes the model a \"kind of\" maturity model.",
            "title": "A Model of Conceptual Maturity"
        },
        {
            "location": "/articles/five-levels/#the-levels-of-conceptual-maturity",
            "text": "I will describe briefly the most important aspects of the various levels. I don't pretend to be exhaustive and I use more detailed materials and samples when I play the consultant for executives.",
            "title": "The Levels of Conceptual Maturity"
        },
        {
            "location": "/articles/five-levels/#level-1-development",
            "text": "Most IT people know how to code, how to produce programs. For that purpose, IT people use programming languages (like Java, Cobol, C++, C, etc.). Behind the coding level, we can add the mastering of the development environment: development standards, debugging, versionning and branching management, build management, testing management, etc.. All those skills are crucial to good developments. Considering all that, if we rated people from 0 to 1 on this topic, I would say that most developers in the market are between 0.5 and 0.8 in development.",
            "title": "Level 1: Development"
        },
        {
            "location": "/articles/five-levels/#level-2-design",
            "text": "IT uses object orientation programming (OOP) commonly for more than two decades. However, OOP is conceptually complicated. To be explained to the non-IT people, design is the activity of building a set of programs structure and knowing where to put the code inside the program structure. Depending on how the programs are internally structured, you will face more or less troubles in the future maintenance and evolutions. Design activity uses lots of design patterns (level 2) which are generally badly mastered by IT teams because IT teams think too often at the \"code level \" (level 1). With years, and with Agile methodologies, IT people were pushed back to level 1. Design is complicated. IT people have to think before they code (very hard for developers); they have to forecast what will be the maintenance. Few people are really capable of good designs.",
            "title": "Level 2: Design"
        },
        {
            "location": "/articles/five-levels/#level-3-application-architecture",
            "text": "Architecting an application today is not easy, because they are a lot of trendy \"off-the-shelf\" components, a lot of competing \"sexy\" technologies. The hype around technologies is extreme and IT people can change frameworks very quickly to follow the last trend. Architecting an application is the skill to choose what components (reusable or not) will be used into an application. It enables the application to be developed correctly by several people, to isolate \"modules\", to manage customizable dependencies inside the IT systems. This is not an easy job. The objective is not just to realize software but to be able to maintain them on the long run and whatever their functional perimeter (that can become quite large with years). In order to build good application architecture, you have to think about \"component borders\" and be able to estimate the pros and cons of using a trendy component in the market. That's quite a hard job.",
            "title": "Level 3: Application Architecture"
        },
        {
            "location": "/articles/five-levels/#level-4-it-architecture",
            "text": "IT is everywhere and companies, even SMEs, have quite quickly a lot of IT systems. Being able to make those systems communicate together without being too hardly coupled, being able to make the IT systems as a whole evolving in a reasonable timing for reasonable costs, being able to communicate with the outside of the company in a safe manner, all that is part of IT architecture. In IT architecture, you also manage the positioning of features inside softwares: when a function is implemented in the wrong software, you can loose millions of dollars. In IT architecture problems are also the integration problems: a good IT architecture helps decreasing the QA costs and improving the TTM of the projects. Good IT architects are very rare in the market. It seems a pity because, for large companies, IT systems are one of the biggest problems, for instance in M&A phases. To add to the complexity, the IT market sells a lot of \"off-the-shelf solutions\" in that area, solutions that often bring confusion in IT systems, when put in place by non level 4 people.",
            "title": "Level 4: IT Architecture"
        },
        {
            "location": "/articles/five-levels/#level-5-enterprise-architecture",
            "text": "The term \"Enterprise Architecture\" (EA) is at the center of a lot of discussions because the industry is not clear about what an enterprise architect is supposed to do. I will provide my own pragmatic vision (which can be challenged). An enterprise architect must primarily ensure that company strategic intentions are concretely declined into the IT systems through auditable projects and programs. Secondly he/she must ensure that the changes brought to the softwares can be managed by end users, that software changes are not destroying efficient business processes but improving them. This is quite a work that requires a lot of IT skills but also a good business understanding and enhanced communication skills.",
            "title": "Level 5: Enterprise Architecture"
        },
        {
            "location": "/articles/five-levels/#the-usual-maturity-path",
            "text": "One common path for IT-related designers or architects is to go from level 1 to level 5. Each level is requiring, following my experience, in average, at least 2 years of real life practice. Most often, when you work at a certain level, you have to consider also the impacts of what you are doing in all other lowest levels (that's why enterprise architects that never coded often make me uncomfortable, because they cannot imagine the consequences of their decisions).  Most people will find happiness in a certain layer and will excel in that layer because it fits their mind and taste and it is perfectly adapted to the level of complexity they like to manage. Without a proper project experience and without training and/or coaching, it is quite hard for people to change levels without having to work for months in real life at the concerned level.",
            "title": "The Usual Maturity Path"
        },
        {
            "location": "/articles/five-levels/#a-maturity-model-related-to-complexity",
            "text": "The model describes the capability to solve IT problems at the proper level, to manage the complexity of the IT problem with the proper approach. For instance, you can take a level 4 problem with a level 1 approach but you have 70% of chances of never solving the problem completely and 95% of chances to spend millions of dollars fixing \"things that don't work\". The \"things that don't work\" were created by people because of their problem solving approach (level 1 instead of level 4). Using the proper approach would never have generated those costs and troubles.",
            "title": "A Maturity Model Related to Complexity"
        },
        {
            "location": "/articles/five-levels/#what-is-the-required-level-of-maturity-for-my-project",
            "text": "Regarding this maturity model, executives should appreciate the level of maturity of their internal IT organization. They are then able to ask themselves the question: what's the adequate level of maturity required to achieve the project I want? Or, we can ask the question the other way round: what projects can I safely achieve considering the maturity of my internal IT organization? Those questions are crucial.  When I look at failed projects, I often see people having a maturity level between 1 and 3 trying to master a multi-million dollars IT projects requiring a level 5. Integrating external consultants can do the job but not in all cases. If your business is complex and quite specific, it is preferable to bet on internal skills in the long term for IT people to understand your business in depth. Moreover, who chooses the casting of your external consultants? Your internal IT organization, and few organizations are able to hire consultants that know more than they do.  This is exactly what happened when big companies got out of the mainframe. Mainframe teams were mostly level 1, some of them were level 2 (level 3 is not relevant in old mainframe application design and level 4 is often just about producing files for the outside or consuming them). Giving such a team an IT program with distributed applications in various technologies to build is doomed to failure, whatever the money and whatever the quality and dedication of people. For sure, with time and the appropriate training, most people can climb rungs in the ladder. But without time and training, people will keep on reproducing failures. Suppose the management is not aware of the core problem and people get fired for project failure, you can end up taking big outsourcing decisions based on a situation that nobody really understood.",
            "title": "What is the Required Level of Maturity for my Project?"
        },
        {
            "location": "/articles/five-levels/#it-industry-should-target-maturity",
            "text": "Because it is only \"soft\"-ware and not hardware, the IT industry seems to like the level 1 so much that every serious attempt to raise to level 2 or 3 is pushed back to level 1 by the IT providers. IT providers love to focus their customers on the \"code level\": because they cannot see the bigger picture and buy their products even if they don't need them.  If the IT industry created models for IT project management (like PMP), the increasing complexity of existing systems and new projects require the appropriate level of maturity in the project designers/architects teams.  The problem today, in most companies, is not only to \"code\" nor only to \"manage projects\": it is to also to maintain the running systems and make the huge amount of legacy applications evolve without risks and conforming to the company strategic intentions. Most IT systems are a pile of geological layers of outdated technological hypes. The code of today becomes the legacy of tomorrow. If some industries (like aviation) forecast their investments for decades, IT should begin to do the same, to work on the adequate long-term skills that are required to ensure durable and quality IT services.  IT industry should target maturity and maturity comes with the identification of our level of mastering of IT complexity.  ( April 2015 )",
            "title": "IT Industry Should Target Maturity"
        },
        {
            "location": "/articles/the-v2-vortex/",
            "text": "The V2 Vortex\n\n\n\n\nA lot of companies enter, at a certain moment in their life, into the \"V2 Vortex\u201d. In this article, we're going to detail the pattern of the harsh replacement of the V1 of the application by the V2.\n\n\nThe Case of Software Companies\n\n\nFor a lot of software companies that enter into the V2 Vortex, the history is similar: the V1 of the product was created long ago, for instance in the late 90s or in the early 2000s. It was a success. The company found its market. It has customers. All is great except that in 15 years, technology changed. And so did the competition.\n\n\nIn those software companies, we can often see a lack of software culture, and often, business-oriented people surfing on a tool that developed and gathered success. The R&D team is generally composed of IT people that preferred implementing new features for customers rather than making the development process change progressively to modernize slowly but surely the product.\n\n\nThe problem looks like a pattern: after a large amount of time, even the best buildings get old when they are not maintained. With years, it becomes difficult to implement new features; evolutions are slower to much slower, and the software bugs are more and more complicated to fix [1].\n\n\nSoftware is a complex activity because when things go wrong, the more customers, the more troubles, the more time and money spent in support, the less money to invest and the less time to fix. This is the dark side of the software moon, the one that enables selling plenty of times a software that was developed once. Multipliers work both ways.\n\n\nSo, one day, the CEO has to face the facts. \"We need a V2\", he says. \u201cWe can invest, I have cash\u201d.\n\n\nThat's when the trouble begins.\n\n\nWho Really Does The Maths?\n\n\nDoes the company have the right skills? Enough people? The right money?\n\n\nAs a consultant, I see that CEOs frequently don't run the maths, or maybe their calculator is bugged, or perhaps they are too optimistic.\n\n\nNew trendy technologies make them dream their product can be rewritten in 2 years maximum (it's always 2 years). For sure, they think current technology is much more productive that the old one they keep on using. And new developers can do so many pretty web stuff. The argument I hear also a lot is that the current software was refactored several times and so its global costs is much lower that the R&D team multiplied by the age of the software  (generally around 10 to 20 years).\n\n\nBut, let's do the maths together: let's suppose the R&D development team was around 4 FTE during all those years. The gross product weight would be 60 men years (MY) for a team that worked 15 years on a product. Let's suppose 1/3 of the charge is not relevant and is corresponding to the refactorings. This leads to 40 useful MY.\n\n\nGlobally, a good developer in Europe costs around 100 k\u20ac a year (with charges). That means that the software company should have approximately 4 M\u20ac available to realize this project. This investment is generally quite risky.\n\n\n40 MY of effort is possible in two years provided you have a team of 30 people (taking ramp-ups into account). For sure, the CEO of the company (or the CTO) is used to manage only 4 R&D people. So he rarely has the (project) management skills for such a big project. Let's suppose he reduces the team and targets 10 people for 4 years, he still has to train them for them to learn the business and to understand the existing product in order to be productive.\n\n\nBut where to find the right people? In IT service companies? This could be a bad move: IT service developer usually don't know how to produce good software; but they make it fast.\n\n\nMost CEOs will choose one of those options:\n\n\n\n\nBe optimistic and start with his 4 developers using new technologies. Things will probably not progressed as expected.\n\n\nOutsource because a service company can convince the CEO that with 1M\u20ac, you can create 2M\u20ac of software. Generally, the budget is consumed but the software is not produced (and not working).\n\n\nBelieve software bandits or software magicians that will promise the CEO that with their code generation MDA approach, they can develop his V2 software in less than 2 years \u2013 provided he pays upfront.\n\n\nDevelop a new product with a limited scope, for new customers, hoping to renew the old company success while fearing about the double team and the double recurring costs. But wouldn't it be a company startup within the company?\n\n\nSell the company when it is still worth something and before it's too late.\n\n\n\n\nIf we do the maths, we can be worried.\n\n\nMost of those CEOs will potentially try successively several options and fail. We can say they entered into the \"V2 Vortex\", a place where many software company go to die. Like the elephant cemetery, they cannot go back. Soon, they'll run out of cash and be obliged to find a more radical solution.\n\n\nThe V2 Vortex in Big Companies\n\n\nIn big companies, unfortunately, things are quite similar. Perhaps the application is much bigger (like a mainframe); perhaps it was not developed over 15 years but over 30; perhaps the development team is 100 people instead of 4; but the fact is here: they definitely are into the V2 Vortex.\n\n\nDoing the maths is really frightening on their side. 30 years with 100 FTE leads to 3,000 MY of effort. Let's apply the 1/3 optimistic ratio: the application weights 2,000 MY... For sure, perhaps they don't have the cash problem. But they have tried to replace their systems 3 or 4 times, and all projects failed. Indeed, they never really understood why. This is too scary to look into it. They also tried several options and spent a lot of cash, and the amount spent is at the size of the ambition.\n\n\nThey, also, would like to get rid of the problem in 2 years, but that would mean 1,000 people in the project team... Nah, not possible.\n\n\nIT service companies are not a big help with this kind of problem. Those applications are core business ones and they are connected to dozens of systems inside and outside their company. It is quite hard to even grasp the full picture.\n\n\nThey are in the V2 Vortex, and despite the fact that they're much bigger than the small software company we spoke about, the people in charge risk their job if they fail.\n\n\nThere is No Magic\n\n\nI must say: there is no magic. I wish I could solve their problems with a lot of money but I cannot. However, there is a path - but they never like it.\n\n\n\u201cWhen you have applications with thousands of function points, you have to think about it as an asset\u201d, I say to them. Ok, it is old, they did not maintain it enough, it is bugged, they are locked in ancient design constraints and old technologies, yes, it is the same for everybody. But it is their asset.\n\n\nMost of time, they can still sell it to customers (one day, they won't), or the application is still massively used (because it is core business). It still solves business issues. It still runs everyday for tons of people that can hate it while not being able to live without it.\n\n\nSo, as they would do with an old unmaintained asset that they have to keep, they have to \nmodernize\n it. They cannot let it go. They have to do what they should have done, a long time ago. And it becomes urgent.\n\n\nRewrite by Parts and Refactor\n\n\nNew technologies brought new tools and new ways of managing legacy code. My advice is to upgrade to new tools, to new source management, to continuous build and integration, to pure local development workstations, to automated testing. The old code must absolutely be ported on recent compilers and OS.\n\n\nSurprisingly, after all the previous failures, their development process is often the same old unproductive non agile process.\n\n\nThe hardest thing to do is to rewrite the software by parts, which implies code refactoring.\n\n\nThe final users of the applications will have to cope with a hybrid product for years, with some modernized parts and some old ones, the time for the team in place to modernize it completely.\n\n\nEnterprise Architecture and Transition Architecture\n\n\nBuilding The Functional Map, the Map to Other Systems and the Transformation Steps\n\n\nModernizing a complex software must be planned through the use of enterprise architecture techniques. As the system can be big in terms of functionality and/or can be connected to a lot of other systems, getting a complete map of the functionality and interfaces seems the best starting point. Archimate is the language for the task [2].\n\n\nWith Archimate, it is possible to represent the software progressive evolutions and the impacts on the user's processes. This is particularly important because some application modifications will have a serious impact on processes whereas others won't.\n\n\nIn terms of software, it is hard to migrate the core of an old application right away. Generally, the peripheric functions will be the easiest to replace. Basically, those functions often use data that, in the database, are not lined to any other data (leaves of the tree of tables).\n\n\nBy migrating function by function, and refactoring continuously the interfaces between the new part of the system and the old part of the system, the old part will become thinner while the new part will grow.\n\n\nTransition Architecture\n\n\nTechnically, a transition architecture must be determined. The current one was supposed to be abandonned for a new one, but if we suppose the application will be composed by both technologies at the same time, it is crucial to have a generic way of moving functions from one environment to the other.\n\n\nIdentifying what process is creating the data and what processes are consuming the data are important knowledge to get in order to do a smooth migration. The migration should enable to transfer the control of data from the old system to the new one progressively.\n\n\nSoftware is Made of People\n\n\nThe HR can be a problem. Let's face it: it is not so easy to convert people that were used for years to work in a certain paradigm in another. Some people will adapt and some won't. Moreover, developing in that phase of the life of the application requires skills that are generally not present in the original team.\n\n\nRecruitment is generally getting on the critical path.\n\n\nBecause software is made of people. If people are closed, the software will be closed; if they are open, it will be open. If their people are messy, the software will be a mess; if they are structured, so will their software.\n\n\nSo, to create a great software, you need to hire great people. And, once you got them, you have to cherish them because bright people can be insecure. And you don't want to lose them.\n\n\nThis change is hard is many context:\n\n\n\n\nIn small companies, it can bring a lot of issues due to feelings towards people (generally the original CTO is part of the founders and should be replaced);\n\n\nIn big companies, the application manager, even if he took in charge for decades an application, must generally be changed in order for new motivated people to take over and do the work.\n\n\n\n\nTabula Rasa (Rewriting) Versus Transformation\n\n\nTransformation Is Not Popular\n\n\nIn the software industry, we  generally damage our software assets when maintaining them. Then comes a time when we must transform the big software.\n\n\nSoftware transformation and refactoring is not very popular, for a lot of reasons:\n\n\n\n\nIt requires a lot of work on the existing system, to really analyze what it does, how it was designed, and what are his good and bad points;\n\n\nIt requires to enter into the fucntional details of the business, which many software engineers are reluctant to do, because they are more interested by technology than by functional aspects;\n\n\nIt implies mastering two environments for a long period of time, and one of them may be very old and not very appealing on a r\u00e9sum\u00e9;\n\n\nIt implies thinking at the software and IT architecture levels (3 and 4), even in a single application, which is not possible for many people [3].\n\n\n\n\nFor all those reasons, most people and managers prefer the \"tabula rasa\" solution. But, if we do the maths, we see that it is very often not an option.\n\n\nThe Rewriting Path\n\n\nThe rewriting path is not impossible, and it is sometimes the only option. It can be a success (even if many rewriting projects fail) but the conditions are numerous for it to succeed:\n\n\n\n\nThe budget must be very big, and so must be the business case to get a ROI in a reasonable delay;\n\n\nThe rewriting project must take into account the migration from the old system to the new one, the earlier possible in the project;\n\n\nThe architecture of the V2 product should respect the semantics of the business, because may old systems do;\n\n\nThe team must be great and work closely with the old system one;\n\n\nEvolutions should be limited in the old system;\n\n\nThe architecture of the new system must enable parallel developments.\n\n\n\n\nConclusion\n\n\nAfter decades of projects and auditing, most of the companies I saw facing the V2 vortex should have considered the transformation path rather than the rewriting path.\n\n\n\n\nNotes\n\n\n[1] - We will come back on technical debt in other parts of this book.\n\n\n[2] - Archimate resources can be found here: http://www.opengroup.org/subjectareas/enterprise/archimate.\n\n\n[3] - Refer to \nThe Five Levels Of Conceptual Maturity for IT Teams\n.\n\n\n(\nJune 2015, corrected November 2017\n)",
            "title": "The V2 Vortex"
        },
        {
            "location": "/articles/the-v2-vortex/#the-v2-vortex",
            "text": "A lot of companies enter, at a certain moment in their life, into the \"V2 Vortex\u201d. In this article, we're going to detail the pattern of the harsh replacement of the V1 of the application by the V2.",
            "title": "The V2 Vortex"
        },
        {
            "location": "/articles/the-v2-vortex/#the-case-of-software-companies",
            "text": "For a lot of software companies that enter into the V2 Vortex, the history is similar: the V1 of the product was created long ago, for instance in the late 90s or in the early 2000s. It was a success. The company found its market. It has customers. All is great except that in 15 years, technology changed. And so did the competition.  In those software companies, we can often see a lack of software culture, and often, business-oriented people surfing on a tool that developed and gathered success. The R&D team is generally composed of IT people that preferred implementing new features for customers rather than making the development process change progressively to modernize slowly but surely the product.  The problem looks like a pattern: after a large amount of time, even the best buildings get old when they are not maintained. With years, it becomes difficult to implement new features; evolutions are slower to much slower, and the software bugs are more and more complicated to fix [1].  Software is a complex activity because when things go wrong, the more customers, the more troubles, the more time and money spent in support, the less money to invest and the less time to fix. This is the dark side of the software moon, the one that enables selling plenty of times a software that was developed once. Multipliers work both ways.  So, one day, the CEO has to face the facts. \"We need a V2\", he says. \u201cWe can invest, I have cash\u201d.  That's when the trouble begins.",
            "title": "The Case of Software Companies"
        },
        {
            "location": "/articles/the-v2-vortex/#who-really-does-the-maths",
            "text": "Does the company have the right skills? Enough people? The right money?  As a consultant, I see that CEOs frequently don't run the maths, or maybe their calculator is bugged, or perhaps they are too optimistic.  New trendy technologies make them dream their product can be rewritten in 2 years maximum (it's always 2 years). For sure, they think current technology is much more productive that the old one they keep on using. And new developers can do so many pretty web stuff. The argument I hear also a lot is that the current software was refactored several times and so its global costs is much lower that the R&D team multiplied by the age of the software  (generally around 10 to 20 years).  But, let's do the maths together: let's suppose the R&D development team was around 4 FTE during all those years. The gross product weight would be 60 men years (MY) for a team that worked 15 years on a product. Let's suppose 1/3 of the charge is not relevant and is corresponding to the refactorings. This leads to 40 useful MY.  Globally, a good developer in Europe costs around 100 k\u20ac a year (with charges). That means that the software company should have approximately 4 M\u20ac available to realize this project. This investment is generally quite risky.  40 MY of effort is possible in two years provided you have a team of 30 people (taking ramp-ups into account). For sure, the CEO of the company (or the CTO) is used to manage only 4 R&D people. So he rarely has the (project) management skills for such a big project. Let's suppose he reduces the team and targets 10 people for 4 years, he still has to train them for them to learn the business and to understand the existing product in order to be productive.  But where to find the right people? In IT service companies? This could be a bad move: IT service developer usually don't know how to produce good software; but they make it fast.  Most CEOs will choose one of those options:   Be optimistic and start with his 4 developers using new technologies. Things will probably not progressed as expected.  Outsource because a service company can convince the CEO that with 1M\u20ac, you can create 2M\u20ac of software. Generally, the budget is consumed but the software is not produced (and not working).  Believe software bandits or software magicians that will promise the CEO that with their code generation MDA approach, they can develop his V2 software in less than 2 years \u2013 provided he pays upfront.  Develop a new product with a limited scope, for new customers, hoping to renew the old company success while fearing about the double team and the double recurring costs. But wouldn't it be a company startup within the company?  Sell the company when it is still worth something and before it's too late.   If we do the maths, we can be worried.  Most of those CEOs will potentially try successively several options and fail. We can say they entered into the \"V2 Vortex\", a place where many software company go to die. Like the elephant cemetery, they cannot go back. Soon, they'll run out of cash and be obliged to find a more radical solution.",
            "title": "Who Really Does The Maths?"
        },
        {
            "location": "/articles/the-v2-vortex/#the-v2-vortex-in-big-companies",
            "text": "In big companies, unfortunately, things are quite similar. Perhaps the application is much bigger (like a mainframe); perhaps it was not developed over 15 years but over 30; perhaps the development team is 100 people instead of 4; but the fact is here: they definitely are into the V2 Vortex.  Doing the maths is really frightening on their side. 30 years with 100 FTE leads to 3,000 MY of effort. Let's apply the 1/3 optimistic ratio: the application weights 2,000 MY... For sure, perhaps they don't have the cash problem. But they have tried to replace their systems 3 or 4 times, and all projects failed. Indeed, they never really understood why. This is too scary to look into it. They also tried several options and spent a lot of cash, and the amount spent is at the size of the ambition.  They, also, would like to get rid of the problem in 2 years, but that would mean 1,000 people in the project team... Nah, not possible.  IT service companies are not a big help with this kind of problem. Those applications are core business ones and they are connected to dozens of systems inside and outside their company. It is quite hard to even grasp the full picture.  They are in the V2 Vortex, and despite the fact that they're much bigger than the small software company we spoke about, the people in charge risk their job if they fail.",
            "title": "The V2 Vortex in Big Companies"
        },
        {
            "location": "/articles/the-v2-vortex/#there-is-no-magic",
            "text": "I must say: there is no magic. I wish I could solve their problems with a lot of money but I cannot. However, there is a path - but they never like it.  \u201cWhen you have applications with thousands of function points, you have to think about it as an asset\u201d, I say to them. Ok, it is old, they did not maintain it enough, it is bugged, they are locked in ancient design constraints and old technologies, yes, it is the same for everybody. But it is their asset.  Most of time, they can still sell it to customers (one day, they won't), or the application is still massively used (because it is core business). It still solves business issues. It still runs everyday for tons of people that can hate it while not being able to live without it.  So, as they would do with an old unmaintained asset that they have to keep, they have to  modernize  it. They cannot let it go. They have to do what they should have done, a long time ago. And it becomes urgent.",
            "title": "There is No Magic"
        },
        {
            "location": "/articles/the-v2-vortex/#rewrite-by-parts-and-refactor",
            "text": "New technologies brought new tools and new ways of managing legacy code. My advice is to upgrade to new tools, to new source management, to continuous build and integration, to pure local development workstations, to automated testing. The old code must absolutely be ported on recent compilers and OS.  Surprisingly, after all the previous failures, their development process is often the same old unproductive non agile process.  The hardest thing to do is to rewrite the software by parts, which implies code refactoring.  The final users of the applications will have to cope with a hybrid product for years, with some modernized parts and some old ones, the time for the team in place to modernize it completely.",
            "title": "Rewrite by Parts and Refactor"
        },
        {
            "location": "/articles/the-v2-vortex/#enterprise-architecture-and-transition-architecture",
            "text": "",
            "title": "Enterprise Architecture and Transition Architecture"
        },
        {
            "location": "/articles/the-v2-vortex/#building-the-functional-map-the-map-to-other-systems-and-the-transformation-steps",
            "text": "Modernizing a complex software must be planned through the use of enterprise architecture techniques. As the system can be big in terms of functionality and/or can be connected to a lot of other systems, getting a complete map of the functionality and interfaces seems the best starting point. Archimate is the language for the task [2].  With Archimate, it is possible to represent the software progressive evolutions and the impacts on the user's processes. This is particularly important because some application modifications will have a serious impact on processes whereas others won't.  In terms of software, it is hard to migrate the core of an old application right away. Generally, the peripheric functions will be the easiest to replace. Basically, those functions often use data that, in the database, are not lined to any other data (leaves of the tree of tables).  By migrating function by function, and refactoring continuously the interfaces between the new part of the system and the old part of the system, the old part will become thinner while the new part will grow.",
            "title": "Building The Functional Map, the Map to Other Systems and the Transformation Steps"
        },
        {
            "location": "/articles/the-v2-vortex/#transition-architecture",
            "text": "Technically, a transition architecture must be determined. The current one was supposed to be abandonned for a new one, but if we suppose the application will be composed by both technologies at the same time, it is crucial to have a generic way of moving functions from one environment to the other.  Identifying what process is creating the data and what processes are consuming the data are important knowledge to get in order to do a smooth migration. The migration should enable to transfer the control of data from the old system to the new one progressively.",
            "title": "Transition Architecture"
        },
        {
            "location": "/articles/the-v2-vortex/#software-is-made-of-people",
            "text": "The HR can be a problem. Let's face it: it is not so easy to convert people that were used for years to work in a certain paradigm in another. Some people will adapt and some won't. Moreover, developing in that phase of the life of the application requires skills that are generally not present in the original team.  Recruitment is generally getting on the critical path.  Because software is made of people. If people are closed, the software will be closed; if they are open, it will be open. If their people are messy, the software will be a mess; if they are structured, so will their software.  So, to create a great software, you need to hire great people. And, once you got them, you have to cherish them because bright people can be insecure. And you don't want to lose them.  This change is hard is many context:   In small companies, it can bring a lot of issues due to feelings towards people (generally the original CTO is part of the founders and should be replaced);  In big companies, the application manager, even if he took in charge for decades an application, must generally be changed in order for new motivated people to take over and do the work.",
            "title": "Software is Made of People"
        },
        {
            "location": "/articles/the-v2-vortex/#tabula-rasa-40rewriting41-versus-transformation",
            "text": "",
            "title": "Tabula Rasa (Rewriting) Versus Transformation"
        },
        {
            "location": "/articles/the-v2-vortex/#transformation-is-not-popular",
            "text": "In the software industry, we  generally damage our software assets when maintaining them. Then comes a time when we must transform the big software.  Software transformation and refactoring is not very popular, for a lot of reasons:   It requires a lot of work on the existing system, to really analyze what it does, how it was designed, and what are his good and bad points;  It requires to enter into the fucntional details of the business, which many software engineers are reluctant to do, because they are more interested by technology than by functional aspects;  It implies mastering two environments for a long period of time, and one of them may be very old and not very appealing on a r\u00e9sum\u00e9;  It implies thinking at the software and IT architecture levels (3 and 4), even in a single application, which is not possible for many people [3].   For all those reasons, most people and managers prefer the \"tabula rasa\" solution. But, if we do the maths, we see that it is very often not an option.",
            "title": "Transformation Is Not Popular"
        },
        {
            "location": "/articles/the-v2-vortex/#the-rewriting-path",
            "text": "The rewriting path is not impossible, and it is sometimes the only option. It can be a success (even if many rewriting projects fail) but the conditions are numerous for it to succeed:   The budget must be very big, and so must be the business case to get a ROI in a reasonable delay;  The rewriting project must take into account the migration from the old system to the new one, the earlier possible in the project;  The architecture of the V2 product should respect the semantics of the business, because may old systems do;  The team must be great and work closely with the old system one;  Evolutions should be limited in the old system;  The architecture of the new system must enable parallel developments.",
            "title": "The Rewriting Path"
        },
        {
            "location": "/articles/the-v2-vortex/#conclusion",
            "text": "After decades of projects and auditing, most of the companies I saw facing the V2 vortex should have considered the transformation path rather than the rewriting path.",
            "title": "Conclusion"
        },
        {
            "location": "/articles/the-v2-vortex/#notes",
            "text": "[1] - We will come back on technical debt in other parts of this book.  [2] - Archimate resources can be found here: http://www.opengroup.org/subjectareas/enterprise/archimate.  [3] - Refer to  The Five Levels Of Conceptual Maturity for IT Teams .  ( June 2015, corrected November 2017 )",
            "title": "Notes"
        },
        {
            "location": "/articles/various-stages/",
            "text": "The Various Stages of Digital Transformation\n\n\n\n\nPresentation done in Airbus Helicopters for the PMI France Chapter.\n\n\nClick to see the \nPrezi presentation\n.\n\n\n(\nJune 2015\n)",
            "title": "The Various Stages of Digital Transformation"
        },
        {
            "location": "/articles/various-stages/#the-various-stages-of-digital-transformation",
            "text": "Presentation done in Airbus Helicopters for the PMI France Chapter.  Click to see the  Prezi presentation .  ( June 2015 )",
            "title": "The Various Stages of Digital Transformation"
        },
        {
            "location": "/articles/about-rest/",
            "text": "Considerations About Rest And Web Services\n\n\n \n\n\nIt's been a very long time since I've been explaining this to a lot of people and maybe today I should try to put the full explanation on paper.\n\n\nA Bit Of History\n\n\nWhen the REST concept was published, sometimes around 2001, I was in a middleware team in a big software company. In that team, we were accustomed to use business services for more than a decade.\n\n\nIndeed, the term \"web service\" or \"SOA\" was not existing at the time. But, in many business domains such as airline commercial business, banking, insurance, etc., many people knew what was RPC and were using it extensively to communicate between systems.\n\n\nRPC means Remote Procedure Call. RPC concept was introduced to me with the DCE (\nDistributed Computing Environment\n). DCE was a very powerful standard that were never completely used, as far as I know, but it explained in great details the basis of interoperability between systems. For sure, the standard was only making a synthesis of ideas that were much older.\n\n\nWhat Is RPC?\n\n\nInteroperability Contract\n\n\nFundamentally RPC is, like it is said in its name, a remote procedure call.\n\n\nTo understand the concept, let's imagine 2 programs that want to communicate, first program being \nA\n and second being \nB\n. \nB\n will publish an API in order to be called. In most procedural programming languages (like C), if the two programs are located on the same machine, \nA\n can call the API of \nB\n (see the top of Figure 1).\n\n\nFigure 1: Local Procedure Call and Remote Procedure Call\n\n\nThe idea of interoperability in RPC is that, if \nB\n is located in a remote machine (or a remote process), \nB\n should not change when invoked by \nA\n. On the other side, \nA\n should not change in its invocation of \nB\n interface. So \nA\n will call a \nB\n interface locally to its system, this interface hiding a client stub that will wrap/serialize data in a certain format to be sent on the wire; on the machine hosting \nB\n, there will be a server stub unwraping/unserializing data to call locally the \nB\n interface.\n\n\nFor sure, in order to work in a complex network, the message sent from \nA\n will have to fond is route to the machine hosting \nB\n. We have here all the elements of the client-server architecture.\n\n\nThe Notion Of \"Verb\"\n\n\nWell, RPC is a bit more than that. Because, when calling a remote procedure (or function), semantically, we call a remote system asking it to do something (the B API) with the input data that we provide (the request). We expect data back (the response).\n\n\nThis is/was called a \"verb\". \nA\n says to \nB\n: \"perform \nB-interface\n contract with my input data and give me back the contract expected output data\".\n\n\nTrends passed on many parameters:\n\n\n\n\n\n\nThe protocols used changed,\n\n\n\n\n\n\nThe addressing schemes changed,\n\n\n\n\n\n\nThe format of the data changed (from many proprietary formats or Edifact, to XML to JSON).\n\n\n\n\n\n\nBut the fact is, in most businesses proposing a certain degree of business complexity, RPC is still there. Most often, verbs are expressing an \"action\" semantic and requests are proposing data trees, so as responses.\n\n\nNo Assumptions on the Technology Used\n\n\nWe must notice that RPC does not make any assumption on the technology used by the server (the one that implements the \nB\n interface).\n\n\nThe contract is limited to a verb, and two trees of data, one for the request and one for the response. We could note the contract as being: \nA\n uses \nresponse = B-interface(request)\n .\n\n\nThe Corba Failed Attempt\n\n\nPrinciple\n\n\nIn the 90s, the objet-oriented programming (OOP) being trendy, the intention behind \nCorba\n was born. The idea was to generalize the object notions to a distributed world. The logic consequence was to imagine a client-server protocol for objects to communicate between one another.\n\n\nThe principle is simple: an object \nA\n calls locally a method \nm(...)\n on an object \nB\n. If we imagine the \nB\n instance as being remote, the idea is the same than RPC: The method should have a client and server stub.\n\n\nFigure 1: Local Method Invocation and Remote Method Invocation\n\n\nThe fact is, despite the fact that it looks like RPC, this model is radically different from it:\n\n\n\n\nIt supposes that the remote system is object oriented;\n\n\nIt supposes that the remote system is stateful (the \nB\n instance must exist for its method to be called);\n\n\nIt supposes an addressing system to find the reference of the instance of B;\n\n\nThe contract of the method is not self sufficient, indeed, conceptually \nA\n asks for: \nresponse = state(B).m(request)\n which introduce some uncertainty on the call (because it depends on B state);\n\n\nThe contract is supposing the state of B should be managed differently from the request parameters, and so, it puts a different semantic weight on B that should be a \"first-class entity\" whereas the request parameters are considered as \"second class entities\".\n\n\n\n\nThis way of thinking distributed systems is leading to consider that the network offers \"objects that can provide services on themselves\", instead of providing simple services (i.e. distributed functions).\n\n\nIn 2017's language, we could say that CORBA is proposing a \"resource-oriented architecture\".\n\n\nThe Drawbacks of the ORB Approach\n\n\nAn ORB (Object Request Broker) is a middleware that enables to manage those distributed objects, their remote invocation and the required addressing schema.\n\n\nThe main drawback of the ORB approach is that a service can be, semantically, much more complex than calling one method of one object. Indeed, if we call a method on an object, we can imagine that the object will process this method on its state, or that it will process it considering its aggregation (\"facade\" pattern).\n\n\nFor services, we do not make any assumptions of the kind. We just call a function that is supposed to do something with the inbound parameters that we provide (inbound parameters that are generally some kind of tree of objects having or not links together).\n\n\nCertainly, we can \"twist\" Corba in order to make it look like RPC: we can use objects that do not contain state and that have \"methods\" that are indeed just \"procedures\".\n\n\nBut the addressing repository will have to manage object instance addresses instead of knowing the service location.\n\n\nAn Idea That Keeps Coming Back\n\n\nThis idea keeps coming back. Rest architecture can be seen as a reformulation of Corba principles:\n\n\n\n\nResources are benefiting from an addressing scheme (URI);\n\n\nResources have a name (class name), they are identified through their instance and they publish methods;\n\n\nInvoking a service is indeed a remote method invocation.\n\n\n\n\nMoreover, a strange design option is taken in the Rest specification: the presentation layer is binded on the underlying network protocol (http) in a very hard way. Indeed, the only verbs that seem to be invokable are CRUD verbs (CRUD standing for Create Retrieve Update Delete). \n\n\nWe can also see this ORB principle applied, in a weaker way, in WSDL. When SOAP web services were really in the spirit of RPC, WSDL standard groups transactions in a way that is sometimes near to the idea of a group of methods operating on a remote object.\n\n\nWe can note also in WSDL, a very strange hard biding on the underlying protocol (the WSDL schema integrates information from several OSI layers which is bad).\n\n\nRest was those last years recently put in front of the scene due to IT marketing and the creation of \nSwagger.io\n\n\nThe Core Problem of Resource Orientation\n\n\nResource orientation, like in an ORB, is semantically a very hard restriction of RPC. RPC is a way of calling a service that will transform a graph of data in another graph of data. In the ORB approach, we call a method on a object, which is very restrictive.\n\n\nThis can work if the semantics of the business is simple. If my business objects are modeled properly by one class, then maybe an ORB can work. If the business objects I have to model need several classes interacting together in a complex manner, then using an ORB will be a real pain. When a service can transform my graph of objects into another graph of objects, an ORB will force me to \"adapt\" my semantics to a pure technical object-oriented method call.\n\n\nFor sure, this is true for Rest, as it was true for Corba. For pure Rest, it is even worse: being able to consider that only the http verbs (network layer) should be used to express the semantics of the presentation layer, the semantics of the \"functional verbs\" seems even more restrictive than Corba.\n\n\nFor social networks like Twitter or Facebook, it seems to work. I can define a Rest API providing all the services offered by the platform. For a business application, we generally cannot use Rest because the constraints (RMI with CRUD verbs) are too strong.\n\n\nAgain, like in Corba, we can cheat: we can use \"almost Rest\" and have a request body with JSON. That turns the method invocation into an almost-service. We can also include in the request a service name, but that is pretending to implement Rest and doing JSON-RPC-like behind the scene.\n\n\nConclusion\n\n\nRest is very practical for the applications which semantics is simple and can be adapted to two constraints:\n\n\n\n\nA resource oriented API (pushing for services to be RMI);\n\n\nA verb semantic limited to a variation of CRUD.\n\n\n\n\nFor other applications, like business applications [1], we believe things never changed for decades. Before SOA, RPC services were existing. They were the same stuff than JSON-RPC like transactions today.\n\n\nService orientation is a much more general way of implementing service distribution than ORB concepts. In particular, service orientation does not presuppose that the remote server sees the data as the caller  does. The service signature is an agreed contract for both the client and the server to communicate together, but each of them can restructure data as they want, in an object oriented way or not.\n\n\nSo, my advice is not to force yourself to implement a Rest API to your application because it is trendy, but to do it only if your business semantics enables it.\n\n\nSee Also\n\n\n\n\nAbout GraphQL\n\n\n\n\nNotes\n\n\n[1] - In some businesses, like the airline one (standardized by \nIATA\n), services have big requests and big responses for decades because the business requires it.\n\n\n(\nDecember 2017\n)",
            "title": "Considerations About Rest And Web Services"
        },
        {
            "location": "/articles/about-rest/#considerations-about-rest-and-web-services",
            "text": "It's been a very long time since I've been explaining this to a lot of people and maybe today I should try to put the full explanation on paper.",
            "title": "Considerations About Rest And Web Services"
        },
        {
            "location": "/articles/about-rest/#a-bit-of-history",
            "text": "When the REST concept was published, sometimes around 2001, I was in a middleware team in a big software company. In that team, we were accustomed to use business services for more than a decade.  Indeed, the term \"web service\" or \"SOA\" was not existing at the time. But, in many business domains such as airline commercial business, banking, insurance, etc., many people knew what was RPC and were using it extensively to communicate between systems.  RPC means Remote Procedure Call. RPC concept was introduced to me with the DCE ( Distributed Computing Environment ). DCE was a very powerful standard that were never completely used, as far as I know, but it explained in great details the basis of interoperability between systems. For sure, the standard was only making a synthesis of ideas that were much older.",
            "title": "A Bit Of History"
        },
        {
            "location": "/articles/about-rest/#what-is-rpc",
            "text": "",
            "title": "What Is RPC?"
        },
        {
            "location": "/articles/about-rest/#interoperability-contract",
            "text": "Fundamentally RPC is, like it is said in its name, a remote procedure call.  To understand the concept, let's imagine 2 programs that want to communicate, first program being  A  and second being  B .  B  will publish an API in order to be called. In most procedural programming languages (like C), if the two programs are located on the same machine,  A  can call the API of  B  (see the top of Figure 1).  Figure 1: Local Procedure Call and Remote Procedure Call  The idea of interoperability in RPC is that, if  B  is located in a remote machine (or a remote process),  B  should not change when invoked by  A . On the other side,  A  should not change in its invocation of  B  interface. So  A  will call a  B  interface locally to its system, this interface hiding a client stub that will wrap/serialize data in a certain format to be sent on the wire; on the machine hosting  B , there will be a server stub unwraping/unserializing data to call locally the  B  interface.  For sure, in order to work in a complex network, the message sent from  A  will have to fond is route to the machine hosting  B . We have here all the elements of the client-server architecture.",
            "title": "Interoperability Contract"
        },
        {
            "location": "/articles/about-rest/#the-notion-of-verb",
            "text": "Well, RPC is a bit more than that. Because, when calling a remote procedure (or function), semantically, we call a remote system asking it to do something (the B API) with the input data that we provide (the request). We expect data back (the response).  This is/was called a \"verb\".  A  says to  B : \"perform  B-interface  contract with my input data and give me back the contract expected output data\".  Trends passed on many parameters:    The protocols used changed,    The addressing schemes changed,    The format of the data changed (from many proprietary formats or Edifact, to XML to JSON).    But the fact is, in most businesses proposing a certain degree of business complexity, RPC is still there. Most often, verbs are expressing an \"action\" semantic and requests are proposing data trees, so as responses.",
            "title": "The Notion Of \"Verb\""
        },
        {
            "location": "/articles/about-rest/#no-assumptions-on-the-technology-used",
            "text": "We must notice that RPC does not make any assumption on the technology used by the server (the one that implements the  B  interface).  The contract is limited to a verb, and two trees of data, one for the request and one for the response. We could note the contract as being:  A  uses  response = B-interface(request)  .",
            "title": "No Assumptions on the Technology Used"
        },
        {
            "location": "/articles/about-rest/#the-corba-failed-attempt",
            "text": "",
            "title": "The Corba Failed Attempt"
        },
        {
            "location": "/articles/about-rest/#principle",
            "text": "In the 90s, the objet-oriented programming (OOP) being trendy, the intention behind  Corba  was born. The idea was to generalize the object notions to a distributed world. The logic consequence was to imagine a client-server protocol for objects to communicate between one another.  The principle is simple: an object  A  calls locally a method  m(...)  on an object  B . If we imagine the  B  instance as being remote, the idea is the same than RPC: The method should have a client and server stub.  Figure 1: Local Method Invocation and Remote Method Invocation  The fact is, despite the fact that it looks like RPC, this model is radically different from it:   It supposes that the remote system is object oriented;  It supposes that the remote system is stateful (the  B  instance must exist for its method to be called);  It supposes an addressing system to find the reference of the instance of B;  The contract of the method is not self sufficient, indeed, conceptually  A  asks for:  response = state(B).m(request)  which introduce some uncertainty on the call (because it depends on B state);  The contract is supposing the state of B should be managed differently from the request parameters, and so, it puts a different semantic weight on B that should be a \"first-class entity\" whereas the request parameters are considered as \"second class entities\".   This way of thinking distributed systems is leading to consider that the network offers \"objects that can provide services on themselves\", instead of providing simple services (i.e. distributed functions).  In 2017's language, we could say that CORBA is proposing a \"resource-oriented architecture\".",
            "title": "Principle"
        },
        {
            "location": "/articles/about-rest/#the-drawbacks-of-the-orb-approach",
            "text": "An ORB (Object Request Broker) is a middleware that enables to manage those distributed objects, their remote invocation and the required addressing schema.  The main drawback of the ORB approach is that a service can be, semantically, much more complex than calling one method of one object. Indeed, if we call a method on an object, we can imagine that the object will process this method on its state, or that it will process it considering its aggregation (\"facade\" pattern).  For services, we do not make any assumptions of the kind. We just call a function that is supposed to do something with the inbound parameters that we provide (inbound parameters that are generally some kind of tree of objects having or not links together).  Certainly, we can \"twist\" Corba in order to make it look like RPC: we can use objects that do not contain state and that have \"methods\" that are indeed just \"procedures\".  But the addressing repository will have to manage object instance addresses instead of knowing the service location.",
            "title": "The Drawbacks of the ORB Approach"
        },
        {
            "location": "/articles/about-rest/#an-idea-that-keeps-coming-back",
            "text": "This idea keeps coming back. Rest architecture can be seen as a reformulation of Corba principles:   Resources are benefiting from an addressing scheme (URI);  Resources have a name (class name), they are identified through their instance and they publish methods;  Invoking a service is indeed a remote method invocation.   Moreover, a strange design option is taken in the Rest specification: the presentation layer is binded on the underlying network protocol (http) in a very hard way. Indeed, the only verbs that seem to be invokable are CRUD verbs (CRUD standing for Create Retrieve Update Delete).   We can also see this ORB principle applied, in a weaker way, in WSDL. When SOAP web services were really in the spirit of RPC, WSDL standard groups transactions in a way that is sometimes near to the idea of a group of methods operating on a remote object.  We can note also in WSDL, a very strange hard biding on the underlying protocol (the WSDL schema integrates information from several OSI layers which is bad).  Rest was those last years recently put in front of the scene due to IT marketing and the creation of  Swagger.io",
            "title": "An Idea That Keeps Coming Back"
        },
        {
            "location": "/articles/about-rest/#the-core-problem-of-resource-orientation",
            "text": "Resource orientation, like in an ORB, is semantically a very hard restriction of RPC. RPC is a way of calling a service that will transform a graph of data in another graph of data. In the ORB approach, we call a method on a object, which is very restrictive.  This can work if the semantics of the business is simple. If my business objects are modeled properly by one class, then maybe an ORB can work. If the business objects I have to model need several classes interacting together in a complex manner, then using an ORB will be a real pain. When a service can transform my graph of objects into another graph of objects, an ORB will force me to \"adapt\" my semantics to a pure technical object-oriented method call.  For sure, this is true for Rest, as it was true for Corba. For pure Rest, it is even worse: being able to consider that only the http verbs (network layer) should be used to express the semantics of the presentation layer, the semantics of the \"functional verbs\" seems even more restrictive than Corba.  For social networks like Twitter or Facebook, it seems to work. I can define a Rest API providing all the services offered by the platform. For a business application, we generally cannot use Rest because the constraints (RMI with CRUD verbs) are too strong.  Again, like in Corba, we can cheat: we can use \"almost Rest\" and have a request body with JSON. That turns the method invocation into an almost-service. We can also include in the request a service name, but that is pretending to implement Rest and doing JSON-RPC-like behind the scene.",
            "title": "The Core Problem of Resource Orientation"
        },
        {
            "location": "/articles/about-rest/#conclusion",
            "text": "Rest is very practical for the applications which semantics is simple and can be adapted to two constraints:   A resource oriented API (pushing for services to be RMI);  A verb semantic limited to a variation of CRUD.   For other applications, like business applications [1], we believe things never changed for decades. Before SOA, RPC services were existing. They were the same stuff than JSON-RPC like transactions today.  Service orientation is a much more general way of implementing service distribution than ORB concepts. In particular, service orientation does not presuppose that the remote server sees the data as the caller  does. The service signature is an agreed contract for both the client and the server to communicate together, but each of them can restructure data as they want, in an object oriented way or not.  So, my advice is not to force yourself to implement a Rest API to your application because it is trendy, but to do it only if your business semantics enables it.",
            "title": "Conclusion"
        },
        {
            "location": "/articles/about-rest/#see-also",
            "text": "About GraphQL",
            "title": "See Also"
        },
        {
            "location": "/articles/about-rest/#notes",
            "text": "[1] - In some businesses, like the airline one (standardized by  IATA ), services have big requests and big responses for decades because the business requires it.  ( December 2017 )",
            "title": "Notes"
        },
        {
            "location": "/articles/graphql-web-services/",
            "text": "GraphQL And Classic Web Services\n\n\n \n\n\nFacebook produced the \nGraphQL specification\n in order to be an alternative to Rest.\n\n\nWe already explained \nwhat we thought of Rest services\n.\n\n\nGraphQL appears to us as a new way to do the same thing that what the industry is doing for decades.\n\n\nThis article is an analysis of GraphQL principles, their consequences and a comparison with traditional ways of doing web services.\n\n\nThe Basics of Services\n\n\nVarious Standards, Same Spirit\n\n\nThe underlying basis of services is RPC (Remote Procedure Call, \nsee the dedicated article on the subject\n). For decades, this concept is reinvented again and again with various flavors.\n\n\nApart from RPC protocols or proprietary protocols (like the \nTuxedo\n ones), \nEdifact\n was for a long time an industry standard in some business domains like travel, banking, medical and logistics. Edifact was replaced by \nXML\n then by \nJSON\n.\n\n\nEvery of those standards aimed to provide the same functionality: exchanging structured data between two different systems, probably implemented with different programming languages and/or paradigms, and having a client system benefiting from a service provided by a server system.\n\n\nBasic Requirements For a Service Oriented Language\n\n\nThe first requirement is to have a \"verb\", meaning the name of the \"transaction\" we expect the remote system to process.\n\n\nThe second requirement is to have a type system to be able to send structured data under the form of a tree of objects. This is generally achieved by proposing basic types (like string, integer, float, etc.) and the capability of defining fields that have those properties and that can be grouped into classes. Classes will generally have to indicate what fields are required and what fields are optional. This is generally done within the context of one particular transaction.\n\n\nOnce classes are defined, they can be grouped in larger structures or other classes to define groups of object structures than define larger entities, most of the time, better suitable to express the business semantics.\n\n\nOptionally, those definitions are expressed in \"grammars\" or \"schemas\". This is the case in RPC systems, in Edifact and in XML, and this is optional for JSON (even if the \nJSON schema\n specification is very usable). Note that, even if there is no explicit schema mechanism, the fact of defining objects and structures leads to the use of implicit schemas. The difference is really what part of the code is \"validating\" that the messages are well formed:\n\n\n\n\nWith an explicit schema definition, we can generate the validation code and create a kind of serializer/unserializer layer (wrapper/unwrapper or proxy/stub layer);\n\n\nWithout an explicit schema definition, the validation code is generally hand written.\n\n\n\n\nAll those requirements are defined in the \nOSI model\n in the \"presentation layer\".\n\n\nDon't forget the protocol\n\n\nThe last requirement is to have some kind of underlying protocol to:\n\n\n\n\nSend and receive messages between systems;\n\n\nCreate a naming convention that enables to use the name of the service (or name of the \"verb\");\n\n\nFind the network configuration that enable to route the request message to the proper entry point;\n\n\nPossibly associate the request and the response with identifiers;\n\n\nPossibly include information about the sender and its security credentials.\n\n\n\n\nOnce again, the \nOSI model\n defines a certain split of responsibilities between all those requirements. Quickly explained:\n\n\n\n\nThe transport layer (layer 4) is defining the addressing scheme and primitives that enables several systems to communicate together;\n\n\nThe session layer (layer 5) enables to manage the sessions and conversations between the client and the server; this layer manages the succession of names messages that the two peers will exchange;\n\n\nThe presentation layer (layer 6) manages the content of data, their structure and their values.\n\n\n\n\nIn the current JSON world, \nJSON-RPC\n is presenting a very basic protocol that can manage the basic single statement and request response messages.\n\n\nGraphQL Answers To Service Oriented Requirements\n\n\nGraphQL seems to us as a new flavor of the same old story. It brings some interesting new stuff (and we will come back on that), but the important point is that we can implement traditional services with this language.\n\n\nThe \"Verb\"\n\n\nEach GraphQL service is an named \nhttp\n entry point. The \nget\n method will be used to access the \nquery\n part of the service and the \npost\n method will access the \nmutation\n part of it.\n\n\nThis looks like Rest but this is rather different, because the idea seems not to access to a single resource (even if it is possible), but to perform a service.\n\n\nThe Type System\n\n\nGraphQL proposes an extended type system that proposes:\n\n\n\n\nBasic types,\n\n\nClasses,\n\n\nGroups of classes,\n\n\nFragments (kind of filters on classes),\n\n\nQueries.\n\n\n\n\nWith all the available semantics, it is very easy to implement all the existing web services that are used currently in the business.\n\n\nMoreover, it is possible to have a richer interface on existing services and a more adaptative way of performing transactions.\n\n\nThis seems rather promising for mobile or chatbot accesses for instance. Those two UIs manipulate a small amount of data and may require some server services to adapt their reply to their particular use.\n\n\nFor sure, we could argue saying: with standard web services in JSON, as a client, we can always extract the subset of the response that is interesting to us. The fact is this feature goes beyond the subset approach and enables to ask the server to adapt the contents of its response to the client.\n\n\nAnd The Protocol?\n\n\nIn the Web, the protocol is \nhttp\n and the approach seems inspired by both Rest and Ajax kind of calls.\n\n\nA Promising Standard\n\n\nAn Intermediate Between JSON-RPC and Rest?\n\n\nGraphQL seems to target the intermediate path between JSON-RPC (which is a simplified SOAP) and Rest. The fact of publishing a schema enables both approaches:\n\n\n\n\nPerform elaborated services (like before);\n\n\nAccess business objects (like the Rest or CRUD approach).\n\n\n\n\nGraphQL seems to propose a way to benefit from both approaches.\n\n\nThe fact of being able to perform complex queries (with a lot of flavours) also adds intelligence to the protocol, an intelligence driven by the client.\n\n\nBut What Impact On The Server Design?\n\n\nThis seems very interesting but the problem caused by this new protocol is located on the backend part.\n\n\nSome questions must be answered:\n\n\n\n\nWhat kind of models and/or application this protocol is targeting? All applications? Applications that should be able to disclose a part of their model?\n\n\nDo we know the extend of the server impacts in terms of design?\n\n\n\n\nModel Your Business Domain As A Graph\n\n\nWe are entering here into delicate topics.\n\n\nFor those how already performed some semantic modeling with \nRDF\n, \nRDFS\n or \nOWL\n, I think you're convinced about the fact that we can model a business domain with a graph.\n\n\nFor those who already used graph databases such as \nNeo\n or \nOrient\n in Big Data use cases or fraud recognition or social applications, may be convinced that a business model can be modeled as a graph.\n\n\nBut for the vast majority of IT people, this assertion is not obvious.\n\n\nWell, we'll come back on this particular topic in future articles, so we'll take for granted that a business domain can be modeled with a graph.\n\n\nGraph Data Means Graph Database\n\n\nWhat Facebook is proposing us is a protocol that enables to query the internal structure of the graph of data.\n\n\nSo, the first consequences seem to be:\n\n\n\n\nYour data are modeled as a graph,\n\n\nYou probably use a graph database of some sort.\n\n\n\n\nConsequently, your GraphQL schema is more or less the same thing as your graph database schema (when this sort of things exists such as in \nOrientDB\n).\n\n\nOpen Heart Data Model\n\n\nThe second consequence is also double:\n\n\n\n\nYou published your graph business model to your clients;\n\n\nYou use your database model as the presentation layer.\n\n\n\n\nThis second point is very problematic. Indeed, it explains why the protocol is proposing so much complexity in some of its part: because it is supposed to be, at the same time, a protocol and a graph query language (like \nCypher\n).\n\n\nIn a certain sense, it is supposed to be the new \nSQL*Net\n or \nODBC\n but for graphs.\n\n\nAnd this is where the approach is questionable.\n\n\nThe Myth Of The Single Representation Of Reality\n\n\nThere are many myths in IT, and this one is a big one. Many people believe that there is only one way of representing the reality-theirs, indeed. And the reality is, they are wrong.\n\n\nNote that, throughout the history of science, many great scientists had the same debate as the \nintuitionist debate of the beginning of the 20th Century\n.\n\n\nAny software designer that worked sufficiently in the domain should have seen several facts:\n\n\n\n\nWhatever the efforts and the design patterns used, you cannot predict how the business will evolve;\n\n\nTwo designer confronted to the same problem will produce two different designs.\n\n\n\n\nIndeed, design is very subjective and, for a lot of reasons we won't explain here, it is not a bad thing that there is no unique model for a particular business domain, on the contrary.\n\n\nWhen Client and Server Share Everything\n\n\nGraphQL is proposing a way for the customer to be very near from its server, so near that its data model is perfectly known, right from the client.\n\n\nThis can be very useful when you work with yourself, when you develop your own client and server in a kind of symbiosis. If you are on an Internet application, you also must know that you core model will be known from the outside, which can be a problems in terms of IP and in terms of security.\n\n\nSo:\n\n\n\n\nGraphQL is OK if your API is \ninternal\n;\n\n\nGraphQL seems not OK if your API is external.\n\n\n\n\nPublishing a GraphQL API\n\n\nWell, if you want to publish a GraphQL API, you have to consider several things:\n\n\n\n\nYou will impose to your client the graph model of your business domain, and maybe it is relevant and maybe not (see next part);\n\n\nYou will disclose a graph representation of your business model, which is not the case in JSON-RPC where you only disclose the \ninteroperability formats\n; This can represent a potential threat on your IT because your software design is very often at the heart of your business value;\n\n\nYou will have to have a complex security system, which is the security system of the graph exploration, and this will not be obvious to address;\n\n\nYou will be bound to implement the protocol complexity that can open more security issues in your software.\n\n\n\n\nFor sure, there are cases where all those arguments may be irrelevant:\n\n\n\n\nYou can work in a domain where the graph model has no value or is very well known (for instance the social media);\n\n\nYou can work for non profit organizations;\n\n\nYou can have a system that will not cause any loss of money, loss of IP or loss of value if hacked.\n\n\n\n\nThe Fundamental Principle Of Interoperability\n\n\nIn complement to the \narticle on REST\n, we will explain the fundamental principle of interoperability.\n\n\nThe context of interoperability is the following:\n\n\n\n\nSystems communicate when they have some interest in doing so: interoperability is business-oriented.\n\n\nTo establish system interconnection costs money and time, this in the project phase but also in the recurring phase: to realize those investments, the client and the server generally have a good business reason.\n\n\nWhen a server is publishing an API, it cannot imagine the list of clients he will have tomorrow, nor make assumptions on their programming language or even design approach.\n\n\n\n\nIn this context, the fundamental principle of interoperability is that the client and the server contracting the exchange \nshould define the common format that is proposing the less semantic constraints possible on both ends\n.\n\n\nBecause the client and the server don't have to commit on their internal structure and functional aspects, the interchange is described with a semantic that must be the simplest possible and that can enable the client and server to define different modeling of the same \"exchanged data model\".\n\n\n\n\nThe figure above shows the core problem of interoperability:\n\n\n\n\nThe client and the server have no reason to model the interchange data the same way. Indeed, they don't. The consequence is that they must adapt their internal formats (memory or database) to the network presentation.\n\n\nThe way they store the data is their problem, and their storage representation can be even quite different from their memory representation (that's why, generally, people use a data access layer to adapt their business objects to the database).\n\n\n\n\nVery commonly, in the industry, we have 3 representations of the data: the client's, the server's and the network presentation.\n\n\nThose adaptation are painful to develop, for sure, but they are the conditions of isolation, and who says isolation says separation of concerns.\n\n\nThis is because the client and the server are different software parts that there is no need to impose more constraints.\n\n\nIn this context, we do not recommend using GraphQL for an external business domain interface, but we can recommend it if the application architecture needs a kinf of ODBC or SQL*Net directly from the UI, and in a context where the API is not made to be exposed to unknown third parties.\n\n\nNote that we did not really enter into the core details of \"how do we really build a graph-oriented business application\"? This will come in later articles.\n\n\nA Correct Intuition?\n\n\nIf we try to step back one minute, we can say that the Facebook team may have had an intuition that, with the graph-oriented modeling approach, this fundamental principle or interoperability could be declared obsolete (and consequently graphs could be in the client, in the server, in the database, with exactly the same structure).\n\n\nIn some cases, that's true: if you master both ends of the wire, that's probably one very efficient way to do it. But it looks like ODBC or SQL*Net anyway.\n\n\nOne thing is sure, the graph-oriented modeling of business domains will revolution the IT world, but perhaps not the way the Facebook team imagined it. We'll come back on that in the future.\n\n\nConclusion\n\n\nGraphQL is a very interesting attempt to propose a middle term between REST and JSON-RPC, as the following diagram is showing it.\n\n\n\n\nHowever, this diagram is very misleading because the 3 inbound protocols have many different impacts on the server design, contrary to what's presented. Indeed, both REST and GraphQL imply a very specific programming model:\n\n\n\n\nREST is imposing a hard resource orientation that is unnatural to business applications (see \nhere\n),\n\n\nGraphQL proposes a graph-oriented ODBC-like protocol that will have the tendency to tie-up strongly the client and the server.\n\n\n\n\nThe conclusion is it seems to us that, so far, only RPC enables to design and deliver reliable and secure business applications, and to do it the way you want. RPC defines a contract that can lead to many various programming paradigms, which is not the case for REST or for GraphQL.\n\n\nThe GraphQL has, however, opened publicly the case of graph-orientation in the design of business applications.\n\n\nThis area is really a core game changer for the IT business and it will be a topic described and explained in the future in this site.\n\n\nSee also\n\n\n\n\nAbout Rest\n\n\n\n\n(\nDecember 2017\n)",
            "title": "GraphQL And Classic Web Services"
        },
        {
            "location": "/articles/graphql-web-services/#graphql-and-classic-web-services",
            "text": "Facebook produced the  GraphQL specification  in order to be an alternative to Rest.  We already explained  what we thought of Rest services .  GraphQL appears to us as a new way to do the same thing that what the industry is doing for decades.  This article is an analysis of GraphQL principles, their consequences and a comparison with traditional ways of doing web services.",
            "title": "GraphQL And Classic Web Services"
        },
        {
            "location": "/articles/graphql-web-services/#the-basics-of-services",
            "text": "",
            "title": "The Basics of Services"
        },
        {
            "location": "/articles/graphql-web-services/#various-standards-same-spirit",
            "text": "The underlying basis of services is RPC (Remote Procedure Call,  see the dedicated article on the subject ). For decades, this concept is reinvented again and again with various flavors.  Apart from RPC protocols or proprietary protocols (like the  Tuxedo  ones),  Edifact  was for a long time an industry standard in some business domains like travel, banking, medical and logistics. Edifact was replaced by  XML  then by  JSON .  Every of those standards aimed to provide the same functionality: exchanging structured data between two different systems, probably implemented with different programming languages and/or paradigms, and having a client system benefiting from a service provided by a server system.",
            "title": "Various Standards, Same Spirit"
        },
        {
            "location": "/articles/graphql-web-services/#basic-requirements-for-a-service-oriented-language",
            "text": "The first requirement is to have a \"verb\", meaning the name of the \"transaction\" we expect the remote system to process.  The second requirement is to have a type system to be able to send structured data under the form of a tree of objects. This is generally achieved by proposing basic types (like string, integer, float, etc.) and the capability of defining fields that have those properties and that can be grouped into classes. Classes will generally have to indicate what fields are required and what fields are optional. This is generally done within the context of one particular transaction.  Once classes are defined, they can be grouped in larger structures or other classes to define groups of object structures than define larger entities, most of the time, better suitable to express the business semantics.  Optionally, those definitions are expressed in \"grammars\" or \"schemas\". This is the case in RPC systems, in Edifact and in XML, and this is optional for JSON (even if the  JSON schema  specification is very usable). Note that, even if there is no explicit schema mechanism, the fact of defining objects and structures leads to the use of implicit schemas. The difference is really what part of the code is \"validating\" that the messages are well formed:   With an explicit schema definition, we can generate the validation code and create a kind of serializer/unserializer layer (wrapper/unwrapper or proxy/stub layer);  Without an explicit schema definition, the validation code is generally hand written.   All those requirements are defined in the  OSI model  in the \"presentation layer\".",
            "title": "Basic Requirements For a Service Oriented Language"
        },
        {
            "location": "/articles/graphql-web-services/#dont-forget-the-protocol",
            "text": "The last requirement is to have some kind of underlying protocol to:   Send and receive messages between systems;  Create a naming convention that enables to use the name of the service (or name of the \"verb\");  Find the network configuration that enable to route the request message to the proper entry point;  Possibly associate the request and the response with identifiers;  Possibly include information about the sender and its security credentials.   Once again, the  OSI model  defines a certain split of responsibilities between all those requirements. Quickly explained:   The transport layer (layer 4) is defining the addressing scheme and primitives that enables several systems to communicate together;  The session layer (layer 5) enables to manage the sessions and conversations between the client and the server; this layer manages the succession of names messages that the two peers will exchange;  The presentation layer (layer 6) manages the content of data, their structure and their values.   In the current JSON world,  JSON-RPC  is presenting a very basic protocol that can manage the basic single statement and request response messages.",
            "title": "Don't forget the protocol"
        },
        {
            "location": "/articles/graphql-web-services/#graphql-answers-to-service-oriented-requirements",
            "text": "GraphQL seems to us as a new flavor of the same old story. It brings some interesting new stuff (and we will come back on that), but the important point is that we can implement traditional services with this language.",
            "title": "GraphQL Answers To Service Oriented Requirements"
        },
        {
            "location": "/articles/graphql-web-services/#the-verb",
            "text": "Each GraphQL service is an named  http  entry point. The  get  method will be used to access the  query  part of the service and the  post  method will access the  mutation  part of it.  This looks like Rest but this is rather different, because the idea seems not to access to a single resource (even if it is possible), but to perform a service.",
            "title": "The \"Verb\""
        },
        {
            "location": "/articles/graphql-web-services/#the-type-system",
            "text": "GraphQL proposes an extended type system that proposes:   Basic types,  Classes,  Groups of classes,  Fragments (kind of filters on classes),  Queries.   With all the available semantics, it is very easy to implement all the existing web services that are used currently in the business.  Moreover, it is possible to have a richer interface on existing services and a more adaptative way of performing transactions.  This seems rather promising for mobile or chatbot accesses for instance. Those two UIs manipulate a small amount of data and may require some server services to adapt their reply to their particular use.  For sure, we could argue saying: with standard web services in JSON, as a client, we can always extract the subset of the response that is interesting to us. The fact is this feature goes beyond the subset approach and enables to ask the server to adapt the contents of its response to the client.",
            "title": "The Type System"
        },
        {
            "location": "/articles/graphql-web-services/#and-the-protocol",
            "text": "In the Web, the protocol is  http  and the approach seems inspired by both Rest and Ajax kind of calls.",
            "title": "And The Protocol?"
        },
        {
            "location": "/articles/graphql-web-services/#a-promising-standard",
            "text": "",
            "title": "A Promising Standard"
        },
        {
            "location": "/articles/graphql-web-services/#an-intermediate-between-json-rpc-and-rest",
            "text": "GraphQL seems to target the intermediate path between JSON-RPC (which is a simplified SOAP) and Rest. The fact of publishing a schema enables both approaches:   Perform elaborated services (like before);  Access business objects (like the Rest or CRUD approach).   GraphQL seems to propose a way to benefit from both approaches.  The fact of being able to perform complex queries (with a lot of flavours) also adds intelligence to the protocol, an intelligence driven by the client.",
            "title": "An Intermediate Between JSON-RPC and Rest?"
        },
        {
            "location": "/articles/graphql-web-services/#but-what-impact-on-the-server-design",
            "text": "This seems very interesting but the problem caused by this new protocol is located on the backend part.  Some questions must be answered:   What kind of models and/or application this protocol is targeting? All applications? Applications that should be able to disclose a part of their model?  Do we know the extend of the server impacts in terms of design?",
            "title": "But What Impact On The Server Design?"
        },
        {
            "location": "/articles/graphql-web-services/#model-your-business-domain-as-a-graph",
            "text": "We are entering here into delicate topics.  For those how already performed some semantic modeling with  RDF ,  RDFS  or  OWL , I think you're convinced about the fact that we can model a business domain with a graph.  For those who already used graph databases such as  Neo  or  Orient  in Big Data use cases or fraud recognition or social applications, may be convinced that a business model can be modeled as a graph.  But for the vast majority of IT people, this assertion is not obvious.  Well, we'll come back on this particular topic in future articles, so we'll take for granted that a business domain can be modeled with a graph.",
            "title": "Model Your Business Domain As A Graph"
        },
        {
            "location": "/articles/graphql-web-services/#graph-data-means-graph-database",
            "text": "What Facebook is proposing us is a protocol that enables to query the internal structure of the graph of data.  So, the first consequences seem to be:   Your data are modeled as a graph,  You probably use a graph database of some sort.   Consequently, your GraphQL schema is more or less the same thing as your graph database schema (when this sort of things exists such as in  OrientDB ).",
            "title": "Graph Data Means Graph Database"
        },
        {
            "location": "/articles/graphql-web-services/#open-heart-data-model",
            "text": "The second consequence is also double:   You published your graph business model to your clients;  You use your database model as the presentation layer.   This second point is very problematic. Indeed, it explains why the protocol is proposing so much complexity in some of its part: because it is supposed to be, at the same time, a protocol and a graph query language (like  Cypher ).  In a certain sense, it is supposed to be the new  SQL*Net  or  ODBC  but for graphs.  And this is where the approach is questionable.",
            "title": "Open Heart Data Model"
        },
        {
            "location": "/articles/graphql-web-services/#the-myth-of-the-single-representation-of-reality",
            "text": "There are many myths in IT, and this one is a big one. Many people believe that there is only one way of representing the reality-theirs, indeed. And the reality is, they are wrong.  Note that, throughout the history of science, many great scientists had the same debate as the  intuitionist debate of the beginning of the 20th Century .  Any software designer that worked sufficiently in the domain should have seen several facts:   Whatever the efforts and the design patterns used, you cannot predict how the business will evolve;  Two designer confronted to the same problem will produce two different designs.   Indeed, design is very subjective and, for a lot of reasons we won't explain here, it is not a bad thing that there is no unique model for a particular business domain, on the contrary.",
            "title": "The Myth Of The Single Representation Of Reality"
        },
        {
            "location": "/articles/graphql-web-services/#when-client-and-server-share-everything",
            "text": "GraphQL is proposing a way for the customer to be very near from its server, so near that its data model is perfectly known, right from the client.  This can be very useful when you work with yourself, when you develop your own client and server in a kind of symbiosis. If you are on an Internet application, you also must know that you core model will be known from the outside, which can be a problems in terms of IP and in terms of security.  So:   GraphQL is OK if your API is  internal ;  GraphQL seems not OK if your API is external.",
            "title": "When Client and Server Share Everything"
        },
        {
            "location": "/articles/graphql-web-services/#publishing-a-graphql-api",
            "text": "Well, if you want to publish a GraphQL API, you have to consider several things:   You will impose to your client the graph model of your business domain, and maybe it is relevant and maybe not (see next part);  You will disclose a graph representation of your business model, which is not the case in JSON-RPC where you only disclose the  interoperability formats ; This can represent a potential threat on your IT because your software design is very often at the heart of your business value;  You will have to have a complex security system, which is the security system of the graph exploration, and this will not be obvious to address;  You will be bound to implement the protocol complexity that can open more security issues in your software.   For sure, there are cases where all those arguments may be irrelevant:   You can work in a domain where the graph model has no value or is very well known (for instance the social media);  You can work for non profit organizations;  You can have a system that will not cause any loss of money, loss of IP or loss of value if hacked.",
            "title": "Publishing a GraphQL API"
        },
        {
            "location": "/articles/graphql-web-services/#the-fundamental-principle-of-interoperability",
            "text": "In complement to the  article on REST , we will explain the fundamental principle of interoperability.  The context of interoperability is the following:   Systems communicate when they have some interest in doing so: interoperability is business-oriented.  To establish system interconnection costs money and time, this in the project phase but also in the recurring phase: to realize those investments, the client and the server generally have a good business reason.  When a server is publishing an API, it cannot imagine the list of clients he will have tomorrow, nor make assumptions on their programming language or even design approach.   In this context, the fundamental principle of interoperability is that the client and the server contracting the exchange  should define the common format that is proposing the less semantic constraints possible on both ends .  Because the client and the server don't have to commit on their internal structure and functional aspects, the interchange is described with a semantic that must be the simplest possible and that can enable the client and server to define different modeling of the same \"exchanged data model\".   The figure above shows the core problem of interoperability:   The client and the server have no reason to model the interchange data the same way. Indeed, they don't. The consequence is that they must adapt their internal formats (memory or database) to the network presentation.  The way they store the data is their problem, and their storage representation can be even quite different from their memory representation (that's why, generally, people use a data access layer to adapt their business objects to the database).   Very commonly, in the industry, we have 3 representations of the data: the client's, the server's and the network presentation.  Those adaptation are painful to develop, for sure, but they are the conditions of isolation, and who says isolation says separation of concerns.  This is because the client and the server are different software parts that there is no need to impose more constraints.  In this context, we do not recommend using GraphQL for an external business domain interface, but we can recommend it if the application architecture needs a kinf of ODBC or SQL*Net directly from the UI, and in a context where the API is not made to be exposed to unknown third parties.  Note that we did not really enter into the core details of \"how do we really build a graph-oriented business application\"? This will come in later articles.",
            "title": "The Fundamental Principle Of Interoperability"
        },
        {
            "location": "/articles/graphql-web-services/#a-correct-intuition",
            "text": "If we try to step back one minute, we can say that the Facebook team may have had an intuition that, with the graph-oriented modeling approach, this fundamental principle or interoperability could be declared obsolete (and consequently graphs could be in the client, in the server, in the database, with exactly the same structure).  In some cases, that's true: if you master both ends of the wire, that's probably one very efficient way to do it. But it looks like ODBC or SQL*Net anyway.  One thing is sure, the graph-oriented modeling of business domains will revolution the IT world, but perhaps not the way the Facebook team imagined it. We'll come back on that in the future.",
            "title": "A Correct Intuition?"
        },
        {
            "location": "/articles/graphql-web-services/#conclusion",
            "text": "GraphQL is a very interesting attempt to propose a middle term between REST and JSON-RPC, as the following diagram is showing it.   However, this diagram is very misleading because the 3 inbound protocols have many different impacts on the server design, contrary to what's presented. Indeed, both REST and GraphQL imply a very specific programming model:   REST is imposing a hard resource orientation that is unnatural to business applications (see  here ),  GraphQL proposes a graph-oriented ODBC-like protocol that will have the tendency to tie-up strongly the client and the server.   The conclusion is it seems to us that, so far, only RPC enables to design and deliver reliable and secure business applications, and to do it the way you want. RPC defines a contract that can lead to many various programming paradigms, which is not the case for REST or for GraphQL.  The GraphQL has, however, opened publicly the case of graph-orientation in the design of business applications.  This area is really a core game changer for the IT business and it will be a topic described and explained in the future in this site.",
            "title": "Conclusion"
        },
        {
            "location": "/articles/graphql-web-services/#see-also",
            "text": "About Rest   ( December 2017 )",
            "title": "See also"
        },
        {
            "location": "/articles/portfolio/",
            "text": "A Simple Meta-Model for Portfolio Management\n\n\n\n\nImage courtesy of freedigitalphotos.net\n.\n\n\nOne of the main problems of companies is to manage many IT projects at the same time. Most often, the companies facing this problem are managing project portfolios.\n\n\nThe main drawback of managing several projects in parallel is that is is not easy to:\n\n\n\n\nUnderstand the dependencies of the various projects together, and so the order in which they should be led;\n\n\nIdentify and deal with the various scope overlaps between the various projects;\n\n\nManage the relationship with the business people that want the various projects to happen.\n\n\n\n\nThis article presents a very simple method to use to address all those points. We will present the original version of the meta-model. An \nArchimate\n version of this model will be presented in another article.\n\n\nThis method mostly targets enterprise or IT architects, program managers or project portfolio managers. Due to its simplicity, many other profiles can use it.\n\n\nMeta-Model Presentation\n\n\nThe first objective is to formalize the client of the IT project. Each organization has several departments in which we have several different customers.\n\n\nOur first artifact type will be \nOrganization\n. We will assume that every organization can include several other organizations. Each organization will also have a customer name as an attribute.\n\n\nWhen the projects intentions are declared, the materials that are produced by the various customers inside the same organization will generally be a list of many projects, with a project name and a more or less developed functional description.\n\n\nWe will define two artifact types to model that:\n\n\n\n\nProject\n to model the project,\n\n\nFunction\n to model the functionality.\n\n\n\n\nIn the existing IT systems, we have applications. With the projects to come, some of the current application will get new functions, some new applications will be created and some other will be decommissioned.\n\n\nWe need one more artifact type: \nApplication\n that models the application; if the application is new, we need a way to tag it as \"new\" or \"existing\".\n\n\nThe \nFunction\n artifact type will be used to attach functionality to \nApplication\ns. For this purpose, we need an attribute on the \nFunction\n that indicates if the function is already existing, if it is new or if it must evolve.\n\n\nAs applications are part of a big part of the IT systems (for instance the financial systems), we will attach the application to an application domain; we will use the \nDomain\n artifact type for that purpose.\n\n\nIn terms of relationships, we have the following semantics:\n\n\n\n\nAn \nOrganization\n can contain other \nOrganization\ns;\n\n\nAn \nOrganization\n will be the customer of several \nProject\ns;\n\n\nProject\ns are aggregating \nFunction\ns;\n\n\nApplication\ns are attached to \nDomain\ns and \nFunction\ns are attached to them.\n\n\n\n\nThe resulting meta-model is presented on the following figure.\n\n\n\n\nMethodology\n\n\nLet's suppose we use this model during the budgeting process that usually happens once a year in companies. Budget time is often the time for difficult choices.\n\n\nThe following method is easy to use, can neutralize affects and help the top level management to choose between projects and to create a feasible roadmap within the limits of a feasible budget.\n\n\nStep 1: Accept All Project Requests and Identify Functions\n\n\nThe first step is consisting in gathering all projects intentions from all organizations.\n\n\nWe will analyze organization per organization. For each customer within a particular organization, the project intentions are gathered.\n\n\nThe objective is to get the best functional description of the requirements possible and to identify the required functions. Very often, the requirements are a list of features (warning: some of them may already have been implemented without the customer to know about it).\n\n\nIn that phase, the customer must not be challenged, but the architect should try his best to understand the \nfunctional\n (and non functional) requirements. To do that, it can be necessary to distinguish between the solution imagined by the customer and the original requirement.\n\n\nAs the model is simple, the requirement artifact does not exist in this version. The identification of the required \nFunction\ns will conclude this phase.\n\n\nOnce this phase is complete, we can promise the customer to:\n\n\n\n\nAnalyze his requirements along with all other requirements and projects and do our best to develop what is required;\n\n\nProvide a detailed feedback on the demands when all demands have been captured.\n\n\n\n\nStep 2: Identify Common Functions to Highlight Dependencies\n\n\nSeveral customers could have defined different projects that are using the same subset of functions. Identifying those functions is very important, this for several reasons:\n\n\n\n\nTo avoid several projects to develop several times the same function, or worse, variations of the same function;\n\n\n\n\nTo identify in what projects this function can be required and to scope it carefully;\n\n\n\n\nDeveloping a reusable function will ease the success of many projects whereas not developing it may cause project delays or failure;\n\n\n\n\n\n\n\n\nTo add this function early in the roadmap;\n\n\n\n\n\n\nTo take a special care to carefully \nplace\n this function \nat the proper spot\n in the rest of the IT systems (step 3);\n\n\n\n\nIndeed, a misplaced function will generate many useless client/server calls, will generate consistency problems and will generate more integration and QA works: Economically, function placement is crucial.\n\n\n\n\n\n\n\n\nFunctions, once identified, should be assigned with a number characterizing the number of projects that require them. The highest number will lead the global roadmapping exercise. We will call this number the \ndependency number\n.\n\n\nStep 3: Associate Functions to Applications\n\n\nThis is a back office step that will be done between IT people, most of the time by architects. It is time to review the main functions of the existing applications to be accurate in the exercise.\n\n\nFunctions will be classified into 3 categories:\n\n\n\n\nExisting functions\n: We can plan to go back to the customer to assess if we missed something in the original requirement and if the existing function should not evolve.\n\n\nNew functions that can be naturally attached to an existing application\n: Some of the functions will naturally find their place as an evolution of an existing application;\n\n\nNew functions that don't seem to be a natural evolution of the existing applications\n: However, some new functions have to be studied in order to determine their best spot in the IT systems (or their \"less worst\" spot).\n\n\n\n\nThe two last steps can give birth to more advanced architecture studies. Covering this topic is not in the scope of this article, but we can insist on leading careful analysis, because function misplacement is a problem that will cause painful and costly problems for years if not for decades.\n\n\nAll the functions that are not in existing applications should be gathered and analyzed as a whole to determine the opportunity of developing new applications.\n\n\nStep 4: Analyze the Roadmap of Each Application, Size it Roughly and Compare With The Skills Available\n\n\nOnce all functions were assigned to applications, we can size them roughly and have a view of the roadmap of each application. For sure, this roadmap is still theoretical and the pure consequence of the customer's requirements.\n\n\nThe Case of Application Maintenance and Evolution\n\n\nAll functions of an existing application can be prioritized in the best order of feasibility to create a first credible application roadmap, that is compliant with technical constraints.\n\n\nAt this stage, we can also identify the functions that are nice to have compared to the ones that are must haves. This classification, even if it can be debated with the business may lead to some negotiation margins during the year to come.\n\n\nThe skills available for each application is important because it will determine the real feasibility of the roadmap per application: We mean IT skills (project manager, product owner, developer, QA) but also business skills.\n\n\nIndeed, in that phase, we can identify the bottlenecks in terms of IT people, when not enough people are skilled to develop the expected application roadmap. Depending on the global priorities of the company, it is possible in that phase to plan people transfer, trainings or recruitments.\n\n\nThe Case of New Applications\n\n\nThose projects are always more risky than the previous ones.\n\n\nIn some cases, it is obvious for the customer that the new requirements will lead to a new application. In some other cases, it is not. Those cases must be dealt with carefully because the customer may argue that the new requirements are part of an existing application. In those cases, the objective is to avoid function misplacement.\n\n\nHowever, it is more difficult to create a new application:\n\n\n\n\nThe project manager has to be found, and she/he has to be able to lead from-scratch projects;\n\n\nThe team has to be found, or in the existing people (staff or consultants), or in new comers;\n\n\nThe business expert(s) has to be named.\n\n\n\n\nSome business people may be a bit worried about the application creation projects because they cause HR troubles and they can face more delays or failures than evolution projects.\n\n\nEnd of the Phase\n\n\nAt the end of the step 4, we should have:\n\n\n\n\nAll functions identified;\n\n\nAll application roadmap sized;\n\n\nAll HR requirements.\n\n\n\n\nStep 5: Introduction of Dependencies to Create Project Visions\n\n\nTo determine which projects to do first in the project portfolio, we need to take the reusable functions with the highest dependency number, in order to schedule them first.\n\n\nThen, the purpose is to play with the constraints to determine the less worst global path considering:\n\n\n\n\nThe list of projects to develop;\n\n\nThe skills available;\n\n\nThe potential HR adjustments (people moving from one project to another, new people).\n\n\n\n\nGenerally, the functions with the highest dependency number will drive the roadmap. That means that a function will be develop if and only if it is in the same project than a function with a high dependency number. That means also that generally a lot of new functions required by the business will not be able to be developed in the coming year, just because they are not part of an \"important\" project.\n\n\nIndeed, this method enables to show to the top level management what is important to the company throughout the functions that are required by many projects (i.e. that have the highest dependency numbers).\n\n\nOnce everything is done, the exercise can be redone if the budget is bigger or lower in some parts.\n\n\nLimits of the Method\n\n\nWe can argue that the functions with the highest dependency numbers may not be the most crucial functions. That can be true.\n\n\nWe can use some tricks in using several levels of functions if needed. Indeed, a function (level 1) required by a certain project could be divided into 3 functions of a smaller granularity (level 2). The level 2 functions could be the functions implemented by the applications. With this model adjustment, we can have a high level view of the function with \"project functions\" (level 1) and an IT view of functions (level 2).\n\n\nThe artifact type \nFunction\n must then be able to aggregate a list of \nFunction\ns. We let the reader update the meta-model.\n\n\nWhat Tools Can Help?\n\n\nYed\n\n\nWhen this method was used for the first time, \nYed\n was the tool used. A Yed model was realized per organization.\n\n\nThe advantage of Yed is its simplicity and its powerful set of graph layout algorithms. The drawback of Yed is when the number of projects is big (we had to deal with the Digital Division and their 130 projects), the model is hardly usable.\n\n\nEMF and Sirius\n\n\nIndeed, when the number of projects is high, a multiple view design tool is required. A possible option is to create and \nEMF\n model and to create the edition views with \nSirius\n. You can also use \nMetaEdit+\n that is a powerful metamodeler.\n\n\nRoadmapping With Archimate\n\n\nArchimate\n is probably the best language to realize this kind of portfolio management. We will explain in a bit more details a possible use of Archimate enterprise architecture modeling language for portfolio roadmapping in another article.\n\n\nConclusion\n\n\nWith this meta-model, it is possible to create and operate a feasible, logic and auditable project portfolio and to have explanations about the constraints and the process of deciding what project should be done in priority.\n\n\nMost often, this kind of tools pacifies the battleground.\n\n\nEven if projects can go wrong during the year to come, with this method, many problems will have been anticipated and will, as per magic, not occur during the year.\n\n\n(\nJanuary 2018\n)",
            "title": "A Simple Meta-Model for Portfolio Management"
        },
        {
            "location": "/articles/portfolio/#a-simple-meta-model-for-portfolio-management",
            "text": "Image courtesy of freedigitalphotos.net .  One of the main problems of companies is to manage many IT projects at the same time. Most often, the companies facing this problem are managing project portfolios.  The main drawback of managing several projects in parallel is that is is not easy to:   Understand the dependencies of the various projects together, and so the order in which they should be led;  Identify and deal with the various scope overlaps between the various projects;  Manage the relationship with the business people that want the various projects to happen.   This article presents a very simple method to use to address all those points. We will present the original version of the meta-model. An  Archimate  version of this model will be presented in another article.  This method mostly targets enterprise or IT architects, program managers or project portfolio managers. Due to its simplicity, many other profiles can use it.",
            "title": "A Simple Meta-Model for Portfolio Management"
        },
        {
            "location": "/articles/portfolio/#meta-model-presentation",
            "text": "The first objective is to formalize the client of the IT project. Each organization has several departments in which we have several different customers.  Our first artifact type will be  Organization . We will assume that every organization can include several other organizations. Each organization will also have a customer name as an attribute.  When the projects intentions are declared, the materials that are produced by the various customers inside the same organization will generally be a list of many projects, with a project name and a more or less developed functional description.  We will define two artifact types to model that:   Project  to model the project,  Function  to model the functionality.   In the existing IT systems, we have applications. With the projects to come, some of the current application will get new functions, some new applications will be created and some other will be decommissioned.  We need one more artifact type:  Application  that models the application; if the application is new, we need a way to tag it as \"new\" or \"existing\".  The  Function  artifact type will be used to attach functionality to  Application s. For this purpose, we need an attribute on the  Function  that indicates if the function is already existing, if it is new or if it must evolve.  As applications are part of a big part of the IT systems (for instance the financial systems), we will attach the application to an application domain; we will use the  Domain  artifact type for that purpose.  In terms of relationships, we have the following semantics:   An  Organization  can contain other  Organization s;  An  Organization  will be the customer of several  Project s;  Project s are aggregating  Function s;  Application s are attached to  Domain s and  Function s are attached to them.   The resulting meta-model is presented on the following figure.",
            "title": "Meta-Model Presentation"
        },
        {
            "location": "/articles/portfolio/#methodology",
            "text": "Let's suppose we use this model during the budgeting process that usually happens once a year in companies. Budget time is often the time for difficult choices.  The following method is easy to use, can neutralize affects and help the top level management to choose between projects and to create a feasible roadmap within the limits of a feasible budget.",
            "title": "Methodology"
        },
        {
            "location": "/articles/portfolio/#step-1-accept-all-project-requests-and-identify-functions",
            "text": "The first step is consisting in gathering all projects intentions from all organizations.  We will analyze organization per organization. For each customer within a particular organization, the project intentions are gathered.  The objective is to get the best functional description of the requirements possible and to identify the required functions. Very often, the requirements are a list of features (warning: some of them may already have been implemented without the customer to know about it).  In that phase, the customer must not be challenged, but the architect should try his best to understand the  functional  (and non functional) requirements. To do that, it can be necessary to distinguish between the solution imagined by the customer and the original requirement.  As the model is simple, the requirement artifact does not exist in this version. The identification of the required  Function s will conclude this phase.  Once this phase is complete, we can promise the customer to:   Analyze his requirements along with all other requirements and projects and do our best to develop what is required;  Provide a detailed feedback on the demands when all demands have been captured.",
            "title": "Step 1: Accept All Project Requests and Identify Functions"
        },
        {
            "location": "/articles/portfolio/#step-2-identify-common-functions-to-highlight-dependencies",
            "text": "Several customers could have defined different projects that are using the same subset of functions. Identifying those functions is very important, this for several reasons:   To avoid several projects to develop several times the same function, or worse, variations of the same function;   To identify in what projects this function can be required and to scope it carefully;   Developing a reusable function will ease the success of many projects whereas not developing it may cause project delays or failure;     To add this function early in the roadmap;    To take a special care to carefully  place  this function  at the proper spot  in the rest of the IT systems (step 3);   Indeed, a misplaced function will generate many useless client/server calls, will generate consistency problems and will generate more integration and QA works: Economically, function placement is crucial.     Functions, once identified, should be assigned with a number characterizing the number of projects that require them. The highest number will lead the global roadmapping exercise. We will call this number the  dependency number .",
            "title": "Step 2: Identify Common Functions to Highlight Dependencies"
        },
        {
            "location": "/articles/portfolio/#step-3-associate-functions-to-applications",
            "text": "This is a back office step that will be done between IT people, most of the time by architects. It is time to review the main functions of the existing applications to be accurate in the exercise.  Functions will be classified into 3 categories:   Existing functions : We can plan to go back to the customer to assess if we missed something in the original requirement and if the existing function should not evolve.  New functions that can be naturally attached to an existing application : Some of the functions will naturally find their place as an evolution of an existing application;  New functions that don't seem to be a natural evolution of the existing applications : However, some new functions have to be studied in order to determine their best spot in the IT systems (or their \"less worst\" spot).   The two last steps can give birth to more advanced architecture studies. Covering this topic is not in the scope of this article, but we can insist on leading careful analysis, because function misplacement is a problem that will cause painful and costly problems for years if not for decades.  All the functions that are not in existing applications should be gathered and analyzed as a whole to determine the opportunity of developing new applications.",
            "title": "Step 3: Associate Functions to Applications"
        },
        {
            "location": "/articles/portfolio/#step-4-analyze-the-roadmap-of-each-application-size-it-roughly-and-compare-with-the-skills-available",
            "text": "Once all functions were assigned to applications, we can size them roughly and have a view of the roadmap of each application. For sure, this roadmap is still theoretical and the pure consequence of the customer's requirements.",
            "title": "Step 4: Analyze the Roadmap of Each Application, Size it Roughly and Compare With The Skills Available"
        },
        {
            "location": "/articles/portfolio/#the-case-of-application-maintenance-and-evolution",
            "text": "All functions of an existing application can be prioritized in the best order of feasibility to create a first credible application roadmap, that is compliant with technical constraints.  At this stage, we can also identify the functions that are nice to have compared to the ones that are must haves. This classification, even if it can be debated with the business may lead to some negotiation margins during the year to come.  The skills available for each application is important because it will determine the real feasibility of the roadmap per application: We mean IT skills (project manager, product owner, developer, QA) but also business skills.  Indeed, in that phase, we can identify the bottlenecks in terms of IT people, when not enough people are skilled to develop the expected application roadmap. Depending on the global priorities of the company, it is possible in that phase to plan people transfer, trainings or recruitments.",
            "title": "The Case of Application Maintenance and Evolution"
        },
        {
            "location": "/articles/portfolio/#the-case-of-new-applications",
            "text": "Those projects are always more risky than the previous ones.  In some cases, it is obvious for the customer that the new requirements will lead to a new application. In some other cases, it is not. Those cases must be dealt with carefully because the customer may argue that the new requirements are part of an existing application. In those cases, the objective is to avoid function misplacement.  However, it is more difficult to create a new application:   The project manager has to be found, and she/he has to be able to lead from-scratch projects;  The team has to be found, or in the existing people (staff or consultants), or in new comers;  The business expert(s) has to be named.   Some business people may be a bit worried about the application creation projects because they cause HR troubles and they can face more delays or failures than evolution projects.",
            "title": "The Case of New Applications"
        },
        {
            "location": "/articles/portfolio/#end-of-the-phase",
            "text": "At the end of the step 4, we should have:   All functions identified;  All application roadmap sized;  All HR requirements.",
            "title": "End of the Phase"
        },
        {
            "location": "/articles/portfolio/#step-5-introduction-of-dependencies-to-create-project-visions",
            "text": "To determine which projects to do first in the project portfolio, we need to take the reusable functions with the highest dependency number, in order to schedule them first.  Then, the purpose is to play with the constraints to determine the less worst global path considering:   The list of projects to develop;  The skills available;  The potential HR adjustments (people moving from one project to another, new people).   Generally, the functions with the highest dependency number will drive the roadmap. That means that a function will be develop if and only if it is in the same project than a function with a high dependency number. That means also that generally a lot of new functions required by the business will not be able to be developed in the coming year, just because they are not part of an \"important\" project.  Indeed, this method enables to show to the top level management what is important to the company throughout the functions that are required by many projects (i.e. that have the highest dependency numbers).  Once everything is done, the exercise can be redone if the budget is bigger or lower in some parts.",
            "title": "Step 5: Introduction of Dependencies to Create Project Visions"
        },
        {
            "location": "/articles/portfolio/#limits-of-the-method",
            "text": "We can argue that the functions with the highest dependency numbers may not be the most crucial functions. That can be true.  We can use some tricks in using several levels of functions if needed. Indeed, a function (level 1) required by a certain project could be divided into 3 functions of a smaller granularity (level 2). The level 2 functions could be the functions implemented by the applications. With this model adjustment, we can have a high level view of the function with \"project functions\" (level 1) and an IT view of functions (level 2).  The artifact type  Function  must then be able to aggregate a list of  Function s. We let the reader update the meta-model.",
            "title": "Limits of the Method"
        },
        {
            "location": "/articles/portfolio/#what-tools-can-help",
            "text": "",
            "title": "What Tools Can Help?"
        },
        {
            "location": "/articles/portfolio/#yed",
            "text": "When this method was used for the first time,  Yed  was the tool used. A Yed model was realized per organization.  The advantage of Yed is its simplicity and its powerful set of graph layout algorithms. The drawback of Yed is when the number of projects is big (we had to deal with the Digital Division and their 130 projects), the model is hardly usable.",
            "title": "Yed"
        },
        {
            "location": "/articles/portfolio/#emf-and-sirius",
            "text": "Indeed, when the number of projects is high, a multiple view design tool is required. A possible option is to create and  EMF  model and to create the edition views with  Sirius . You can also use  MetaEdit+  that is a powerful metamodeler.",
            "title": "EMF and Sirius"
        },
        {
            "location": "/articles/portfolio/#roadmapping-with-archimate",
            "text": "Archimate  is probably the best language to realize this kind of portfolio management. We will explain in a bit more details a possible use of Archimate enterprise architecture modeling language for portfolio roadmapping in another article.",
            "title": "Roadmapping With Archimate"
        },
        {
            "location": "/articles/portfolio/#conclusion",
            "text": "With this meta-model, it is possible to create and operate a feasible, logic and auditable project portfolio and to have explanations about the constraints and the process of deciding what project should be done in priority.  Most often, this kind of tools pacifies the battleground.  Even if projects can go wrong during the year to come, with this method, many problems will have been anticipated and will, as per magic, not occur during the year.  ( January 2018 )",
            "title": "Conclusion"
        },
        {
            "location": "/articles/archimate-intro/",
            "text": "Introduction to the Archimate Revolution\n\n\n \n\n\nIn the last decade, a quiet revolution took place in the Enterprise Architecture (EA) domain. This revolution is called \nArchimate\n.\n\n\nThis article is the first of a series of articles focusing on some specific aspects of Archimate and practices around the standard.\n\n\nAll Archimate diagrams of this site are done using \nArchi\n. This tool is free and great. If you use it, consider making a donation.\n\n\nA Semantic Modeling Language\n\n\nBrief Introduction To The Archimate Meta-Model\n\n\nArchimate is a modeling language that enables to describe and study several aspects of the enterprise:\n\n\n\n\nIts strategy and motivations,\n\n\nIts projects,\n\n\nAnd the 4 core layers of enterprise description:\n\n\nThe business layer,\n\n\nThe software layer,\n\n\nThe technology layer (infrastructure),\n\n\nThe physical layer.\n\n\n\n\n\n\n\n\nAll those aspects propose:\n\n\n\n\nTyped artifacts,\n\n\nTyped relationships between artifacts.\n\n\n\n\nNote that several relationships types can interconnect many different types or can be used in many contexts. The detailed description of all artifacts is not in the scope of this article and we can advise the reader to refer to the book \nEnterprise Architecture At Work\n from \nMark Lankshorst\n.\n\n\nArchimate also defines interlayer relationships which enables to \"connect\" the various layers together. This is an absolutely fundamental feature of the language.\n\n\nA Graph Model\n\n\nIndeed, an Archimate model is actually a \ngraph\n. For those who are familiar with the Semantic Web (\nRDF, RDFS\n, and \nOWL\n), any Archimate model is a semantic graph.\n\n\nThe graph model is the result of the union of all diagrams implemented with the same set of artifacts. For each artifact, the union of all its incoming and outgoing relationships creates the \nneighborhood\n of the artifact in the graph model.\n\n\nWhen you use Archimate, you represent things in views, using the various artifacts that are available to you. Each element you draw on a certain view (there are many types of views in the standard) will have a certain type, like \"Business Process\" or \"Application Function\". Thus, the resulting model will be a set of views, each of them presenting many interconnected artifacts that all are instances of types that have a specific \nmeaning\n.\n\n\nViewpoint And Meta-Model\n\n\nThe creation of a semantic language generally implies the creation a meta-model.\n\n\nMany tools are existing to create meta-models (for instance \nEclipse EMF\n with \nSirius\n, or \nMetaEdit+\n depending if you want to pay or not).\n\n\nIn the history of enterprise architecture, many approaches defined the notion of \"view point\": in order to act on the enterprise as a whole (or as a \nsystem\n), the first step is to describe the enterprise. In order to describe it, due to the complexity of the task, the architect must use several viewpoints. The union of those viewpoints is defining the model of the enterprise.\n\n\nThe \nZachman framework\n was one of the first publicly available enterprise architecture \"framework\", enabling to address the many viewpoints describing the enterprise.\n\n\nThe problem with this framework, but also with many other enterprise architecture frameworks, is that they propose many viewpoints, generally each of them proposing a complete \nseparate\n meta-model for each viewpoint (with artifact types and relationship types), but no consistent view of those various viewpoints.\n\n\nThis is a major issue for the framework because:\n\n\n\n\nSeparate meta-models for separate viewpoints will, most often, enable to create a globally ambiguous model;\n\n\nSeparate meta-models can create various ways of representing the same reality;\n\n\nThere can be semantic overlap between various viewpoints and their respective meta-model;\n\n\nWhat should be a single viewpoint can artificially be split into different view points with different meta-models (we'll see later in this article the dramatic consequences of the Long\u00e9p\u00e9 Enterprise Architecture model in France).\n\n\n\n\nIf the consistency is not \"built in\" the framework (i.e. in its meta-model), if it is not part of the primary requirements, then the framework will be very difficult to use.\n\n\nIndeed, interconnecting various meta-models to be able to create something consistent is not an easy task. Indeed, Archimate seems to be the best illustration of it, and I fear I don't know many other samples [1].\n\n\nSamples of Non Consistent Modeling Approaches\n\n\nIn IT, we have forgotten about the crucial importance of having a consistent way to describe the \"reality\", or let's say the concepts that we are manipulating frequently.\n\n\nSample #1: UML\n\n\nUML can be the first example of the non consistency of a meta-model. UML is not consistent because it proposes many meta-models that are not semantically connected together as a whole.\n\n\nSuppose you made class diagrams and sequence diagrams for a set of classes. You can add a state diagram for a particular process of a specific class \nA\n of your model. In UML, there is no way to know if this state diagram is consistent or not with the rest of the diagrams that include \nA\n. This is due to the fact that UML proposes a set of various kinds of views that are not \nlinked together\n.\n\n\nEach view type has its own meta-model. Some of the meta-models are reusing the same artifacts, which enable modeling tools to connect the artifacts to the views they are used into. But many diagram types are just disconnected meta-models.\n\n\nThis problem can perhaps explain why modeling in UML was progressively abandoned by many projects; because it was not really suited to express in a consistent way what the code should be. If, during design, doing UML model cannot prevent you from making design mistakes, the use of the modeling language is much less interesting [2].\n\n\nSample #2: Long\u00e9p\u00e9\n\n\nAnother dramatic inconsistent model is the Long\u00e9p\u00e9 French model which defined, instead of Archimate \"software layer\", two different layers:\n\n\n\n\nOne \"functional architecture layer\", that could be interpreted as the Application Function part of the Archimate meta-model,\n\n\nOne \"application architecture layer\", that could be interpreted as the Application Component part of the Archimate meta-model.\n\n\n\n\nAs in Archimate, there is, for Long\u00e9p\u00e9, a business layer and an infrastructure layer, which scopes are almost the same than Archimate's.\n\n\nIn the Long\u00e9p\u00e9 model, every layer has a model, and every model can be \"derived\" from the model of the superior layer. This means that the application layer can be \"derived from\" the functional layer. This assertion is obscure, misleading and semantically erroneous.\n\n\nIn Archimate, the application function is \"assigned to\" a application component. That enables to manage the good and the bad assignment, what is called the \"urbanisation\" in French, notion of good or bad function positioning in the global IT systems:\n\n\n\n\nIf some functions are well positioned in the IT systems, that means that they communicate easily and can evolve in a natural way without questioning the full architecture;\n\n\nIf some functions are badly positioned in the IT systems (so assigned to the wrong components), we will generate useless dependencies between applications, useless web services, difficulties in terms of evolution, costs and complexity.\n\n\n\n\nThe problem of \"function positioning\" is not in the scope of this article, but it is at the heart of the Long\u00e9p\u00e9's reflection, despite the invention of a very bad meta-model that totally confuses the notion through the concept of derivation. \n\n\nThis artificial split of one layer (the application layer of Archimate) into two layers which dependency is erroneously defined, caused \na huge number of French IT projects to fail\n, and numerous errors of interpretation and understanding for French architects. The book of Christophe Long\u00e9p\u00e9 was at the origin of many misjudgments, errors, confusions and money loss in the French market since the publication of its first edition in 2001.\n\n\nSample #3: Projects Creating Their Own Modeling Framework\n\n\nIn consulting missions, I also saw strange practices, as the one of creating a project-specific enterprise architecture modeling framework that evolved throughout the project. Most of the time, the internal framework was incorporating progressively various inconsistent meta-models coming from various modeling standards (such as \nUML\n or \nBPMN\n or \nTOGAF\n).\n\n\nFor sure, most architecture works in the evolving project-specific EA framework were confusing and not usable by software engineers. This often led or contributed to lead the project to its failure.\n\n\nArchimate, a Consistent Approach\n\n\nArchimate is proposing a consistent meta-model. This is the first fundamental characteristic which is at the heart of the usability of the modeling standard.\n\n\nI deeply encourage the meta-model creators to think about it, because it defines the real power of the multiple-views paradigm: multiple views on the same reality can be used \nprovided there can be consistency between those views\n, the consistency being in a unique meta-model.\n\n\nNote that proposing a consistent meta-model does not imply that the modeling framework is closed and will not evolve. Archimate proposes, since its version 3, a way to extend the meta-model.\n\n\nWe could say that the multiple views consistency-enable graph model is the revolution #0 of the Archimate language.\n\n\nThe Many Revolutions of Archimate\n\n\nRevolution #1: The Language Just Works\n\n\nBeing consistent is not sufficient for a meta-model.\n\n\nThe \nquality\n of the meta-model lies in the pertinence of the semantic artifacts that it proposes (being nodes or relationships). Those artifacts must be as semantically clearly defined and must not bring confusion or multiple alternative representations. In particular, semantic overlapping must be avoided as much as possible [3].\n\n\nIn other terms, if the quality of the meta-model is high, then the modeling will be good. However, if the meta-model is bad, the modeling will be very bad and will cause damages in the project.\n\n\nIndeed, Archimate propose artifacts that enable the non-ambiguous description of the enterprise processes, strategy and IT.\n\n\nMore: it pushes the architects to describe the reality \nin a sane way\n, which means in a way that will push for problem exploration and make visible the possible solutions.\n\n\nFor instance, in the case of function positioning, it is easy to count the dependencies between two functions (through derived relationships). If the count is low and always directed in the same way, that probably means the functions are well positioned; If the count if high and/or the directions go both ways, that probably means that the two functions should be one single function.\n\n\nArchimate language helps and in that sense, it is doing the job. In other terms: the language just \nworks\n.\n\n\nThis is, really, a revolution. In some cases, meta-models induce architects to think badly, to force themselves to think in an inconsistent model where the semantics are confusing. In Archimate, this is not the case.\n\n\nRevolution #2: Architects Can Share And Propose Auditable Works\n\n\nEven if Archimate will not guarantee that 2 enterprise architects will produce the same modeling when representing the same things, using the same standardized language enable each of them to understand the modeling of the other, to challenge it and to discuss it.\n\n\nWe can forget Visio or Powerpoint schema based modeling, that are very ambiguous at several levels. The works become auditable by other Archimate architects.\n\n\nFor sure, documenting textually the views and the artifact themselves is very helpful. As in all modeling languages, without an effort, some views may be a be difficult to understand if we don't know what question it is supposed to answer.\n\n\nThis is a revolution in Enterprise Architecture but also in IT or business architecture. We can work on something \nshareable\n and \nauditable\n and so begin to work the simplistic boxes and arrow diagrams.\n\n\nEnterprise Architecture is becoming more an engineering discipline when it was considered, too often, as a blur esoteric and confusing witchcraft.\n\n\nRevolution #3: Managing Complex Representations\n\n\nIn Archimate, you can tackle very complex problems.\n\n\nDue to the fact that you can study several granularity of problems at the same time in the same model [4], the studied complexity can be at several levels:\n\n\n\n\nYou can address very big IT systems or one company or of a group of companies;\n\n\nYou can analyze in a very detailed way sets of very small intertwined functionality;\n\n\nYou can study highly distributed systems.\n\n\n\n\nIndeed, big models for IT transformation will quickly contain thousands of artifacts. Note that to ensure consistency on those models, there is a huge systematic work to do.\n\n\nThis is a revolution. The last big program that I did without Archimate was in 2008-2009. It was big, functionally very complex and with a lot of architecture problems. I struggled with UML and BPMN and had to manage the functional consistency by hand. Archimate would have been of great help at the time.\n\n\nRevolution #4: Aggregate Various Sources of Knowledge\n\n\nWhen you are doing a big projects, you have many sources of information, some of them oral (like end users or IT people interviews) and some other being documents or diagrams or wikis, or existing code. It is very easy to forget important stuff or weak signals that hide structural constraints.\n\n\nWith Archimate, you can define views per source of information and work in back office on the consistency of all the information provided when it begin to touch the same domains or software. At the same time, you can formalize what was said in a specific workshop (process, functions, software, etc.) and recreate this consistent view from the graph model.\n\n\nDespite the fact that it is suite a work, the semantic modeling of Archimate enables to highlight what is consistent and what is not and what complementary information you would need to complete the assessment.\n\n\nVery often in missions, I can say that my model contains \nall\n relevant information that I found.\n\n\nRevolution #5: Managing Dependencies\n\n\nDependencies are the 21rst century problem. Companies have existing IT systems that grew sometimes in a great chaos. Changing stuff is, objectively, complicated, risky, costly.\n\n\nWith Archimate, you can work on dependencies, and so address those risks before the project is started. You can even \"objectify\" the difficulty or a certain project. If you change a system that is not connected to many other systems, it can be easier than trying to change the core system without knowing about the impacts and the problems that will be induced by this change.\n\n\nDependency management are at the heart of the complexity of digital transformation. Because operations must go on, enterprise and IT architects cannot do whatever they want. They have to work on a credible plan that will create a roadmap of transformation taking dependencies into accounts.\n\n\nRevolution #6: Modeling Transformation, Modeling Time\n\n\nModeling digital transformation is a real piece of work if we want to do a serious job. \nBut it is possible\n - and that's a revolution that Archimate enables.\n\n\nYes, it is possible to create, pilot, manage, anticipate, huge  and complex enterprise digital transformation with Archimate. And frankly, I cannot see what other approach can do that.\n\n\nAt a certain point, reading the forums, I am not convinced that many architects really understood this point. We can now create pretty accurate  scenarios of transformation inside Archimate models, taking into account the organization, the strategy, the IT systems, and so on.\n\n\nMaybe we have here the real role of the enterprise architect. Maybe the enterprise architect should be named the \nenterprise \ntransformation\n architect\n.\n\n\nEnterprises should realize that transforming the processes and the IT systems can become an engineering discipline \nat last\n, and that using Archimate for this kind of critical topics enables to avoid many troubles and to spend unnecessary money.\n\n\nModeling transformation is modeling time and so conventions must be chosen to tag artifacts as being existent or not at a certain phase of the transformation process. As of now, the Archimate modeling tools do not support a temporal view of models (like \nFranz Gruff AllegroGraph\n visualizer), we have to take conventions [5].\n\n\nRevolution #7: Using Archimate In Many Software Activities\n\n\nIndeed, many software activities can use Archimate on a regular basis, even if the language is more targeting architecture purposes. When we look carefully at nowadays software, most software are highly interconnected to other software or within the enterprise or in the Cloud. Archimate can help analyzing the structural impacts of those interconnections inside the software itself.\n\n\nCartography of Systems\n\n\nArchimate can be used to create cartographies of IT systems but many architects should realize that Archimate models are working tools more than poster tools.\n\n\nConclusion\n\n\nArchimate is, for me, \nthe\n engineering revolution of digital transformation of the last 15 years. It redefines the enterprise architect role as an enterprise transformation architect role that is able to study with all stakeholders the operational application of a company digital strategy and its impacts on the business, the processes and the IT systems. Once the study is done, Archimate models are of great help to tackle the complex programs, manage the project dependencies and optimize the whole transformation, in terms of time and in terms of efficiency and costs.\n\n\nWe must never forget that the enterprise architect should be a crucial change agent that proposes solutions to a strategic problem and should work in the plain knowledge of the business processes. Most digital transformation offices should also carefully consider using Archimate to manage their transformation plans and try to be in control of the costs and schedule.\n\n\nNotes\n\n\n[1] The only modeling approach that look like Archimate is the \nAris methodology\n. Even if the methodology did not cover the full scope of Archimate, the theoretical approach was similar.\n\n\n[2] We could compare a UML design tool with the \nCAD\n tools. The use of the modeling language must enable to work on the model of the programs before coding in order to create the best design possible, a design that will be implementable quite straight forwardly. Unfortunately with UML, the objective is not reached.\n\n\n[3] Some big Enterprise Architecture frameworks used in the military world are proposing very complex meta-models that are proposing, at the same time, sets of disconnected meta-models and semantic overlap between artifacts. Those frameworks are often at the center of big project failures in the military industry.\n\n\n[4] In projects, I often use or advise to use \"level\" (or scale) indications of some artifacts like business processes or business functions. The indication can be put in the name of the artifact or as an attribute. For instance, \nAccounting\n will become \n(L1) Accounting\n to indicate that, in some views, the business functions will be represented in a high level way. All detailed functions inside this one (aggregation or composition link) will be flagged \n(L2)\n such as \n(L2) Centralization\n. This enables to have various levels of relationships between concepts and to be able to drill down inside the model in order to see more detailed (and more accurate) representations.\n\n\n[5] The temporal conventions can be as the ones we mentioned for scale management, something like \n(P3)\n for phase 3. Tagging the artifact name enables to have possibly many instances of the same artifact, instances that will evolve. The conventions must be chosen very carefully to be able to see what changes and what is remaining the same throughout the whole change process.\n\n\n(January 2018)",
            "title": "An Introduction to The Archimate Revolution"
        },
        {
            "location": "/articles/archimate-intro/#introduction-to-the-archimate-revolution",
            "text": "In the last decade, a quiet revolution took place in the Enterprise Architecture (EA) domain. This revolution is called  Archimate .  This article is the first of a series of articles focusing on some specific aspects of Archimate and practices around the standard.  All Archimate diagrams of this site are done using  Archi . This tool is free and great. If you use it, consider making a donation.",
            "title": "Introduction to the Archimate Revolution"
        },
        {
            "location": "/articles/archimate-intro/#a-semantic-modeling-language",
            "text": "",
            "title": "A Semantic Modeling Language"
        },
        {
            "location": "/articles/archimate-intro/#brief-introduction-to-the-archimate-meta-model",
            "text": "Archimate is a modeling language that enables to describe and study several aspects of the enterprise:   Its strategy and motivations,  Its projects,  And the 4 core layers of enterprise description:  The business layer,  The software layer,  The technology layer (infrastructure),  The physical layer.     All those aspects propose:   Typed artifacts,  Typed relationships between artifacts.   Note that several relationships types can interconnect many different types or can be used in many contexts. The detailed description of all artifacts is not in the scope of this article and we can advise the reader to refer to the book  Enterprise Architecture At Work  from  Mark Lankshorst .  Archimate also defines interlayer relationships which enables to \"connect\" the various layers together. This is an absolutely fundamental feature of the language.",
            "title": "Brief Introduction To The Archimate Meta-Model"
        },
        {
            "location": "/articles/archimate-intro/#a-graph-model",
            "text": "Indeed, an Archimate model is actually a  graph . For those who are familiar with the Semantic Web ( RDF, RDFS , and  OWL ), any Archimate model is a semantic graph.  The graph model is the result of the union of all diagrams implemented with the same set of artifacts. For each artifact, the union of all its incoming and outgoing relationships creates the  neighborhood  of the artifact in the graph model.  When you use Archimate, you represent things in views, using the various artifacts that are available to you. Each element you draw on a certain view (there are many types of views in the standard) will have a certain type, like \"Business Process\" or \"Application Function\". Thus, the resulting model will be a set of views, each of them presenting many interconnected artifacts that all are instances of types that have a specific  meaning .",
            "title": "A Graph Model"
        },
        {
            "location": "/articles/archimate-intro/#viewpoint-and-meta-model",
            "text": "The creation of a semantic language generally implies the creation a meta-model.  Many tools are existing to create meta-models (for instance  Eclipse EMF  with  Sirius , or  MetaEdit+  depending if you want to pay or not).  In the history of enterprise architecture, many approaches defined the notion of \"view point\": in order to act on the enterprise as a whole (or as a  system ), the first step is to describe the enterprise. In order to describe it, due to the complexity of the task, the architect must use several viewpoints. The union of those viewpoints is defining the model of the enterprise.  The  Zachman framework  was one of the first publicly available enterprise architecture \"framework\", enabling to address the many viewpoints describing the enterprise.  The problem with this framework, but also with many other enterprise architecture frameworks, is that they propose many viewpoints, generally each of them proposing a complete  separate  meta-model for each viewpoint (with artifact types and relationship types), but no consistent view of those various viewpoints.  This is a major issue for the framework because:   Separate meta-models for separate viewpoints will, most often, enable to create a globally ambiguous model;  Separate meta-models can create various ways of representing the same reality;  There can be semantic overlap between various viewpoints and their respective meta-model;  What should be a single viewpoint can artificially be split into different view points with different meta-models (we'll see later in this article the dramatic consequences of the Long\u00e9p\u00e9 Enterprise Architecture model in France).   If the consistency is not \"built in\" the framework (i.e. in its meta-model), if it is not part of the primary requirements, then the framework will be very difficult to use.  Indeed, interconnecting various meta-models to be able to create something consistent is not an easy task. Indeed, Archimate seems to be the best illustration of it, and I fear I don't know many other samples [1].",
            "title": "Viewpoint And Meta-Model"
        },
        {
            "location": "/articles/archimate-intro/#samples-of-non-consistent-modeling-approaches",
            "text": "In IT, we have forgotten about the crucial importance of having a consistent way to describe the \"reality\", or let's say the concepts that we are manipulating frequently.",
            "title": "Samples of Non Consistent Modeling Approaches"
        },
        {
            "location": "/articles/archimate-intro/#sample-1-uml",
            "text": "UML can be the first example of the non consistency of a meta-model. UML is not consistent because it proposes many meta-models that are not semantically connected together as a whole.  Suppose you made class diagrams and sequence diagrams for a set of classes. You can add a state diagram for a particular process of a specific class  A  of your model. In UML, there is no way to know if this state diagram is consistent or not with the rest of the diagrams that include  A . This is due to the fact that UML proposes a set of various kinds of views that are not  linked together .  Each view type has its own meta-model. Some of the meta-models are reusing the same artifacts, which enable modeling tools to connect the artifacts to the views they are used into. But many diagram types are just disconnected meta-models.  This problem can perhaps explain why modeling in UML was progressively abandoned by many projects; because it was not really suited to express in a consistent way what the code should be. If, during design, doing UML model cannot prevent you from making design mistakes, the use of the modeling language is much less interesting [2].",
            "title": "Sample #1: UML"
        },
        {
            "location": "/articles/archimate-intro/#sample-2-longepe",
            "text": "Another dramatic inconsistent model is the Long\u00e9p\u00e9 French model which defined, instead of Archimate \"software layer\", two different layers:   One \"functional architecture layer\", that could be interpreted as the Application Function part of the Archimate meta-model,  One \"application architecture layer\", that could be interpreted as the Application Component part of the Archimate meta-model.   As in Archimate, there is, for Long\u00e9p\u00e9, a business layer and an infrastructure layer, which scopes are almost the same than Archimate's.  In the Long\u00e9p\u00e9 model, every layer has a model, and every model can be \"derived\" from the model of the superior layer. This means that the application layer can be \"derived from\" the functional layer. This assertion is obscure, misleading and semantically erroneous.  In Archimate, the application function is \"assigned to\" a application component. That enables to manage the good and the bad assignment, what is called the \"urbanisation\" in French, notion of good or bad function positioning in the global IT systems:   If some functions are well positioned in the IT systems, that means that they communicate easily and can evolve in a natural way without questioning the full architecture;  If some functions are badly positioned in the IT systems (so assigned to the wrong components), we will generate useless dependencies between applications, useless web services, difficulties in terms of evolution, costs and complexity.   The problem of \"function positioning\" is not in the scope of this article, but it is at the heart of the Long\u00e9p\u00e9's reflection, despite the invention of a very bad meta-model that totally confuses the notion through the concept of derivation.   This artificial split of one layer (the application layer of Archimate) into two layers which dependency is erroneously defined, caused  a huge number of French IT projects to fail , and numerous errors of interpretation and understanding for French architects. The book of Christophe Long\u00e9p\u00e9 was at the origin of many misjudgments, errors, confusions and money loss in the French market since the publication of its first edition in 2001.",
            "title": "Sample #2: Long\u00e9p\u00e9"
        },
        {
            "location": "/articles/archimate-intro/#sample-3-projects-creating-their-own-modeling-framework",
            "text": "In consulting missions, I also saw strange practices, as the one of creating a project-specific enterprise architecture modeling framework that evolved throughout the project. Most of the time, the internal framework was incorporating progressively various inconsistent meta-models coming from various modeling standards (such as  UML  or  BPMN  or  TOGAF ).  For sure, most architecture works in the evolving project-specific EA framework were confusing and not usable by software engineers. This often led or contributed to lead the project to its failure.",
            "title": "Sample #3: Projects Creating Their Own Modeling Framework"
        },
        {
            "location": "/articles/archimate-intro/#archimate-a-consistent-approach",
            "text": "Archimate is proposing a consistent meta-model. This is the first fundamental characteristic which is at the heart of the usability of the modeling standard.  I deeply encourage the meta-model creators to think about it, because it defines the real power of the multiple-views paradigm: multiple views on the same reality can be used  provided there can be consistency between those views , the consistency being in a unique meta-model.  Note that proposing a consistent meta-model does not imply that the modeling framework is closed and will not evolve. Archimate proposes, since its version 3, a way to extend the meta-model.  We could say that the multiple views consistency-enable graph model is the revolution #0 of the Archimate language.",
            "title": "Archimate, a Consistent Approach"
        },
        {
            "location": "/articles/archimate-intro/#the-many-revolutions-of-archimate",
            "text": "",
            "title": "The Many Revolutions of Archimate"
        },
        {
            "location": "/articles/archimate-intro/#revolution-1-the-language-just-works",
            "text": "Being consistent is not sufficient for a meta-model.  The  quality  of the meta-model lies in the pertinence of the semantic artifacts that it proposes (being nodes or relationships). Those artifacts must be as semantically clearly defined and must not bring confusion or multiple alternative representations. In particular, semantic overlapping must be avoided as much as possible [3].  In other terms, if the quality of the meta-model is high, then the modeling will be good. However, if the meta-model is bad, the modeling will be very bad and will cause damages in the project.  Indeed, Archimate propose artifacts that enable the non-ambiguous description of the enterprise processes, strategy and IT.  More: it pushes the architects to describe the reality  in a sane way , which means in a way that will push for problem exploration and make visible the possible solutions.  For instance, in the case of function positioning, it is easy to count the dependencies between two functions (through derived relationships). If the count is low and always directed in the same way, that probably means the functions are well positioned; If the count if high and/or the directions go both ways, that probably means that the two functions should be one single function.  Archimate language helps and in that sense, it is doing the job. In other terms: the language just  works .  This is, really, a revolution. In some cases, meta-models induce architects to think badly, to force themselves to think in an inconsistent model where the semantics are confusing. In Archimate, this is not the case.",
            "title": "Revolution #1: The Language Just Works"
        },
        {
            "location": "/articles/archimate-intro/#revolution-2-architects-can-share-and-propose-auditable-works",
            "text": "Even if Archimate will not guarantee that 2 enterprise architects will produce the same modeling when representing the same things, using the same standardized language enable each of them to understand the modeling of the other, to challenge it and to discuss it.  We can forget Visio or Powerpoint schema based modeling, that are very ambiguous at several levels. The works become auditable by other Archimate architects.  For sure, documenting textually the views and the artifact themselves is very helpful. As in all modeling languages, without an effort, some views may be a be difficult to understand if we don't know what question it is supposed to answer.  This is a revolution in Enterprise Architecture but also in IT or business architecture. We can work on something  shareable  and  auditable  and so begin to work the simplistic boxes and arrow diagrams.  Enterprise Architecture is becoming more an engineering discipline when it was considered, too often, as a blur esoteric and confusing witchcraft.",
            "title": "Revolution #2: Architects Can Share And Propose Auditable Works"
        },
        {
            "location": "/articles/archimate-intro/#revolution-3-managing-complex-representations",
            "text": "In Archimate, you can tackle very complex problems.  Due to the fact that you can study several granularity of problems at the same time in the same model [4], the studied complexity can be at several levels:   You can address very big IT systems or one company or of a group of companies;  You can analyze in a very detailed way sets of very small intertwined functionality;  You can study highly distributed systems.   Indeed, big models for IT transformation will quickly contain thousands of artifacts. Note that to ensure consistency on those models, there is a huge systematic work to do.  This is a revolution. The last big program that I did without Archimate was in 2008-2009. It was big, functionally very complex and with a lot of architecture problems. I struggled with UML and BPMN and had to manage the functional consistency by hand. Archimate would have been of great help at the time.",
            "title": "Revolution #3: Managing Complex Representations"
        },
        {
            "location": "/articles/archimate-intro/#revolution-4-aggregate-various-sources-of-knowledge",
            "text": "When you are doing a big projects, you have many sources of information, some of them oral (like end users or IT people interviews) and some other being documents or diagrams or wikis, or existing code. It is very easy to forget important stuff or weak signals that hide structural constraints.  With Archimate, you can define views per source of information and work in back office on the consistency of all the information provided when it begin to touch the same domains or software. At the same time, you can formalize what was said in a specific workshop (process, functions, software, etc.) and recreate this consistent view from the graph model.  Despite the fact that it is suite a work, the semantic modeling of Archimate enables to highlight what is consistent and what is not and what complementary information you would need to complete the assessment.  Very often in missions, I can say that my model contains  all  relevant information that I found.",
            "title": "Revolution #4: Aggregate Various Sources of Knowledge"
        },
        {
            "location": "/articles/archimate-intro/#revolution-5-managing-dependencies",
            "text": "Dependencies are the 21rst century problem. Companies have existing IT systems that grew sometimes in a great chaos. Changing stuff is, objectively, complicated, risky, costly.  With Archimate, you can work on dependencies, and so address those risks before the project is started. You can even \"objectify\" the difficulty or a certain project. If you change a system that is not connected to many other systems, it can be easier than trying to change the core system without knowing about the impacts and the problems that will be induced by this change.  Dependency management are at the heart of the complexity of digital transformation. Because operations must go on, enterprise and IT architects cannot do whatever they want. They have to work on a credible plan that will create a roadmap of transformation taking dependencies into accounts.",
            "title": "Revolution #5: Managing Dependencies"
        },
        {
            "location": "/articles/archimate-intro/#revolution-6-modeling-transformation-modeling-time",
            "text": "Modeling digital transformation is a real piece of work if we want to do a serious job.  But it is possible  - and that's a revolution that Archimate enables.  Yes, it is possible to create, pilot, manage, anticipate, huge  and complex enterprise digital transformation with Archimate. And frankly, I cannot see what other approach can do that.  At a certain point, reading the forums, I am not convinced that many architects really understood this point. We can now create pretty accurate  scenarios of transformation inside Archimate models, taking into account the organization, the strategy, the IT systems, and so on.  Maybe we have here the real role of the enterprise architect. Maybe the enterprise architect should be named the  enterprise  transformation  architect .  Enterprises should realize that transforming the processes and the IT systems can become an engineering discipline  at last , and that using Archimate for this kind of critical topics enables to avoid many troubles and to spend unnecessary money.  Modeling transformation is modeling time and so conventions must be chosen to tag artifacts as being existent or not at a certain phase of the transformation process. As of now, the Archimate modeling tools do not support a temporal view of models (like  Franz Gruff AllegroGraph  visualizer), we have to take conventions [5].",
            "title": "Revolution #6: Modeling Transformation, Modeling Time"
        },
        {
            "location": "/articles/archimate-intro/#revolution-7-using-archimate-in-many-software-activities",
            "text": "Indeed, many software activities can use Archimate on a regular basis, even if the language is more targeting architecture purposes. When we look carefully at nowadays software, most software are highly interconnected to other software or within the enterprise or in the Cloud. Archimate can help analyzing the structural impacts of those interconnections inside the software itself.",
            "title": "Revolution #7: Using Archimate In Many Software Activities"
        },
        {
            "location": "/articles/archimate-intro/#cartography-of-systems",
            "text": "Archimate can be used to create cartographies of IT systems but many architects should realize that Archimate models are working tools more than poster tools.",
            "title": "Cartography of Systems"
        },
        {
            "location": "/articles/archimate-intro/#conclusion",
            "text": "Archimate is, for me,  the  engineering revolution of digital transformation of the last 15 years. It redefines the enterprise architect role as an enterprise transformation architect role that is able to study with all stakeholders the operational application of a company digital strategy and its impacts on the business, the processes and the IT systems. Once the study is done, Archimate models are of great help to tackle the complex programs, manage the project dependencies and optimize the whole transformation, in terms of time and in terms of efficiency and costs.  We must never forget that the enterprise architect should be a crucial change agent that proposes solutions to a strategic problem and should work in the plain knowledge of the business processes. Most digital transformation offices should also carefully consider using Archimate to manage their transformation plans and try to be in control of the costs and schedule.",
            "title": "Conclusion"
        },
        {
            "location": "/articles/archimate-intro/#notes",
            "text": "[1] The only modeling approach that look like Archimate is the  Aris methodology . Even if the methodology did not cover the full scope of Archimate, the theoretical approach was similar.  [2] We could compare a UML design tool with the  CAD  tools. The use of the modeling language must enable to work on the model of the programs before coding in order to create the best design possible, a design that will be implementable quite straight forwardly. Unfortunately with UML, the objective is not reached.  [3] Some big Enterprise Architecture frameworks used in the military world are proposing very complex meta-models that are proposing, at the same time, sets of disconnected meta-models and semantic overlap between artifacts. Those frameworks are often at the center of big project failures in the military industry.  [4] In projects, I often use or advise to use \"level\" (or scale) indications of some artifacts like business processes or business functions. The indication can be put in the name of the artifact or as an attribute. For instance,  Accounting  will become  (L1) Accounting  to indicate that, in some views, the business functions will be represented in a high level way. All detailed functions inside this one (aggregation or composition link) will be flagged  (L2)  such as  (L2) Centralization . This enables to have various levels of relationships between concepts and to be able to drill down inside the model in order to see more detailed (and more accurate) representations.  [5] The temporal conventions can be as the ones we mentioned for scale management, something like  (P3)  for phase 3. Tagging the artifact name enables to have possibly many instances of the same artifact, instances that will evolve. The conventions must be chosen very carefully to be able to see what changes and what is remaining the same throughout the whole change process.  (January 2018)",
            "title": "Notes"
        },
        {
            "location": "/graph/first-article/",
            "text": "First article on graph-oriented programming\n\n\n\n\nIn 2013, I started a personal project around directed attributed graph databases. The objective was to find a programming model that would enable the software to be as \"soft\" as the database.\n\n\nFirst works\n\n\nThe concept was defined and setup during 2014 and 2015.\n\n\nTwo articles on graph transformations were a crucial inspiration to the project.\n\n\nThe AGG Aproach\n published in the volume 2 of the \"Handbook of Graph Grammars\". AGG was particularly inspiring.\n\n\nErmel, Claudia, Michael Rudolf, and Gabriele Taentzer. \"The AGG approach: Language and environment.\" Handbook Of Graph Grammars And Computing By Graph Transformation: Volume 2: Applications, Languages and Tools. 1999. 551-603.\n\n\nAnother article from the same book was also a source of inspiration, \"The Progres approach\" (which doesn't seem to be available online).\n\n\nSch\u00fcrr, Andy, Andreas J. Winter, and Albert Z\u00fcndorf. \"The PROGRES approach: Language and environment.\" Handbook Of Graph Grammars And Computing By Graph Transformation: Volume 2: Applications, Languages and Tools. 1999. 487-550.\n\n\nBeginning of 2015, the graph-oriented programming concept was completed, and its capability of solving the useless technical debt was theoretically proven.\n\n\nThe graph transformations are playing a core role in graph-oriented programming.\n\n\nClosed-source prototype\n\n\nIn 2016, it was prototyped in a company called GraphApps in a closed source approach. I am no longer part of this company.\n\n\nGlobal article on graph-oriented programming\n\n\nA very first article was written in 2016 to explain all the concepts of this programming model.\n\n\nThis article is far from being perfect and didactic but it browses a lot of topics that would deserve special attention (especially in the part concerning graph transformations).\n\n\nThe original article can be found here: \nThe graph-oriented programming paradigm\n.\n\n\n(July 2018)",
            "title": "First article on graph-oriented programming"
        },
        {
            "location": "/graph/first-article/#first-article-on-graph-oriented-programming",
            "text": "In 2013, I started a personal project around directed attributed graph databases. The objective was to find a programming model that would enable the software to be as \"soft\" as the database.",
            "title": "First article on graph-oriented programming"
        },
        {
            "location": "/graph/first-article/#first-works",
            "text": "The concept was defined and setup during 2014 and 2015.  Two articles on graph transformations were a crucial inspiration to the project.  The AGG Aproach  published in the volume 2 of the \"Handbook of Graph Grammars\". AGG was particularly inspiring.  Ermel, Claudia, Michael Rudolf, and Gabriele Taentzer. \"The AGG approach: Language and environment.\" Handbook Of Graph Grammars And Computing By Graph Transformation: Volume 2: Applications, Languages and Tools. 1999. 551-603.  Another article from the same book was also a source of inspiration, \"The Progres approach\" (which doesn't seem to be available online).  Sch\u00fcrr, Andy, Andreas J. Winter, and Albert Z\u00fcndorf. \"The PROGRES approach: Language and environment.\" Handbook Of Graph Grammars And Computing By Graph Transformation: Volume 2: Applications, Languages and Tools. 1999. 487-550.  Beginning of 2015, the graph-oriented programming concept was completed, and its capability of solving the useless technical debt was theoretically proven.  The graph transformations are playing a core role in graph-oriented programming.",
            "title": "First works"
        },
        {
            "location": "/graph/first-article/#closed-source-prototype",
            "text": "In 2016, it was prototyped in a company called GraphApps in a closed source approach. I am no longer part of this company.",
            "title": "Closed-source prototype"
        },
        {
            "location": "/graph/first-article/#global-article-on-graph-oriented-programming",
            "text": "A very first article was written in 2016 to explain all the concepts of this programming model.  This article is far from being perfect and didactic but it browses a lot of topics that would deserve special attention (especially in the part concerning graph transformations).  The original article can be found here:  The graph-oriented programming paradigm .  (July 2018)",
            "title": "Global article on graph-oriented programming"
        },
        {
            "location": "/graph/staf-icgt2018/",
            "text": "Conference at the STAF/ICGT 2018 in Toulouse\n\n\n \n\n\nThe prototyping works of the GraphApps company were stopped in December 2016 for economic reasons, the products (Design and Web Framework) staying closed source.\n\n\nHowever, I kept on working on the graph-oriented programming topic, in particular in the perspective of solving the technical debt.\n\n\nMarburg University\n\n\nDuring Fall 2017, some conference calls were organized with Prof. Dr. \nGabriele Taentzer\n. Those discussions were about graph-oriented programming but also about \nHenshin\n.\n\n\nBeginning of 2018, Prof. Dr. Taentzer proposed that I was keynote speaker at the \nSTAF/ICGT 2018\n in Toulouse, conference organized by Dr. \nLeen Lambers\n and Prof. Dr. \nJens Weber\n. I thank all of them for their invitation and support.\n\n\nIntroductory papers\n\n\nFor the conference, two introductory papers were produced:\n\n\n\n\nAn abstract of the presentation\n for the \nSpringer proceedings\n;\n\n\nA non published introduction\n to graph-oriented programming.\n\n\n\n\nSlides of the presentation\n\n\nThe slides of the presentation can be found hereafter: \nIntroduction to graph-oriented programming\n\n\n\n\nA copy of those slides can also be found on the conference website \nhere\n.\n\n\nWhat's next?\n\n\nMy works on graph-oriented programming will go on, but with probably less time that I had in 2016.\n\n\n(July 2018)",
            "title": "Conference at the STAF/ICGT 2018 in Toulouse"
        },
        {
            "location": "/graph/staf-icgt2018/#conference-at-the-staficgt-2018-in-toulouse",
            "text": "The prototyping works of the GraphApps company were stopped in December 2016 for economic reasons, the products (Design and Web Framework) staying closed source.  However, I kept on working on the graph-oriented programming topic, in particular in the perspective of solving the technical debt.",
            "title": "Conference at the STAF/ICGT 2018 in Toulouse"
        },
        {
            "location": "/graph/staf-icgt2018/#marburg-university",
            "text": "During Fall 2017, some conference calls were organized with Prof. Dr.  Gabriele Taentzer . Those discussions were about graph-oriented programming but also about  Henshin .  Beginning of 2018, Prof. Dr. Taentzer proposed that I was keynote speaker at the  STAF/ICGT 2018  in Toulouse, conference organized by Dr.  Leen Lambers  and Prof. Dr.  Jens Weber . I thank all of them for their invitation and support.",
            "title": "Marburg University"
        },
        {
            "location": "/graph/staf-icgt2018/#introductory-papers",
            "text": "For the conference, two introductory papers were produced:   An abstract of the presentation  for the  Springer proceedings ;  A non published introduction  to graph-oriented programming.",
            "title": "Introductory papers"
        },
        {
            "location": "/graph/staf-icgt2018/#slides-of-the-presentation",
            "text": "The slides of the presentation can be found hereafter:  Introduction to graph-oriented programming   A copy of those slides can also be found on the conference website  here .",
            "title": "Slides of the presentation"
        },
        {
            "location": "/graph/staf-icgt2018/#whats-next",
            "text": "My works on graph-oriented programming will go on, but with probably less time that I had in 2016.  (July 2018)",
            "title": "What's next?"
        },
        {
            "location": "/about/about/",
            "text": "About The Author\n\n\nOlivier Rey\n\n\n\n\nSee my \nLinkedIn profile\n\n\nWrite to me: rey [dot] olivier [at] gmail [dot] com\n\n\n\n\nAreas of interest\n\n\n\n\nSoftware engineering, programming languages, programming paradigms\n\n\nModeling\n\n\nArchimate and enterprise architecture modeling frameworks\n\n\nMBSE\n\n\nDomain specific modeling\n\n\nDSL\n\n\n\n\n\n\nMathematics (topology, graphs, category theory, etc.)\n\n\n\n\nArticles to write or migrate\n\n\n\n\nArchimate version of the \nportfolio management metamodel\n.\n\n\nDeming and the system of profound knowledge\n\n\nFrench articles\n\n\n\n\nOld stuff\n\n\n\n\nMigrate interesting stuff from old blog (and translate)\n\n\nVB and Delphi refactoring practices\n\n\n\n\n\n\n\n\nGraph-oriented programming\n\n\n\n\nNew demo version (designer and web framework), open source this time\n\n\n\n\nOngoing graph studies and questions\n\n\nThese are my current topics of interest.\n\n\n\n\nModeling complex temporal events with graphs and topological graph transformations\n\n\nLike highway traffic\n\n\n\n\n\n\nThe graph transformation changes the graph topology. What kind of mathematical object is it?\n\n\nScale laws in graph (for instance adherence of domains in a multi-graph mode)\n\n\nWhat is the true nature of interdomain relationship? And how does it relate to the higher level relationship?\n\n\n\n\n\n\nDistance comparison between two graphs\n\n\nThe fact one and the referential one\n\n\nUseful for AI projects, knowledge representation\n\n\n\n\n\n\nIsomorphic semantic representations: what graph transformations enable that?\n\n\nModeling psychology frameworks (Freud, Jung, etc.) with graph patterns and graph distance measurement\n\n\nRewiring rules and their limitations\n\n\nCloning nodes and sub-graph: rules?\n\n\n\n\n(July 2018)",
            "title": "Author"
        },
        {
            "location": "/about/about/#about-the-author",
            "text": "",
            "title": "About The Author"
        },
        {
            "location": "/about/about/#olivier-rey",
            "text": "See my  LinkedIn profile  Write to me: rey [dot] olivier [at] gmail [dot] com",
            "title": "Olivier Rey"
        },
        {
            "location": "/about/about/#areas-of-interest",
            "text": "Software engineering, programming languages, programming paradigms  Modeling  Archimate and enterprise architecture modeling frameworks  MBSE  Domain specific modeling  DSL    Mathematics (topology, graphs, category theory, etc.)",
            "title": "Areas of interest"
        },
        {
            "location": "/about/about/#articles-to-write-or-migrate",
            "text": "Archimate version of the  portfolio management metamodel .  Deming and the system of profound knowledge  French articles   Old stuff   Migrate interesting stuff from old blog (and translate)  VB and Delphi refactoring practices",
            "title": "Articles to write or migrate"
        },
        {
            "location": "/about/about/#graph-oriented-programming",
            "text": "New demo version (designer and web framework), open source this time",
            "title": "Graph-oriented programming"
        },
        {
            "location": "/about/about/#ongoing-graph-studies-and-questions",
            "text": "These are my current topics of interest.   Modeling complex temporal events with graphs and topological graph transformations  Like highway traffic    The graph transformation changes the graph topology. What kind of mathematical object is it?  Scale laws in graph (for instance adherence of domains in a multi-graph mode)  What is the true nature of interdomain relationship? And how does it relate to the higher level relationship?    Distance comparison between two graphs  The fact one and the referential one  Useful for AI projects, knowledge representation    Isomorphic semantic representations: what graph transformations enable that?  Modeling psychology frameworks (Freud, Jung, etc.) with graph patterns and graph distance measurement  Rewiring rules and their limitations  Cloning nodes and sub-graph: rules?   (July 2018)",
            "title": "Ongoing graph studies and questions"
        },
        {
            "location": "/about/LICENSE/",
            "text": "This site is licensed under the terms and conditions of the GNU FDL V3 that can be found hereafter.\n\n\nGNU Free Documentation License\n\n\nVersion 1.3, 3 November 2008\n\n\nCopyright \u00a9 2000, 2001, 2002, 2007, 2008 Free Software Foundation, Inc. \nhttps://fsf.org/\n\n\nEveryone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n\n\n0. PREAMBLE\n\n\nThe purpose of this License is to make a manual, textbook, or other functional and useful document \"free\" in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or noncommercially. Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others.\n\n\nThis License is a kind of \"copyleft\", which means that derivative works of the document must themselves be free in the same sense. It complements the GNU General Public License, which is a copyleft license designed for free software.\n\n\nWe have designed this License in order to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does. But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book. We recommend this License principally for works whose purpose is instruction or reference.\n\n\n1. APPLICABILITY AND DEFINITIONS\n\n\nThis License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License. Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein. The \"Document\", below, refers to any such manual or work. Any member of the public is a licensee, and is addressed as \"you\". You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law.\n\n\nA \"Modified Version\" of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language.\n\n\nA \"Secondary Section\" is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document's overall subject (or to related matters) and contains nothing that could fall directly within that overall subject. (Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them.\n\n\nThe \"Invariant Sections\" are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License. If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant. The Document may contain zero Invariant Sections. If the Document does not identify any Invariant Sections then there are none.\n\n\nThe \"Cover Texts\" are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License. A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words.\n\n\nA \"Transparent\" copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters. A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent. An image format is not Transparent if used for any substantial amount of text. A copy that is not \"Transparent\" is called \"Opaque\".\n\n\nExamples of suitable formats for Transparent copies include plain ASCII without markup, Texinfo input format, LaTeX input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification. Examples of transparent image formats include PNG, XCF and JPG. Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only.\n\n\nThe \"Title Page\" means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page. For works in formats which do not have any title page as such, \"Title Page\" means the text near the most prominent appearance of the work's title, preceding the beginning of the body of the text.\n\n\nThe \"publisher\" means any person or entity that distributes copies of the Document to the public.\n\n\nA section \"Entitled XYZ\" means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language. (Here XYZ stands for a specific section name mentioned below, such as \"Acknowledgements\", \"Dedications\", \"Endorsements\", or \"History\".) To \"Preserve the Title\" of such a section when you modify the Document means that it remains a section \"Entitled XYZ\" according to this definition.\n\n\nThe Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document. These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License.\n\n\n2. VERBATIM COPYING\n\n\nYou may copy and distribute the Document in any medium, either commercially or noncommercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License. You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute. However, you may accept compensation in exchange for copies. If you distribute a large enough number of copies you must also follow the conditions in section 3.\n\n\nYou may also lend copies, under the same conditions stated above, and you may publicly display copies.\n\n\n3. COPYING IN QUANTITY\n\n\nIf you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document's license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover. Both covers must also clearly and legibly identify you as the publisher of these copies. The front cover must present the full title with all words of the title equally prominent and visible. You may add other material on the covers in addition. Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects.\n\n\nIf the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages.\n\n\nIf you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material. If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public.\n\n\nIt is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document.\n\n\n4. MODIFICATIONS\n\n\nYou may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it. In addition, you must do these things in the Modified Version:\n\n\nA. Use in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission.\nB. List on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement.\nC. State on the Title page the name of the publisher of the Modified Version, as the publisher.\nD. Preserve all the copyright notices of the Document.\nE. Add an appropriate copyright notice for your modifications adjacent to the other copyright notices.\nF. Include, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below.\nG. Preserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document's license notice.\nH. Include an unaltered copy of this License.\nI. Preserve the section Entitled \"History\", Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled \"History\" in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence.\nJ. Preserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the \"History\" section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission.\nK. For any section Entitled \"Acknowledgements\" or \"Dedications\", Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein.\nL. Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles.\nM. Delete any section Entitled \"Endorsements\". Such a section may not be included in the Modified Version.\nN. Do not retitle any existing section to be Entitled \"Endorsements\" or to conflict in title with any Invariant Section.\nO. Preserve any Warranty Disclaimers.\n\n\nIf the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant. To do this, add their titles to the list of Invariant Sections in the Modified Version's license notice. These titles must be distinct from any other section titles.\n\n\nYou may add a section Entitled \"Endorsements\", provided it contains nothing but endorsements of your Modified Version by various parties\u2014for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard.\n\n\nYou may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version. Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity. If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one.\n\n\nThe author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version.\n\n\n5. COMBINING DOCUMENTS\n\n\nYou may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers.\n\n\nThe combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy. If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number. Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work.\n\n\nIn the combination, you must combine any sections Entitled \"History\" in the various original documents, forming one section Entitled \"History\"; likewise combine any sections Entitled \"Acknowledgements\", and any sections Entitled \"Dedications\". You must delete all sections Entitled \"Endorsements\".\n\n\n6. COLLECTIONS OF DOCUMENTS\n\n\nYou may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects.\n\n\nYou may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document.\n\n\n7. AGGREGATION WITH INDEPENDENT WORKS\n\n\nA compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the copyright resulting from the compilation is not used to limit the legal rights of the compilation's users beyond what the individual works permit. When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document.\n\n\nIf the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document's Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form. Otherwise they must appear on printed covers that bracket the whole aggregate.\n\n\n8. TRANSLATION\n\n\nTranslation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4. Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections. You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers. In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail.\n\n\nIf a section in the Document is Entitled \"Acknowledgements\", \"Dedications\", or \"History\", the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title.\n\n\n9. TERMINATION\n\n\nYou may not copy, modify, sublicense, or distribute the Document except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense, or distribute it is void, and will automatically terminate your rights under this License.\n\n\nHowever, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.\n\n\nMoreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.\n\n\nTermination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, receipt of a copy of some or all of the same material does not give you any rights to use it.\n\n\n10. FUTURE REVISIONS OF THIS LICENSE\n\n\nThe Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. See https://www.gnu.org/licenses/.\n\n\nEach version of the License is given a distinguishing version number. If the Document specifies that a particular numbered version of this License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation. If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation. If the Document specifies that a proxy can decide which future versions of this License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Document.\n\n\n11. RELICENSING\n\n\n\"Massive Multiauthor Collaboration Site\" (or \"MMC Site\") means any World Wide Web server that publishes copyrightable works and also provides prominent facilities for anybody to edit those works. A public wiki that anybody can edit is an example of such a server. A \"Massive Multiauthor Collaboration\" (or \"MMC\") contained in the site means any set of copyrightable works thus published on the MMC site.\n\n\n\"CC-BY-SA\" means the Creative Commons Attribution-Share Alike 3.0 license published by Creative Commons Corporation, a not-for-profit corporation with a principal place of business in San Francisco, California, as well as future copyleft versions of that license published by that same organization.\n\n\n\"Incorporate\" means to publish or republish a Document, in whole or in part, as part of another Document.\n\n\nAn MMC is \"eligible for relicensing\" if it is licensed under this License, and if all works that were first published under this License somewhere other than this MMC, and subsequently incorporated in whole or in part into the MMC, (1) had no cover texts or invariant sections, and (2) were thus incorporated prior to November 1, 2008.\n\n\nThe operator of an MMC Site may republish an MMC contained in the site under CC-BY-SA on the same site at any time before August 1, 2009, provided the MMC is eligible for relicensing.\nADDENDUM: How to use this License for your documents\n\n\nTo use this License in a document you have written, include a copy of the License in the document and put the following copyright and license notices just after the title page:",
            "title": "License"
        },
        {
            "location": "/about/LICENSE/#gnu-free-documentation-license",
            "text": "Version 1.3, 3 November 2008  Copyright \u00a9 2000, 2001, 2002, 2007, 2008 Free Software Foundation, Inc.  https://fsf.org/  Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.",
            "title": "GNU Free Documentation License"
        },
        {
            "location": "/about/LICENSE/#0-preamble",
            "text": "The purpose of this License is to make a manual, textbook, or other functional and useful document \"free\" in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or noncommercially. Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others.  This License is a kind of \"copyleft\", which means that derivative works of the document must themselves be free in the same sense. It complements the GNU General Public License, which is a copyleft license designed for free software.  We have designed this License in order to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does. But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book. We recommend this License principally for works whose purpose is instruction or reference.",
            "title": "0. PREAMBLE"
        },
        {
            "location": "/about/LICENSE/#1-applicability-and-definitions",
            "text": "This License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License. Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein. The \"Document\", below, refers to any such manual or work. Any member of the public is a licensee, and is addressed as \"you\". You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law.  A \"Modified Version\" of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language.  A \"Secondary Section\" is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document's overall subject (or to related matters) and contains nothing that could fall directly within that overall subject. (Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them.  The \"Invariant Sections\" are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License. If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant. The Document may contain zero Invariant Sections. If the Document does not identify any Invariant Sections then there are none.  The \"Cover Texts\" are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License. A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words.  A \"Transparent\" copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters. A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent. An image format is not Transparent if used for any substantial amount of text. A copy that is not \"Transparent\" is called \"Opaque\".  Examples of suitable formats for Transparent copies include plain ASCII without markup, Texinfo input format, LaTeX input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification. Examples of transparent image formats include PNG, XCF and JPG. Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only.  The \"Title Page\" means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page. For works in formats which do not have any title page as such, \"Title Page\" means the text near the most prominent appearance of the work's title, preceding the beginning of the body of the text.  The \"publisher\" means any person or entity that distributes copies of the Document to the public.  A section \"Entitled XYZ\" means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language. (Here XYZ stands for a specific section name mentioned below, such as \"Acknowledgements\", \"Dedications\", \"Endorsements\", or \"History\".) To \"Preserve the Title\" of such a section when you modify the Document means that it remains a section \"Entitled XYZ\" according to this definition.  The Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document. These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License.",
            "title": "1. APPLICABILITY AND DEFINITIONS"
        },
        {
            "location": "/about/LICENSE/#2-verbatim-copying",
            "text": "You may copy and distribute the Document in any medium, either commercially or noncommercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License. You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute. However, you may accept compensation in exchange for copies. If you distribute a large enough number of copies you must also follow the conditions in section 3.  You may also lend copies, under the same conditions stated above, and you may publicly display copies.",
            "title": "2. VERBATIM COPYING"
        },
        {
            "location": "/about/LICENSE/#3-copying-in-quantity",
            "text": "If you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document's license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover. Both covers must also clearly and legibly identify you as the publisher of these copies. The front cover must present the full title with all words of the title equally prominent and visible. You may add other material on the covers in addition. Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects.  If the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages.  If you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material. If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public.  It is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document.",
            "title": "3. COPYING IN QUANTITY"
        },
        {
            "location": "/about/LICENSE/#4-modifications",
            "text": "You may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it. In addition, you must do these things in the Modified Version:  A. Use in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission.\nB. List on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement.\nC. State on the Title page the name of the publisher of the Modified Version, as the publisher.\nD. Preserve all the copyright notices of the Document.\nE. Add an appropriate copyright notice for your modifications adjacent to the other copyright notices.\nF. Include, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below.\nG. Preserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document's license notice.\nH. Include an unaltered copy of this License.\nI. Preserve the section Entitled \"History\", Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled \"History\" in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence.\nJ. Preserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the \"History\" section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission.\nK. For any section Entitled \"Acknowledgements\" or \"Dedications\", Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein.\nL. Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles.\nM. Delete any section Entitled \"Endorsements\". Such a section may not be included in the Modified Version.\nN. Do not retitle any existing section to be Entitled \"Endorsements\" or to conflict in title with any Invariant Section.\nO. Preserve any Warranty Disclaimers.  If the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant. To do this, add their titles to the list of Invariant Sections in the Modified Version's license notice. These titles must be distinct from any other section titles.  You may add a section Entitled \"Endorsements\", provided it contains nothing but endorsements of your Modified Version by various parties\u2014for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard.  You may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version. Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity. If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one.  The author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version.",
            "title": "4. MODIFICATIONS"
        },
        {
            "location": "/about/LICENSE/#5-combining-documents",
            "text": "You may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers.  The combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy. If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number. Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work.  In the combination, you must combine any sections Entitled \"History\" in the various original documents, forming one section Entitled \"History\"; likewise combine any sections Entitled \"Acknowledgements\", and any sections Entitled \"Dedications\". You must delete all sections Entitled \"Endorsements\".",
            "title": "5. COMBINING DOCUMENTS"
        },
        {
            "location": "/about/LICENSE/#6-collections-of-documents",
            "text": "You may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects.  You may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document.",
            "title": "6. COLLECTIONS OF DOCUMENTS"
        },
        {
            "location": "/about/LICENSE/#7-aggregation-with-independent-works",
            "text": "A compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the copyright resulting from the compilation is not used to limit the legal rights of the compilation's users beyond what the individual works permit. When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document.  If the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document's Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form. Otherwise they must appear on printed covers that bracket the whole aggregate.",
            "title": "7. AGGREGATION WITH INDEPENDENT WORKS"
        },
        {
            "location": "/about/LICENSE/#8-translation",
            "text": "Translation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4. Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections. You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers. In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail.  If a section in the Document is Entitled \"Acknowledgements\", \"Dedications\", or \"History\", the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title.",
            "title": "8. TRANSLATION"
        },
        {
            "location": "/about/LICENSE/#9-termination",
            "text": "You may not copy, modify, sublicense, or distribute the Document except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense, or distribute it is void, and will automatically terminate your rights under this License.  However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation.  Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice.  Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, receipt of a copy of some or all of the same material does not give you any rights to use it.",
            "title": "9. TERMINATION"
        },
        {
            "location": "/about/LICENSE/#10-future-revisions-of-this-license",
            "text": "The Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. See https://www.gnu.org/licenses/.  Each version of the License is given a distinguishing version number. If the Document specifies that a particular numbered version of this License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation. If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation. If the Document specifies that a proxy can decide which future versions of this License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Document.",
            "title": "10. FUTURE REVISIONS OF THIS LICENSE"
        },
        {
            "location": "/about/LICENSE/#11-relicensing",
            "text": "\"Massive Multiauthor Collaboration Site\" (or \"MMC Site\") means any World Wide Web server that publishes copyrightable works and also provides prominent facilities for anybody to edit those works. A public wiki that anybody can edit is an example of such a server. A \"Massive Multiauthor Collaboration\" (or \"MMC\") contained in the site means any set of copyrightable works thus published on the MMC site.  \"CC-BY-SA\" means the Creative Commons Attribution-Share Alike 3.0 license published by Creative Commons Corporation, a not-for-profit corporation with a principal place of business in San Francisco, California, as well as future copyleft versions of that license published by that same organization.  \"Incorporate\" means to publish or republish a Document, in whole or in part, as part of another Document.  An MMC is \"eligible for relicensing\" if it is licensed under this License, and if all works that were first published under this License somewhere other than this MMC, and subsequently incorporated in whole or in part into the MMC, (1) had no cover texts or invariant sections, and (2) were thus incorporated prior to November 1, 2008.  The operator of an MMC Site may republish an MMC contained in the site under CC-BY-SA on the same site at any time before August 1, 2009, provided the MMC is eligible for relicensing.\nADDENDUM: How to use this License for your documents  To use this License in a document you have written, include a copy of the License in the document and put the following copyright and license notices just after the title page:",
            "title": "11. RELICENSING"
        }
    ]
}