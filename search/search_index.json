{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Olivier Rey's Papers Papers NEW - The real nature of data Military Frameworks, Systems Engineering and Enterprise Architecture Archimate Recipes An Introduction to The Archimate Revolution A Simple Meta-Model for Portfolio Management GraphQL And Classic Web Services Considerations about Rest and Web Services The Various Stages of Digital Transformation The V2 Vortex The Five Levels of Conceptual Maturity for IT Teams Graph-oriented programming This section contains materials about graph-oriented programming, solving the technical debt issue and using graph transformations in software engineering. First article on graph-oriented programming Conference at the STAF/ICGT 2018 in Toulouse Project page: GraphApps project project (2013-2018) Semantic web Using semantic web technologies for aerospace industrial data migration, Madics 2019 Project page: Semantic data migration project (2018-now) Research section Index Graph transformations Basic graph transformations Basic semantic graph transformations DSL for graph topology check Graph transformation applicability Grammar of graph transformation Graph-oriented programming language More Reflections on industry data RDF design patterns - ongoing, important in a professional context Towards a graph-oriented programming language? Resources (Last update: June 2020)","title":"Home"},{"location":"#olivier-reys-papers","text":"","title":"Olivier Rey's Papers"},{"location":"#papers","text":"NEW - The real nature of data Military Frameworks, Systems Engineering and Enterprise Architecture Archimate Recipes An Introduction to The Archimate Revolution A Simple Meta-Model for Portfolio Management GraphQL And Classic Web Services Considerations about Rest and Web Services The Various Stages of Digital Transformation The V2 Vortex The Five Levels of Conceptual Maturity for IT Teams","title":"Papers"},{"location":"#graph-oriented-programming","text":"This section contains materials about graph-oriented programming, solving the technical debt issue and using graph transformations in software engineering. First article on graph-oriented programming Conference at the STAF/ICGT 2018 in Toulouse Project page: GraphApps project project (2013-2018)","title":"Graph-oriented programming"},{"location":"#semantic-web","text":"Using semantic web technologies for aerospace industrial data migration, Madics 2019 Project page: Semantic data migration project (2018-now)","title":"Semantic web"},{"location":"#research-section","text":"Index Graph transformations Basic graph transformations Basic semantic graph transformations DSL for graph topology check Graph transformation applicability Grammar of graph transformation Graph-oriented programming language More Reflections on industry data RDF design patterns - ongoing, important in a professional context Towards a graph-oriented programming language? Resources (Last update: June 2020)","title":"Research section"},{"location":"about/LICENSE/","text":"This site is licensed under the terms and conditions of the GNU FDL V3 that can be found hereafter. GNU Free Documentation License Version 1.3, 3 November 2008 Copyright \u00a9 2000, 2001, 2002, 2007, 2008 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. 0. PREAMBLE The purpose of this License is to make a manual, textbook, or other functional and useful document \"free\" in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or noncommercially. Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others. This License is a kind of \"copyleft\", which means that derivative works of the document must themselves be free in the same sense. It complements the GNU General Public License, which is a copyleft license designed for free software. We have designed this License in order to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does. But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book. We recommend this License principally for works whose purpose is instruction or reference. 1. APPLICABILITY AND DEFINITIONS This License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License. Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein. The \"Document\", below, refers to any such manual or work. Any member of the public is a licensee, and is addressed as \"you\". You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law. A \"Modified Version\" of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language. A \"Secondary Section\" is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document's overall subject (or to related matters) and contains nothing that could fall directly within that overall subject. (Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them. The \"Invariant Sections\" are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License. If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant. The Document may contain zero Invariant Sections. If the Document does not identify any Invariant Sections then there are none. The \"Cover Texts\" are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License. A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words. A \"Transparent\" copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters. A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent. An image format is not Transparent if used for any substantial amount of text. A copy that is not \"Transparent\" is called \"Opaque\". Examples of suitable formats for Transparent copies include plain ASCII without markup, Texinfo input format, LaTeX input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification. Examples of transparent image formats include PNG, XCF and JPG. Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only. The \"Title Page\" means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page. For works in formats which do not have any title page as such, \"Title Page\" means the text near the most prominent appearance of the work's title, preceding the beginning of the body of the text. The \"publisher\" means any person or entity that distributes copies of the Document to the public. A section \"Entitled XYZ\" means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language. (Here XYZ stands for a specific section name mentioned below, such as \"Acknowledgements\", \"Dedications\", \"Endorsements\", or \"History\".) To \"Preserve the Title\" of such a section when you modify the Document means that it remains a section \"Entitled XYZ\" according to this definition. The Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document. These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License. 2. VERBATIM COPYING You may copy and distribute the Document in any medium, either commercially or noncommercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License. You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute. However, you may accept compensation in exchange for copies. If you distribute a large enough number of copies you must also follow the conditions in section 3. You may also lend copies, under the same conditions stated above, and you may publicly display copies. 3. COPYING IN QUANTITY If you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document's license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover. Both covers must also clearly and legibly identify you as the publisher of these copies. The front cover must present the full title with all words of the title equally prominent and visible. You may add other material on the covers in addition. Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects. If the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages. If you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material. If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public. It is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document. 4. MODIFICATIONS You may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it. In addition, you must do these things in the Modified Version: A. Use in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission. B. List on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement. C. State on the Title page the name of the publisher of the Modified Version, as the publisher. D. Preserve all the copyright notices of the Document. E. Add an appropriate copyright notice for your modifications adjacent to the other copyright notices. F. Include, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below. G. Preserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document's license notice. H. Include an unaltered copy of this License. I. Preserve the section Entitled \"History\", Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled \"History\" in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence. J. Preserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the \"History\" section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission. K. For any section Entitled \"Acknowledgements\" or \"Dedications\", Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein. L. Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles. M. Delete any section Entitled \"Endorsements\". Such a section may not be included in the Modified Version. N. Do not retitle any existing section to be Entitled \"Endorsements\" or to conflict in title with any Invariant Section. O. Preserve any Warranty Disclaimers. If the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant. To do this, add their titles to the list of Invariant Sections in the Modified Version's license notice. These titles must be distinct from any other section titles. You may add a section Entitled \"Endorsements\", provided it contains nothing but endorsements of your Modified Version by various parties\u2014for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard. You may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version. Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity. If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one. The author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version. 5. COMBINING DOCUMENTS You may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers. The combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy. If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number. Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work. In the combination, you must combine any sections Entitled \"History\" in the various original documents, forming one section Entitled \"History\"; likewise combine any sections Entitled \"Acknowledgements\", and any sections Entitled \"Dedications\". You must delete all sections Entitled \"Endorsements\". 6. COLLECTIONS OF DOCUMENTS You may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects. You may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document. 7. AGGREGATION WITH INDEPENDENT WORKS A compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the copyright resulting from the compilation is not used to limit the legal rights of the compilation's users beyond what the individual works permit. When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document. If the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document's Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form. Otherwise they must appear on printed covers that bracket the whole aggregate. 8. TRANSLATION Translation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4. Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections. You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers. In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail. If a section in the Document is Entitled \"Acknowledgements\", \"Dedications\", or \"History\", the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title. 9. TERMINATION You may not copy, modify, sublicense, or distribute the Document except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense, or distribute it is void, and will automatically terminate your rights under this License. However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, receipt of a copy of some or all of the same material does not give you any rights to use it. 10. FUTURE REVISIONS OF THIS LICENSE The Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. See https://www.gnu.org/licenses/. Each version of the License is given a distinguishing version number. If the Document specifies that a particular numbered version of this License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation. If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation. If the Document specifies that a proxy can decide which future versions of this License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Document. 11. RELICENSING \"Massive Multiauthor Collaboration Site\" (or \"MMC Site\") means any World Wide Web server that publishes copyrightable works and also provides prominent facilities for anybody to edit those works. A public wiki that anybody can edit is an example of such a server. A \"Massive Multiauthor Collaboration\" (or \"MMC\") contained in the site means any set of copyrightable works thus published on the MMC site. \"CC-BY-SA\" means the Creative Commons Attribution-Share Alike 3.0 license published by Creative Commons Corporation, a not-for-profit corporation with a principal place of business in San Francisco, California, as well as future copyleft versions of that license published by that same organization. \"Incorporate\" means to publish or republish a Document, in whole or in part, as part of another Document. An MMC is \"eligible for relicensing\" if it is licensed under this License, and if all works that were first published under this License somewhere other than this MMC, and subsequently incorporated in whole or in part into the MMC, (1) had no cover texts or invariant sections, and (2) were thus incorporated prior to November 1, 2008. The operator of an MMC Site may republish an MMC contained in the site under CC-BY-SA on the same site at any time before August 1, 2009, provided the MMC is eligible for relicensing. ADDENDUM: How to use this License for your documents To use this License in a document you have written, include a copy of the License in the document and put the following copyright and license notices just after the title page:","title":"License"},{"location":"about/LICENSE/#gnu-free-documentation-license","text":"Version 1.3, 3 November 2008 Copyright \u00a9 2000, 2001, 2002, 2007, 2008 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.","title":"GNU Free Documentation License"},{"location":"about/LICENSE/#0-preamble","text":"The purpose of this License is to make a manual, textbook, or other functional and useful document \"free\" in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or noncommercially. Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others. This License is a kind of \"copyleft\", which means that derivative works of the document must themselves be free in the same sense. It complements the GNU General Public License, which is a copyleft license designed for free software. We have designed this License in order to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does. But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book. We recommend this License principally for works whose purpose is instruction or reference.","title":"0. PREAMBLE"},{"location":"about/LICENSE/#1-applicability-and-definitions","text":"This License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License. Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein. The \"Document\", below, refers to any such manual or work. Any member of the public is a licensee, and is addressed as \"you\". You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law. A \"Modified Version\" of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language. A \"Secondary Section\" is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document's overall subject (or to related matters) and contains nothing that could fall directly within that overall subject. (Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them. The \"Invariant Sections\" are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License. If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant. The Document may contain zero Invariant Sections. If the Document does not identify any Invariant Sections then there are none. The \"Cover Texts\" are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License. A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words. A \"Transparent\" copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters. A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent. An image format is not Transparent if used for any substantial amount of text. A copy that is not \"Transparent\" is called \"Opaque\". Examples of suitable formats for Transparent copies include plain ASCII without markup, Texinfo input format, LaTeX input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification. Examples of transparent image formats include PNG, XCF and JPG. Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only. The \"Title Page\" means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page. For works in formats which do not have any title page as such, \"Title Page\" means the text near the most prominent appearance of the work's title, preceding the beginning of the body of the text. The \"publisher\" means any person or entity that distributes copies of the Document to the public. A section \"Entitled XYZ\" means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language. (Here XYZ stands for a specific section name mentioned below, such as \"Acknowledgements\", \"Dedications\", \"Endorsements\", or \"History\".) To \"Preserve the Title\" of such a section when you modify the Document means that it remains a section \"Entitled XYZ\" according to this definition. The Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document. These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License.","title":"1. APPLICABILITY AND DEFINITIONS"},{"location":"about/LICENSE/#2-verbatim-copying","text":"You may copy and distribute the Document in any medium, either commercially or noncommercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License. You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute. However, you may accept compensation in exchange for copies. If you distribute a large enough number of copies you must also follow the conditions in section 3. You may also lend copies, under the same conditions stated above, and you may publicly display copies.","title":"2. VERBATIM COPYING"},{"location":"about/LICENSE/#3-copying-in-quantity","text":"If you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document's license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover. Both covers must also clearly and legibly identify you as the publisher of these copies. The front cover must present the full title with all words of the title equally prominent and visible. You may add other material on the covers in addition. Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects. If the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages. If you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material. If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public. It is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document.","title":"3. COPYING IN QUANTITY"},{"location":"about/LICENSE/#4-modifications","text":"You may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it. In addition, you must do these things in the Modified Version: A. Use in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission. B. List on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement. C. State on the Title page the name of the publisher of the Modified Version, as the publisher. D. Preserve all the copyright notices of the Document. E. Add an appropriate copyright notice for your modifications adjacent to the other copyright notices. F. Include, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below. G. Preserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document's license notice. H. Include an unaltered copy of this License. I. Preserve the section Entitled \"History\", Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled \"History\" in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence. J. Preserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the \"History\" section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission. K. For any section Entitled \"Acknowledgements\" or \"Dedications\", Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein. L. Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles. M. Delete any section Entitled \"Endorsements\". Such a section may not be included in the Modified Version. N. Do not retitle any existing section to be Entitled \"Endorsements\" or to conflict in title with any Invariant Section. O. Preserve any Warranty Disclaimers. If the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant. To do this, add their titles to the list of Invariant Sections in the Modified Version's license notice. These titles must be distinct from any other section titles. You may add a section Entitled \"Endorsements\", provided it contains nothing but endorsements of your Modified Version by various parties\u2014for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard. You may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version. Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity. If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one. The author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version.","title":"4. MODIFICATIONS"},{"location":"about/LICENSE/#5-combining-documents","text":"You may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers. The combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy. If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number. Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work. In the combination, you must combine any sections Entitled \"History\" in the various original documents, forming one section Entitled \"History\"; likewise combine any sections Entitled \"Acknowledgements\", and any sections Entitled \"Dedications\". You must delete all sections Entitled \"Endorsements\".","title":"5. COMBINING DOCUMENTS"},{"location":"about/LICENSE/#6-collections-of-documents","text":"You may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects. You may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document.","title":"6. COLLECTIONS OF DOCUMENTS"},{"location":"about/LICENSE/#7-aggregation-with-independent-works","text":"A compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the copyright resulting from the compilation is not used to limit the legal rights of the compilation's users beyond what the individual works permit. When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document. If the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document's Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form. Otherwise they must appear on printed covers that bracket the whole aggregate.","title":"7. AGGREGATION WITH INDEPENDENT WORKS"},{"location":"about/LICENSE/#8-translation","text":"Translation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4. Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections. You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers. In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail. If a section in the Document is Entitled \"Acknowledgements\", \"Dedications\", or \"History\", the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title.","title":"8. TRANSLATION"},{"location":"about/LICENSE/#9-termination","text":"You may not copy, modify, sublicense, or distribute the Document except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense, or distribute it is void, and will automatically terminate your rights under this License. However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, receipt of a copy of some or all of the same material does not give you any rights to use it.","title":"9. TERMINATION"},{"location":"about/LICENSE/#10-future-revisions-of-this-license","text":"The Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. See https://www.gnu.org/licenses/. Each version of the License is given a distinguishing version number. If the Document specifies that a particular numbered version of this License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation. If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation. If the Document specifies that a proxy can decide which future versions of this License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Document.","title":"10. FUTURE REVISIONS OF THIS LICENSE"},{"location":"about/LICENSE/#11-relicensing","text":"\"Massive Multiauthor Collaboration Site\" (or \"MMC Site\") means any World Wide Web server that publishes copyrightable works and also provides prominent facilities for anybody to edit those works. A public wiki that anybody can edit is an example of such a server. A \"Massive Multiauthor Collaboration\" (or \"MMC\") contained in the site means any set of copyrightable works thus published on the MMC site. \"CC-BY-SA\" means the Creative Commons Attribution-Share Alike 3.0 license published by Creative Commons Corporation, a not-for-profit corporation with a principal place of business in San Francisco, California, as well as future copyleft versions of that license published by that same organization. \"Incorporate\" means to publish or republish a Document, in whole or in part, as part of another Document. An MMC is \"eligible for relicensing\" if it is licensed under this License, and if all works that were first published under this License somewhere other than this MMC, and subsequently incorporated in whole or in part into the MMC, (1) had no cover texts or invariant sections, and (2) were thus incorporated prior to November 1, 2008. The operator of an MMC Site may republish an MMC contained in the site under CC-BY-SA on the same site at any time before August 1, 2009, provided the MMC is eligible for relicensing. ADDENDUM: How to use this License for your documents To use this License in a document you have written, include a copy of the License in the document and put the following copyright and license notices just after the title page:","title":"11. RELICENSING"},{"location":"about/about/","text":"About The Author Olivier Rey LinkedIn profile rey [dot] olivier [at] gmail [dot] com Papers ResearchGate Areas of interest Software engineering, programming languages, programming paradigms, semantic web Modeling Archimate and enterprise architecture modeling frameworks MBSE/MBE Domain specific modeling DSL Mathematics (topology, graphs, category theory, algebraic geometry) Articles to write or migrate Deming and the system of profound knowledge French articles? Old stuff Migrate interesting stuff from old blog (and translate) VB and Delphi refactoring practices? Archive Topics: Modeling complex temporal events with graphs and topological graph transformations Like highway traffic The graph transformation changes the graph topology. What kind of mathematical object is it? Scale laws in graph (for instance adherence of domains in a multi-graph mode) What is the true nature of interdomain relationship? And how does it relate to the higher level relationship? Distance comparison between two graphs The fact one and the referential one Useful for AI projects, knowledge representation Isomorphic semantic representations: what graph transformations enable that? Modeling psychology frameworks (Freud, Jung, etc.) with graph patterns and graph distance measurement Rewiring rules and their limitations Cloning nodes and sub-graph: rules? (November 2019)","title":"Author"},{"location":"about/about/#about-the-author","text":"","title":"About The Author"},{"location":"about/about/#olivier-rey","text":"LinkedIn profile rey [dot] olivier [at] gmail [dot] com Papers ResearchGate","title":"Olivier Rey"},{"location":"about/about/#areas-of-interest","text":"Software engineering, programming languages, programming paradigms, semantic web Modeling Archimate and enterprise architecture modeling frameworks MBSE/MBE Domain specific modeling DSL Mathematics (topology, graphs, category theory, algebraic geometry)","title":"Areas of interest"},{"location":"about/about/#articles-to-write-or-migrate","text":"Deming and the system of profound knowledge French articles? Old stuff Migrate interesting stuff from old blog (and translate) VB and Delphi refactoring practices?","title":"Articles to write or migrate"},{"location":"about/about/#archive","text":"Topics: Modeling complex temporal events with graphs and topological graph transformations Like highway traffic The graph transformation changes the graph topology. What kind of mathematical object is it? Scale laws in graph (for instance adherence of domains in a multi-graph mode) What is the true nature of interdomain relationship? And how does it relate to the higher level relationship? Distance comparison between two graphs The fact one and the referential one Useful for AI projects, knowledge representation Isomorphic semantic representations: what graph transformations enable that? Modeling psychology frameworks (Freud, Jung, etc.) with graph patterns and graph distance measurement Rewiring rules and their limitations Cloning nodes and sub-graph: rules? (November 2019)","title":"Archive"},{"location":"articles/about-rest/","text":"Considerations About Rest And Web Services It's been a very long time since I've been explaining this to a lot of people and maybe today I should try to put the full explanation on paper. A Bit Of History When the REST concept was published, sometimes around 2001, I was in a middleware team in a big software company. In that team, we were accustomed to use business services for more than a decade. Indeed, the term \"web service\" or \"SOA\" was not existing at the time. But, in many business domains such as airline commercial business, banking, insurance, etc., many people knew what was RPC and were using it extensively to communicate between systems. RPC means Remote Procedure Call. RPC concept was introduced to me with the DCE ( Distributed Computing Environment ). DCE was a very powerful standard that were never completely used, as far as I know, but it explained in great details the basis of interoperability between systems. For sure, the standard was only making a synthesis of ideas that were much older. What Is RPC? Interoperability Contract Fundamentally RPC is, like it is said in its name, a remote procedure call. To understand the concept, let's imagine 2 programs that want to communicate, first program being A and second being B . B will publish an API in order to be called. In most procedural programming languages (like C), if the two programs are located on the same machine, A can call the API of B (see the top of Figure 1). Figure 1: Local Procedure Call and Remote Procedure Call The idea of interoperability in RPC is that, if B is located in a remote machine (or a remote process), B should not change when invoked by A . On the other side, A should not change in its invocation of B interface. So A will call a B interface locally to its system, this interface hiding a client stub that will wrap/serialize data in a certain format to be sent on the wire; on the machine hosting B , there will be a server stub unwraping/unserializing data to call locally the B interface. For sure, in order to work in a complex network, the message sent from A will have to fond is route to the machine hosting B . We have here all the elements of the client-server architecture. The Notion Of \"Verb\" Well, RPC is a bit more than that. Because, when calling a remote procedure (or function), semantically, we call a remote system asking it to do something (the B API) with the input data that we provide (the request). We expect data back (the response). This is/was called a \"verb\". A says to B : \"perform B-interface contract with my input data and give me back the contract expected output data\". Trends passed on many parameters: The protocols used changed, The addressing schemes changed, The format of the data changed (from many proprietary formats or Edifact, to XML to JSON). But the fact is, in most businesses proposing a certain degree of business complexity, RPC is still there. Most often, verbs are expressing an \"action\" semantic and requests are proposing data trees, so as responses. No Assumptions on the Technology Used We must notice that RPC does not make any assumption on the technology used by the server (the one that implements the B interface). The contract is limited to a verb, and two trees of data, one for the request and one for the response. We could note the contract as being: A uses response = B-interface(request) . The Corba Failed Attempt Principle In the 90s, the objet-oriented programming (OOP) being trendy, the intention behind Corba was born. The idea was to generalize the object notions to a distributed world. The logic consequence was to imagine a client-server protocol for objects to communicate between one another. The principle is simple: an object A calls locally a method m(...) on an object B . If we imagine the B instance as being remote, the idea is the same than RPC: The method should have a client and server stub. Figure 1: Local Method Invocation and Remote Method Invocation The fact is, despite the fact that it looks like RPC, this model is radically different from it: It supposes that the remote system is object oriented; It supposes that the remote system is stateful (the B instance must exist for its method to be called); It supposes an addressing system to find the reference of the instance of B; The contract of the method is not self sufficient, indeed, conceptually A asks for: response = state(B).m(request) which introduce some uncertainty on the call (because it depends on B state); The contract is supposing the state of B should be managed differently from the request parameters, and so, it puts a different semantic weight on B that should be a \"first-class entity\" whereas the request parameters are considered as \"second class entities\". This way of thinking distributed systems is leading to consider that the network offers \"objects that can provide services on themselves\", instead of providing simple services (i.e. distributed functions). In 2017's language, we could say that CORBA is proposing a \"resource-oriented architecture\". The Drawbacks of the ORB Approach An ORB (Object Request Broker) is a middleware that enables to manage those distributed objects, their remote invocation and the required addressing schema. The main drawback of the ORB approach is that a service can be, semantically, much more complex than calling one method of one object. Indeed, if we call a method on an object, we can imagine that the object will process this method on its state, or that it will process it considering its aggregation (\"facade\" pattern). For services, we do not make any assumptions of the kind. We just call a function that is supposed to do something with the inbound parameters that we provide (inbound parameters that are generally some kind of tree of objects having or not links together). Certainly, we can \"twist\" Corba in order to make it look like RPC: we can use objects that do not contain state and that have \"methods\" that are indeed just \"procedures\". But the addressing repository will have to manage object instance addresses instead of knowing the service location. An Idea That Keeps Coming Back This idea keeps coming back. Rest architecture can be seen as a reformulation of Corba principles: Resources are benefiting from an addressing scheme (URI); Resources have a name (class name), they are identified through their instance and they publish methods; Invoking a service is indeed a remote method invocation. Moreover, a strange design option is taken in the Rest specification: the presentation layer is binded on the underlying network protocol (http) in a very hard way. Indeed, the only verbs that seem to be invokable are CRUD verbs (CRUD standing for Create Retrieve Update Delete). We can also see this ORB principle applied, in a weaker way, in WSDL. When SOAP web services were really in the spirit of RPC, WSDL standard groups transactions in a way that is sometimes near to the idea of a group of methods operating on a remote object. We can note also in WSDL, a very strange hard biding on the underlying protocol (the WSDL schema integrates information from several OSI layers which is bad). Rest was those last years recently put in front of the scene due to IT marketing and the creation of Swagger.io The Core Problem of Resource Orientation Resource orientation, like in an ORB, is semantically a very hard restriction of RPC. RPC is a way of calling a service that will transform a graph of data in another graph of data. In the ORB approach, we call a method on a object, which is very restrictive. This can work if the semantics of the business is simple. If my business objects are modeled properly by one class, then maybe an ORB can work. If the business objects I have to model need several classes interacting together in a complex manner, then using an ORB will be a real pain. When a service can transform my graph of objects into another graph of objects, an ORB will force me to \"adapt\" my semantics to a pure technical object-oriented method call. For sure, this is true for Rest, as it was true for Corba. For pure Rest, it is even worse: being able to consider that only the http verbs (network layer) should be used to express the semantics of the presentation layer, the semantics of the \"functional verbs\" seems even more restrictive than Corba. For social networks like Twitter or Facebook, it seems to work. I can define a Rest API providing all the services offered by the platform. For a business application, we generally cannot use Rest because the constraints (RMI with CRUD verbs) are too strong. Again, like in Corba, we can cheat: we can use \"almost Rest\" and have a request body with JSON. That turns the method invocation into an almost-service. We can also include in the request a service name, but that is pretending to implement Rest and doing JSON-RPC-like behind the scene. Conclusion Rest is very practical for the applications which semantics is simple and can be adapted to two constraints: A resource oriented API (pushing for services to be RMI); A verb semantic limited to a variation of CRUD. For other applications, like business applications [1], we believe things never changed for decades. Before SOA, RPC services were existing. They were the same stuff than JSON-RPC like transactions today. Service orientation is a much more general way of implementing service distribution than ORB concepts. In particular, service orientation does not presuppose that the remote server sees the data as the caller does. The service signature is an agreed contract for both the client and the server to communicate together, but each of them can restructure data as they want, in an object oriented way or not. So, my advice is not to force yourself to implement a Rest API to your application because it is trendy, but to do it only if your business semantics enables it. See Also About GraphQL Notes [1] - In some businesses, like the airline one (standardized by IATA ), services have big requests and big responses for decades because the business requires it. ( December 2017 )","title":"Considerations About Rest And Web Services"},{"location":"articles/about-rest/#considerations-about-rest-and-web-services","text":"It's been a very long time since I've been explaining this to a lot of people and maybe today I should try to put the full explanation on paper.","title":"Considerations About Rest And Web Services"},{"location":"articles/about-rest/#a-bit-of-history","text":"When the REST concept was published, sometimes around 2001, I was in a middleware team in a big software company. In that team, we were accustomed to use business services for more than a decade. Indeed, the term \"web service\" or \"SOA\" was not existing at the time. But, in many business domains such as airline commercial business, banking, insurance, etc., many people knew what was RPC and were using it extensively to communicate between systems. RPC means Remote Procedure Call. RPC concept was introduced to me with the DCE ( Distributed Computing Environment ). DCE was a very powerful standard that were never completely used, as far as I know, but it explained in great details the basis of interoperability between systems. For sure, the standard was only making a synthesis of ideas that were much older.","title":"A Bit Of History"},{"location":"articles/about-rest/#what-is-rpc","text":"","title":"What Is RPC?"},{"location":"articles/about-rest/#interoperability-contract","text":"Fundamentally RPC is, like it is said in its name, a remote procedure call. To understand the concept, let's imagine 2 programs that want to communicate, first program being A and second being B . B will publish an API in order to be called. In most procedural programming languages (like C), if the two programs are located on the same machine, A can call the API of B (see the top of Figure 1). Figure 1: Local Procedure Call and Remote Procedure Call The idea of interoperability in RPC is that, if B is located in a remote machine (or a remote process), B should not change when invoked by A . On the other side, A should not change in its invocation of B interface. So A will call a B interface locally to its system, this interface hiding a client stub that will wrap/serialize data in a certain format to be sent on the wire; on the machine hosting B , there will be a server stub unwraping/unserializing data to call locally the B interface. For sure, in order to work in a complex network, the message sent from A will have to fond is route to the machine hosting B . We have here all the elements of the client-server architecture.","title":"Interoperability Contract"},{"location":"articles/about-rest/#the-notion-of-verb","text":"Well, RPC is a bit more than that. Because, when calling a remote procedure (or function), semantically, we call a remote system asking it to do something (the B API) with the input data that we provide (the request). We expect data back (the response). This is/was called a \"verb\". A says to B : \"perform B-interface contract with my input data and give me back the contract expected output data\". Trends passed on many parameters: The protocols used changed, The addressing schemes changed, The format of the data changed (from many proprietary formats or Edifact, to XML to JSON). But the fact is, in most businesses proposing a certain degree of business complexity, RPC is still there. Most often, verbs are expressing an \"action\" semantic and requests are proposing data trees, so as responses.","title":"The Notion Of \"Verb\""},{"location":"articles/about-rest/#no-assumptions-on-the-technology-used","text":"We must notice that RPC does not make any assumption on the technology used by the server (the one that implements the B interface). The contract is limited to a verb, and two trees of data, one for the request and one for the response. We could note the contract as being: A uses response = B-interface(request) .","title":"No Assumptions on the Technology Used"},{"location":"articles/about-rest/#the-corba-failed-attempt","text":"","title":"The Corba Failed Attempt"},{"location":"articles/about-rest/#principle","text":"In the 90s, the objet-oriented programming (OOP) being trendy, the intention behind Corba was born. The idea was to generalize the object notions to a distributed world. The logic consequence was to imagine a client-server protocol for objects to communicate between one another. The principle is simple: an object A calls locally a method m(...) on an object B . If we imagine the B instance as being remote, the idea is the same than RPC: The method should have a client and server stub. Figure 1: Local Method Invocation and Remote Method Invocation The fact is, despite the fact that it looks like RPC, this model is radically different from it: It supposes that the remote system is object oriented; It supposes that the remote system is stateful (the B instance must exist for its method to be called); It supposes an addressing system to find the reference of the instance of B; The contract of the method is not self sufficient, indeed, conceptually A asks for: response = state(B).m(request) which introduce some uncertainty on the call (because it depends on B state); The contract is supposing the state of B should be managed differently from the request parameters, and so, it puts a different semantic weight on B that should be a \"first-class entity\" whereas the request parameters are considered as \"second class entities\". This way of thinking distributed systems is leading to consider that the network offers \"objects that can provide services on themselves\", instead of providing simple services (i.e. distributed functions). In 2017's language, we could say that CORBA is proposing a \"resource-oriented architecture\".","title":"Principle"},{"location":"articles/about-rest/#the-drawbacks-of-the-orb-approach","text":"An ORB (Object Request Broker) is a middleware that enables to manage those distributed objects, their remote invocation and the required addressing schema. The main drawback of the ORB approach is that a service can be, semantically, much more complex than calling one method of one object. Indeed, if we call a method on an object, we can imagine that the object will process this method on its state, or that it will process it considering its aggregation (\"facade\" pattern). For services, we do not make any assumptions of the kind. We just call a function that is supposed to do something with the inbound parameters that we provide (inbound parameters that are generally some kind of tree of objects having or not links together). Certainly, we can \"twist\" Corba in order to make it look like RPC: we can use objects that do not contain state and that have \"methods\" that are indeed just \"procedures\". But the addressing repository will have to manage object instance addresses instead of knowing the service location.","title":"The Drawbacks of the ORB Approach"},{"location":"articles/about-rest/#an-idea-that-keeps-coming-back","text":"This idea keeps coming back. Rest architecture can be seen as a reformulation of Corba principles: Resources are benefiting from an addressing scheme (URI); Resources have a name (class name), they are identified through their instance and they publish methods; Invoking a service is indeed a remote method invocation. Moreover, a strange design option is taken in the Rest specification: the presentation layer is binded on the underlying network protocol (http) in a very hard way. Indeed, the only verbs that seem to be invokable are CRUD verbs (CRUD standing for Create Retrieve Update Delete). We can also see this ORB principle applied, in a weaker way, in WSDL. When SOAP web services were really in the spirit of RPC, WSDL standard groups transactions in a way that is sometimes near to the idea of a group of methods operating on a remote object. We can note also in WSDL, a very strange hard biding on the underlying protocol (the WSDL schema integrates information from several OSI layers which is bad). Rest was those last years recently put in front of the scene due to IT marketing and the creation of Swagger.io","title":"An Idea That Keeps Coming Back"},{"location":"articles/about-rest/#the-core-problem-of-resource-orientation","text":"Resource orientation, like in an ORB, is semantically a very hard restriction of RPC. RPC is a way of calling a service that will transform a graph of data in another graph of data. In the ORB approach, we call a method on a object, which is very restrictive. This can work if the semantics of the business is simple. If my business objects are modeled properly by one class, then maybe an ORB can work. If the business objects I have to model need several classes interacting together in a complex manner, then using an ORB will be a real pain. When a service can transform my graph of objects into another graph of objects, an ORB will force me to \"adapt\" my semantics to a pure technical object-oriented method call. For sure, this is true for Rest, as it was true for Corba. For pure Rest, it is even worse: being able to consider that only the http verbs (network layer) should be used to express the semantics of the presentation layer, the semantics of the \"functional verbs\" seems even more restrictive than Corba. For social networks like Twitter or Facebook, it seems to work. I can define a Rest API providing all the services offered by the platform. For a business application, we generally cannot use Rest because the constraints (RMI with CRUD verbs) are too strong. Again, like in Corba, we can cheat: we can use \"almost Rest\" and have a request body with JSON. That turns the method invocation into an almost-service. We can also include in the request a service name, but that is pretending to implement Rest and doing JSON-RPC-like behind the scene.","title":"The Core Problem of Resource Orientation"},{"location":"articles/about-rest/#conclusion","text":"Rest is very practical for the applications which semantics is simple and can be adapted to two constraints: A resource oriented API (pushing for services to be RMI); A verb semantic limited to a variation of CRUD. For other applications, like business applications [1], we believe things never changed for decades. Before SOA, RPC services were existing. They were the same stuff than JSON-RPC like transactions today. Service orientation is a much more general way of implementing service distribution than ORB concepts. In particular, service orientation does not presuppose that the remote server sees the data as the caller does. The service signature is an agreed contract for both the client and the server to communicate together, but each of them can restructure data as they want, in an object oriented way or not. So, my advice is not to force yourself to implement a Rest API to your application because it is trendy, but to do it only if your business semantics enables it.","title":"Conclusion"},{"location":"articles/about-rest/#see-also","text":"About GraphQL","title":"See Also"},{"location":"articles/about-rest/#notes","text":"[1] - In some businesses, like the airline one (standardized by IATA ), services have big requests and big responses for decades because the business requires it. ( December 2017 )","title":"Notes"},{"location":"articles/archimate-intro/","text":"Introduction to the Archimate Revolution In the last decade, a quiet revolution took place in the Enterprise Architecture (EA) domain. This revolution is called Archimate . This article is the first of a series of articles focusing on some specific aspects of Archimate and practices around the standard. All Archimate diagrams of this site are done using Archi . This tool is free and great. If you use it, consider making a donation. A Semantic Modeling Language Brief Introduction To The Archimate Meta-Model Archimate is a modeling language that enables to describe and study several aspects of the enterprise: Its strategy and motivations, Its projects, And the 4 core layers of enterprise description: The business layer, The software layer, The technology layer (infrastructure), The physical layer. All those aspects propose: Typed artifacts, Typed relationships between artifacts. Note that several relationships types can interconnect many different types or can be used in many contexts. The detailed description of all artifacts is not in the scope of this article and we can advise the reader to refer to the book Enterprise Architecture At Work from Mark Lankshorst . Archimate also defines interlayer relationships which enables to \"connect\" the various layers together. This is an absolutely fundamental feature of the language. A Graph Model Indeed, an Archimate model is actually a graph . For those who are familiar with the Semantic Web ( RDF, RDFS , and OWL ), any Archimate model is a semantic graph. The graph model is the result of the union of all diagrams implemented with the same set of artifacts. For each artifact, the union of all its incoming and outgoing relationships creates the neighborhood of the artifact in the graph model. When you use Archimate, you represent things in views, using the various artifacts that are available to you. Each element you draw on a certain view (there are many types of views in the standard) will have a certain type, like \"Business Process\" or \"Application Function\". Thus, the resulting model will be a set of views, each of them presenting many interconnected artifacts that all are instances of types that have a specific meaning . Viewpoint And Meta-Model The creation of a semantic language generally implies the creation a meta-model. Many tools are existing to create meta-models (for instance Eclipse EMF with Sirius , or MetaEdit+ depending if you want to pay or not). In the history of enterprise architecture, many approaches defined the notion of \"view point\": in order to act on the enterprise as a whole (or as a system ), the first step is to describe the enterprise. In order to describe it, due to the complexity of the task, the architect must use several viewpoints. The union of those viewpoints is defining the model of the enterprise. The Zachman framework was one of the first publicly available enterprise architecture \"framework\", enabling to address the many viewpoints describing the enterprise. The problem with this framework, but also with many other enterprise architecture frameworks, is that they propose many viewpoints, generally each of them proposing a complete separate meta-model for each viewpoint (with artifact types and relationship types), but no consistent view of those various viewpoints. This is a major issue for the framework because: Separate meta-models for separate viewpoints will, most often, enable to create a globally ambiguous model; Separate meta-models can create various ways of representing the same reality; There can be semantic overlap between various viewpoints and their respective meta-model; What should be a single viewpoint can artificially be split into different view points with different meta-models (we'll see later in this article the dramatic consequences of the Long\u00e9p\u00e9 Enterprise Architecture model in France). If the consistency is not \"built in\" the framework (i.e. in its meta-model), if it is not part of the primary requirements, then the framework will be very difficult to use. Indeed, interconnecting various meta-models to be able to create something consistent is not an easy task. Indeed, Archimate seems to be the best illustration of it, and I fear I don't know many other samples [1]. Samples of Non Consistent Modeling Approaches In IT, we have forgotten about the crucial importance of having a consistent way to describe the \"reality\", or let's say the concepts that we are manipulating frequently. Sample #1: UML UML can be the first example of the non consistency of a meta-model. UML is not consistent because it proposes many meta-models that are not semantically connected together as a whole. Suppose you made class diagrams and sequence diagrams for a set of classes. You can add a state diagram for a particular process of a specific class A of your model. In UML, there is no way to know if this state diagram is consistent or not with the rest of the diagrams that include A . This is due to the fact that UML proposes a set of various kinds of views that are not linked together . Each view type has its own meta-model. Some of the meta-models are reusing the same artifacts, which enable modeling tools to connect the artifacts to the views they are used into. But many diagram types are just disconnected meta-models. This problem can perhaps explain why modeling in UML was progressively abandoned by many projects; because it was not really suited to express in a consistent way what the code should be. If, during design, doing UML model cannot prevent you from making design mistakes, the use of the modeling language is much less interesting [2]. Sample #2: Long\u00e9p\u00e9 Another dramatic inconsistent model is the Long\u00e9p\u00e9 French model which defined, instead of Archimate \"software layer\", two different layers: One \"functional architecture layer\", that could be interpreted as the Application Function part of the Archimate meta-model, One \"application architecture layer\", that could be interpreted as the Application Component part of the Archimate meta-model. As in Archimate, there is, for Long\u00e9p\u00e9, a business layer and an infrastructure layer, which scopes are almost the same than Archimate's. In the Long\u00e9p\u00e9 model, every layer has a model, and every model can be \"derived\" from the model of the superior layer. This means that the application layer can be \"derived from\" the functional layer. This assertion is obscure, misleading and semantically erroneous. In Archimate, the application function is \"assigned to\" a application component. That enables to manage the good and the bad assignment, what is called the \"urbanisation\" in French, notion of good or bad function positioning in the global IT systems: If some functions are well positioned in the IT systems, that means that they communicate easily and can evolve in a natural way without questioning the full architecture; If some functions are badly positioned in the IT systems (so assigned to the wrong components), we will generate useless dependencies between applications, useless web services, difficulties in terms of evolution, costs and complexity. The problem of \"function positioning\" is not in the scope of this article, but it is at the heart of the Long\u00e9p\u00e9's reflection, despite the invention of a very bad meta-model that totally confuses the notion through the concept of derivation. This artificial split of one layer (the application layer of Archimate) into two layers which dependency is erroneously defined, caused a huge number of French IT projects to fail , and numerous errors of interpretation and understanding for French architects. The book of Christophe Long\u00e9p\u00e9 was at the origin of many misjudgments, errors, confusions and money loss in the French market since the publication of its first edition in 2001. Sample #3: Projects Creating Their Own Modeling Framework In consulting missions, I also saw strange practices, as the one of creating a project-specific enterprise architecture modeling framework that evolved throughout the project. Most of the time, the internal framework was incorporating progressively various inconsistent meta-models coming from various modeling standards (such as UML or BPMN or TOGAF ). For sure, most architecture works in the evolving project-specific EA framework were confusing and not usable by software engineers. This often led or contributed to lead the project to its failure. Archimate, a Consistent Approach Archimate is proposing a consistent meta-model. This is the first fundamental characteristic which is at the heart of the usability of the modeling standard. I deeply encourage the meta-model creators to think about it, because it defines the real power of the multiple-views paradigm: multiple views on the same reality can be used provided there can be consistency between those views , the consistency being in a unique meta-model. Note that proposing a consistent meta-model does not imply that the modeling framework is closed and will not evolve. Archimate proposes, since its version 3, a way to extend the meta-model. We could say that the multiple views consistency-enable graph model is the revolution #0 of the Archimate language. The Many Revolutions of Archimate Revolution #1: The Language Just Works Being consistent is not sufficient for a meta-model. The quality of the meta-model lies in the pertinence of the semantic artifacts that it proposes (being nodes or relationships). Those artifacts must be as semantically clearly defined and must not bring confusion or multiple alternative representations. In particular, semantic overlapping must be avoided as much as possible [3]. In other terms, if the quality of the meta-model is high, then the modeling will be good. However, if the meta-model is bad, the modeling will be very bad and will cause damages in the project. Indeed, Archimate propose artifacts that enable the non-ambiguous description of the enterprise processes, strategy and IT. More: it pushes the architects to describe the reality in a sane way , which means in a way that will push for problem exploration and make visible the possible solutions. For instance, in the case of function positioning, it is easy to count the dependencies between two functions (through derived relationships). If the count is low and always directed in the same way, that probably means the functions are well positioned; If the count if high and/or the directions go both ways, that probably means that the two functions should be one single function. Archimate language helps and in that sense, it is doing the job. In other terms: the language just works . This is, really, a revolution. In some cases, meta-models induce architects to think badly, to force themselves to think in an inconsistent model where the semantics are confusing. In Archimate, this is not the case. Revolution #2: Architects Can Share And Propose Auditable Works Even if Archimate will not guarantee that 2 enterprise architects will produce the same modeling when representing the same things, using the same standardized language enable each of them to understand the modeling of the other, to challenge it and to discuss it. We can forget Visio or Powerpoint schema based modeling, that are very ambiguous at several levels. The works become auditable by other Archimate architects. For sure, documenting textually the views and the artifact themselves is very helpful. As in all modeling languages, without an effort, some views may be a be difficult to understand if we don't know what question it is supposed to answer. This is a revolution in Enterprise Architecture but also in IT or business architecture. We can work on something shareable and auditable and so begin to work the simplistic boxes and arrow diagrams. Enterprise Architecture is becoming more an engineering discipline when it was considered, too often, as a blur esoteric and confusing witchcraft. Revolution #3: Managing Complex Representations In Archimate, you can tackle very complex problems. Due to the fact that you can study several granularity of problems at the same time in the same model [4], the studied complexity can be at several levels: You can address very big IT systems or one company or of a group of companies; You can analyze in a very detailed way sets of very small intertwined functionality; You can study highly distributed systems. Indeed, big models for IT transformation will quickly contain thousands of artifacts. Note that to ensure consistency on those models, there is a huge systematic work to do. This is a revolution. The last big program that I did without Archimate was in 2008-2009. It was big, functionally very complex and with a lot of architecture problems. I struggled with UML and BPMN and had to manage the functional consistency by hand. Archimate would have been of great help at the time. Revolution #4: Aggregate Various Sources of Knowledge When you are doing a big projects, you have many sources of information, some of them oral (like end users or IT people interviews) and some other being documents or diagrams or wikis, or existing code. It is very easy to forget important stuff or weak signals that hide structural constraints. With Archimate, you can define views per source of information and work in back office on the consistency of all the information provided when it begin to touch the same domains or software. At the same time, you can formalize what was said in a specific workshop (process, functions, software, etc.) and recreate this consistent view from the graph model. Despite the fact that it is suite a work, the semantic modeling of Archimate enables to highlight what is consistent and what is not and what complementary information you would need to complete the assessment. Very often in missions, I can say that my model contains all relevant information that I found. Revolution #5: Managing Dependencies Dependencies are the 21rst century problem. Companies have existing IT systems that grew sometimes in a great chaos. Changing stuff is, objectively, complicated, risky, costly. With Archimate, you can work on dependencies, and so address those risks before the project is started. You can even \"objectify\" the difficulty or a certain project. If you change a system that is not connected to many other systems, it can be easier than trying to change the core system without knowing about the impacts and the problems that will be induced by this change. Dependency management are at the heart of the complexity of digital transformation. Because operations must go on, enterprise and IT architects cannot do whatever they want. They have to work on a credible plan that will create a roadmap of transformation taking dependencies into accounts. Revolution #6: Modeling Transformation, Modeling Time Modeling digital transformation is a real piece of work if we want to do a serious job. But it is possible - and that's a revolution that Archimate enables. Yes, it is possible to create, pilot, manage, anticipate, huge and complex enterprise digital transformation with Archimate. And frankly, I cannot see what other approach can do that. At a certain point, reading the forums, I am not convinced that many architects really understood this point. We can now create pretty accurate scenarios of transformation inside Archimate models, taking into account the organization, the strategy, the IT systems, and so on. Maybe we have here the real role of the enterprise architect. Maybe the enterprise architect should be named the enterprise transformation architect . Enterprises should realize that transforming the processes and the IT systems can become an engineering discipline at last , and that using Archimate for this kind of critical topics enables to avoid many troubles and to spend unnecessary money. Modeling transformation is modeling time and so conventions must be chosen to tag artifacts as being existent or not at a certain phase of the transformation process. As of now, the Archimate modeling tools do not support a temporal view of models (like Franz Gruff AllegroGraph visualizer), we have to take conventions [5]. Revolution #7: Using Archimate In Many Software Activities Indeed, many software activities can use Archimate on a regular basis, even if the language is more targeting architecture purposes. When we look carefully at nowadays software, most software are highly interconnected to other software or within the enterprise or in the Cloud. Archimate can help analyzing the structural impacts of those interconnections inside the software itself. Cartography of Systems Archimate can be used to create cartographies of IT systems but many architects should realize that Archimate models are working tools more than poster tools. Conclusion Archimate is, for me, the engineering revolution of digital transformation of the last 15 years. It redefines the enterprise architect role as an enterprise transformation architect role that is able to study with all stakeholders the operational application of a company digital strategy and its impacts on the business, the processes and the IT systems. Once the study is done, Archimate models are of great help to tackle the complex programs, manage the project dependencies and optimize the whole transformation, in terms of time and in terms of efficiency and costs. We must never forget that the enterprise architect should be a crucial change agent that proposes solutions to a strategic problem and should work in the plain knowledge of the business processes. Most digital transformation offices should also carefully consider using Archimate to manage their transformation plans and try to be in control of the costs and schedule. See also Archimate recipes Notes [1] The only modeling approach that look like Archimate is the Aris methodology . Even if the methodology did not cover the full scope of Archimate, the theoretical approach was similar. [2] We could compare a UML design tool with the CAD tools. The use of the modeling language must enable to work on the model of the programs before coding in order to create the best design possible, a design that will be implementable quite straight forwardly. Unfortunately with UML, the objective is not reached. [3] Some big Enterprise Architecture frameworks used in the military world are proposing very complex meta-models that are proposing, at the same time, sets of disconnected meta-models and semantic overlap between artifacts. Those frameworks are often at the center of big project failures in the military industry. [4] In projects, I often use or advise to use \"level\" (or scale) indications of some artifacts like business processes or business functions. The indication can be put in the name of the artifact or as an attribute. For instance, Accounting will become (L1) Accounting to indicate that, in some views, the business functions will be represented in a high level way. All detailed functions inside this one (aggregation or composition link) will be flagged (L2) such as (L2) Centralization . This enables to have various levels of relationships between concepts and to be able to drill down inside the model in order to see more detailed (and more accurate) representations. [5] The temporal conventions can be as the ones we mentioned for scale management, something like (P3) for phase 3. Tagging the artifact name enables to have possibly many instances of the same artifact, instances that will evolve. The conventions must be chosen very carefully to be able to see what changes and what is remaining the same throughout the whole change process. (January 2018)","title":"An Introduction to The Archimate Revolution"},{"location":"articles/archimate-intro/#introduction-to-the-archimate-revolution","text":"In the last decade, a quiet revolution took place in the Enterprise Architecture (EA) domain. This revolution is called Archimate . This article is the first of a series of articles focusing on some specific aspects of Archimate and practices around the standard. All Archimate diagrams of this site are done using Archi . This tool is free and great. If you use it, consider making a donation.","title":"Introduction to the Archimate Revolution"},{"location":"articles/archimate-intro/#a-semantic-modeling-language","text":"","title":"A Semantic Modeling Language"},{"location":"articles/archimate-intro/#brief-introduction-to-the-archimate-meta-model","text":"Archimate is a modeling language that enables to describe and study several aspects of the enterprise: Its strategy and motivations, Its projects, And the 4 core layers of enterprise description: The business layer, The software layer, The technology layer (infrastructure), The physical layer. All those aspects propose: Typed artifacts, Typed relationships between artifacts. Note that several relationships types can interconnect many different types or can be used in many contexts. The detailed description of all artifacts is not in the scope of this article and we can advise the reader to refer to the book Enterprise Architecture At Work from Mark Lankshorst . Archimate also defines interlayer relationships which enables to \"connect\" the various layers together. This is an absolutely fundamental feature of the language.","title":"Brief Introduction To The Archimate Meta-Model"},{"location":"articles/archimate-intro/#a-graph-model","text":"Indeed, an Archimate model is actually a graph . For those who are familiar with the Semantic Web ( RDF, RDFS , and OWL ), any Archimate model is a semantic graph. The graph model is the result of the union of all diagrams implemented with the same set of artifacts. For each artifact, the union of all its incoming and outgoing relationships creates the neighborhood of the artifact in the graph model. When you use Archimate, you represent things in views, using the various artifacts that are available to you. Each element you draw on a certain view (there are many types of views in the standard) will have a certain type, like \"Business Process\" or \"Application Function\". Thus, the resulting model will be a set of views, each of them presenting many interconnected artifacts that all are instances of types that have a specific meaning .","title":"A Graph Model"},{"location":"articles/archimate-intro/#viewpoint-and-meta-model","text":"The creation of a semantic language generally implies the creation a meta-model. Many tools are existing to create meta-models (for instance Eclipse EMF with Sirius , or MetaEdit+ depending if you want to pay or not). In the history of enterprise architecture, many approaches defined the notion of \"view point\": in order to act on the enterprise as a whole (or as a system ), the first step is to describe the enterprise. In order to describe it, due to the complexity of the task, the architect must use several viewpoints. The union of those viewpoints is defining the model of the enterprise. The Zachman framework was one of the first publicly available enterprise architecture \"framework\", enabling to address the many viewpoints describing the enterprise. The problem with this framework, but also with many other enterprise architecture frameworks, is that they propose many viewpoints, generally each of them proposing a complete separate meta-model for each viewpoint (with artifact types and relationship types), but no consistent view of those various viewpoints. This is a major issue for the framework because: Separate meta-models for separate viewpoints will, most often, enable to create a globally ambiguous model; Separate meta-models can create various ways of representing the same reality; There can be semantic overlap between various viewpoints and their respective meta-model; What should be a single viewpoint can artificially be split into different view points with different meta-models (we'll see later in this article the dramatic consequences of the Long\u00e9p\u00e9 Enterprise Architecture model in France). If the consistency is not \"built in\" the framework (i.e. in its meta-model), if it is not part of the primary requirements, then the framework will be very difficult to use. Indeed, interconnecting various meta-models to be able to create something consistent is not an easy task. Indeed, Archimate seems to be the best illustration of it, and I fear I don't know many other samples [1].","title":"Viewpoint And Meta-Model"},{"location":"articles/archimate-intro/#samples-of-non-consistent-modeling-approaches","text":"In IT, we have forgotten about the crucial importance of having a consistent way to describe the \"reality\", or let's say the concepts that we are manipulating frequently.","title":"Samples of Non Consistent Modeling Approaches"},{"location":"articles/archimate-intro/#sample-1-uml","text":"UML can be the first example of the non consistency of a meta-model. UML is not consistent because it proposes many meta-models that are not semantically connected together as a whole. Suppose you made class diagrams and sequence diagrams for a set of classes. You can add a state diagram for a particular process of a specific class A of your model. In UML, there is no way to know if this state diagram is consistent or not with the rest of the diagrams that include A . This is due to the fact that UML proposes a set of various kinds of views that are not linked together . Each view type has its own meta-model. Some of the meta-models are reusing the same artifacts, which enable modeling tools to connect the artifacts to the views they are used into. But many diagram types are just disconnected meta-models. This problem can perhaps explain why modeling in UML was progressively abandoned by many projects; because it was not really suited to express in a consistent way what the code should be. If, during design, doing UML model cannot prevent you from making design mistakes, the use of the modeling language is much less interesting [2].","title":"Sample #1: UML"},{"location":"articles/archimate-intro/#sample-2-longepe","text":"Another dramatic inconsistent model is the Long\u00e9p\u00e9 French model which defined, instead of Archimate \"software layer\", two different layers: One \"functional architecture layer\", that could be interpreted as the Application Function part of the Archimate meta-model, One \"application architecture layer\", that could be interpreted as the Application Component part of the Archimate meta-model. As in Archimate, there is, for Long\u00e9p\u00e9, a business layer and an infrastructure layer, which scopes are almost the same than Archimate's. In the Long\u00e9p\u00e9 model, every layer has a model, and every model can be \"derived\" from the model of the superior layer. This means that the application layer can be \"derived from\" the functional layer. This assertion is obscure, misleading and semantically erroneous. In Archimate, the application function is \"assigned to\" a application component. That enables to manage the good and the bad assignment, what is called the \"urbanisation\" in French, notion of good or bad function positioning in the global IT systems: If some functions are well positioned in the IT systems, that means that they communicate easily and can evolve in a natural way without questioning the full architecture; If some functions are badly positioned in the IT systems (so assigned to the wrong components), we will generate useless dependencies between applications, useless web services, difficulties in terms of evolution, costs and complexity. The problem of \"function positioning\" is not in the scope of this article, but it is at the heart of the Long\u00e9p\u00e9's reflection, despite the invention of a very bad meta-model that totally confuses the notion through the concept of derivation. This artificial split of one layer (the application layer of Archimate) into two layers which dependency is erroneously defined, caused a huge number of French IT projects to fail , and numerous errors of interpretation and understanding for French architects. The book of Christophe Long\u00e9p\u00e9 was at the origin of many misjudgments, errors, confusions and money loss in the French market since the publication of its first edition in 2001.","title":"Sample #2: Long\u00e9p\u00e9"},{"location":"articles/archimate-intro/#sample-3-projects-creating-their-own-modeling-framework","text":"In consulting missions, I also saw strange practices, as the one of creating a project-specific enterprise architecture modeling framework that evolved throughout the project. Most of the time, the internal framework was incorporating progressively various inconsistent meta-models coming from various modeling standards (such as UML or BPMN or TOGAF ). For sure, most architecture works in the evolving project-specific EA framework were confusing and not usable by software engineers. This often led or contributed to lead the project to its failure.","title":"Sample #3: Projects Creating Their Own Modeling Framework"},{"location":"articles/archimate-intro/#archimate-a-consistent-approach","text":"Archimate is proposing a consistent meta-model. This is the first fundamental characteristic which is at the heart of the usability of the modeling standard. I deeply encourage the meta-model creators to think about it, because it defines the real power of the multiple-views paradigm: multiple views on the same reality can be used provided there can be consistency between those views , the consistency being in a unique meta-model. Note that proposing a consistent meta-model does not imply that the modeling framework is closed and will not evolve. Archimate proposes, since its version 3, a way to extend the meta-model. We could say that the multiple views consistency-enable graph model is the revolution #0 of the Archimate language.","title":"Archimate, a Consistent Approach"},{"location":"articles/archimate-intro/#the-many-revolutions-of-archimate","text":"","title":"The Many Revolutions of Archimate"},{"location":"articles/archimate-intro/#revolution-1-the-language-just-works","text":"Being consistent is not sufficient for a meta-model. The quality of the meta-model lies in the pertinence of the semantic artifacts that it proposes (being nodes or relationships). Those artifacts must be as semantically clearly defined and must not bring confusion or multiple alternative representations. In particular, semantic overlapping must be avoided as much as possible [3]. In other terms, if the quality of the meta-model is high, then the modeling will be good. However, if the meta-model is bad, the modeling will be very bad and will cause damages in the project. Indeed, Archimate propose artifacts that enable the non-ambiguous description of the enterprise processes, strategy and IT. More: it pushes the architects to describe the reality in a sane way , which means in a way that will push for problem exploration and make visible the possible solutions. For instance, in the case of function positioning, it is easy to count the dependencies between two functions (through derived relationships). If the count is low and always directed in the same way, that probably means the functions are well positioned; If the count if high and/or the directions go both ways, that probably means that the two functions should be one single function. Archimate language helps and in that sense, it is doing the job. In other terms: the language just works . This is, really, a revolution. In some cases, meta-models induce architects to think badly, to force themselves to think in an inconsistent model where the semantics are confusing. In Archimate, this is not the case.","title":"Revolution #1: The Language Just Works"},{"location":"articles/archimate-intro/#revolution-2-architects-can-share-and-propose-auditable-works","text":"Even if Archimate will not guarantee that 2 enterprise architects will produce the same modeling when representing the same things, using the same standardized language enable each of them to understand the modeling of the other, to challenge it and to discuss it. We can forget Visio or Powerpoint schema based modeling, that are very ambiguous at several levels. The works become auditable by other Archimate architects. For sure, documenting textually the views and the artifact themselves is very helpful. As in all modeling languages, without an effort, some views may be a be difficult to understand if we don't know what question it is supposed to answer. This is a revolution in Enterprise Architecture but also in IT or business architecture. We can work on something shareable and auditable and so begin to work the simplistic boxes and arrow diagrams. Enterprise Architecture is becoming more an engineering discipline when it was considered, too often, as a blur esoteric and confusing witchcraft.","title":"Revolution #2: Architects Can Share And Propose Auditable Works"},{"location":"articles/archimate-intro/#revolution-3-managing-complex-representations","text":"In Archimate, you can tackle very complex problems. Due to the fact that you can study several granularity of problems at the same time in the same model [4], the studied complexity can be at several levels: You can address very big IT systems or one company or of a group of companies; You can analyze in a very detailed way sets of very small intertwined functionality; You can study highly distributed systems. Indeed, big models for IT transformation will quickly contain thousands of artifacts. Note that to ensure consistency on those models, there is a huge systematic work to do. This is a revolution. The last big program that I did without Archimate was in 2008-2009. It was big, functionally very complex and with a lot of architecture problems. I struggled with UML and BPMN and had to manage the functional consistency by hand. Archimate would have been of great help at the time.","title":"Revolution #3: Managing Complex Representations"},{"location":"articles/archimate-intro/#revolution-4-aggregate-various-sources-of-knowledge","text":"When you are doing a big projects, you have many sources of information, some of them oral (like end users or IT people interviews) and some other being documents or diagrams or wikis, or existing code. It is very easy to forget important stuff or weak signals that hide structural constraints. With Archimate, you can define views per source of information and work in back office on the consistency of all the information provided when it begin to touch the same domains or software. At the same time, you can formalize what was said in a specific workshop (process, functions, software, etc.) and recreate this consistent view from the graph model. Despite the fact that it is suite a work, the semantic modeling of Archimate enables to highlight what is consistent and what is not and what complementary information you would need to complete the assessment. Very often in missions, I can say that my model contains all relevant information that I found.","title":"Revolution #4: Aggregate Various Sources of Knowledge"},{"location":"articles/archimate-intro/#revolution-5-managing-dependencies","text":"Dependencies are the 21rst century problem. Companies have existing IT systems that grew sometimes in a great chaos. Changing stuff is, objectively, complicated, risky, costly. With Archimate, you can work on dependencies, and so address those risks before the project is started. You can even \"objectify\" the difficulty or a certain project. If you change a system that is not connected to many other systems, it can be easier than trying to change the core system without knowing about the impacts and the problems that will be induced by this change. Dependency management are at the heart of the complexity of digital transformation. Because operations must go on, enterprise and IT architects cannot do whatever they want. They have to work on a credible plan that will create a roadmap of transformation taking dependencies into accounts.","title":"Revolution #5: Managing Dependencies"},{"location":"articles/archimate-intro/#revolution-6-modeling-transformation-modeling-time","text":"Modeling digital transformation is a real piece of work if we want to do a serious job. But it is possible - and that's a revolution that Archimate enables. Yes, it is possible to create, pilot, manage, anticipate, huge and complex enterprise digital transformation with Archimate. And frankly, I cannot see what other approach can do that. At a certain point, reading the forums, I am not convinced that many architects really understood this point. We can now create pretty accurate scenarios of transformation inside Archimate models, taking into account the organization, the strategy, the IT systems, and so on. Maybe we have here the real role of the enterprise architect. Maybe the enterprise architect should be named the enterprise transformation architect . Enterprises should realize that transforming the processes and the IT systems can become an engineering discipline at last , and that using Archimate for this kind of critical topics enables to avoid many troubles and to spend unnecessary money. Modeling transformation is modeling time and so conventions must be chosen to tag artifacts as being existent or not at a certain phase of the transformation process. As of now, the Archimate modeling tools do not support a temporal view of models (like Franz Gruff AllegroGraph visualizer), we have to take conventions [5].","title":"Revolution #6: Modeling Transformation, Modeling Time"},{"location":"articles/archimate-intro/#revolution-7-using-archimate-in-many-software-activities","text":"Indeed, many software activities can use Archimate on a regular basis, even if the language is more targeting architecture purposes. When we look carefully at nowadays software, most software are highly interconnected to other software or within the enterprise or in the Cloud. Archimate can help analyzing the structural impacts of those interconnections inside the software itself.","title":"Revolution #7: Using Archimate In Many Software Activities"},{"location":"articles/archimate-intro/#cartography-of-systems","text":"Archimate can be used to create cartographies of IT systems but many architects should realize that Archimate models are working tools more than poster tools.","title":"Cartography of Systems"},{"location":"articles/archimate-intro/#conclusion","text":"Archimate is, for me, the engineering revolution of digital transformation of the last 15 years. It redefines the enterprise architect role as an enterprise transformation architect role that is able to study with all stakeholders the operational application of a company digital strategy and its impacts on the business, the processes and the IT systems. Once the study is done, Archimate models are of great help to tackle the complex programs, manage the project dependencies and optimize the whole transformation, in terms of time and in terms of efficiency and costs. We must never forget that the enterprise architect should be a crucial change agent that proposes solutions to a strategic problem and should work in the plain knowledge of the business processes. Most digital transformation offices should also carefully consider using Archimate to manage their transformation plans and try to be in control of the costs and schedule.","title":"Conclusion"},{"location":"articles/archimate-intro/#see-also","text":"Archimate recipes","title":"See also"},{"location":"articles/archimate-intro/#notes","text":"[1] The only modeling approach that look like Archimate is the Aris methodology . Even if the methodology did not cover the full scope of Archimate, the theoretical approach was similar. [2] We could compare a UML design tool with the CAD tools. The use of the modeling language must enable to work on the model of the programs before coding in order to create the best design possible, a design that will be implementable quite straight forwardly. Unfortunately with UML, the objective is not reached. [3] Some big Enterprise Architecture frameworks used in the military world are proposing very complex meta-models that are proposing, at the same time, sets of disconnected meta-models and semantic overlap between artifacts. Those frameworks are often at the center of big project failures in the military industry. [4] In projects, I often use or advise to use \"level\" (or scale) indications of some artifacts like business processes or business functions. The indication can be put in the name of the artifact or as an attribute. For instance, Accounting will become (L1) Accounting to indicate that, in some views, the business functions will be represented in a high level way. All detailed functions inside this one (aggregation or composition link) will be flagged (L2) such as (L2) Centralization . This enables to have various levels of relationships between concepts and to be able to drill down inside the model in order to see more detailed (and more accurate) representations. [5] The temporal conventions can be as the ones we mentioned for scale management, something like (P3) for phase 3. Tagging the artifact name enables to have possibly many instances of the same artifact, instances that will evolve. The conventions must be chosen very carefully to be able to see what changes and what is remaining the same throughout the whole change process. (January 2018)","title":"Notes"},{"location":"articles/archimate-recipes/","text":"Archimate recipes Modeling in Archimate can sometimes present some difficulties. In this article, we will expose some tips and tricks that can facilitate your daily life as an enterprise architect using Archimate in your modeling activities. Prerequisites This article is using Archi . This tool is great. If you are using it, please consider making a donation. Introduction to tags About tags Archimate is a very powerful language but sometimes, it can be very useful to create tags to distinguish artifacts or group them into sets. We will put some tags in the name of the artifact in some cases, and in other cases, we can benefit from the use of properties (for instance to process some calculation based on some property values). The convention we use in this article is to \"tag\" the names of the artifacts by placing a label between parenthesis. Tags are sometimes linked to derived relationships but often they answer to other needs. Why not using stereotypes? Modeling languages like UML or SysML use stereotypes to specialialize model elements and to create new artifacts that derive from a master artifact (one from the standard). Example: If you want to model a graph of data, you may need 2 new artifacts: Node and Relationship instead of Data Objet . You can use the left representation of Figure 1 and create new data objects from the specializations with the UML convention <<stereotype>> . Figure 1 : Modeling a graph with and without stereotypes Stereotypes are creating new classes of objects. Mentioning stereotypes can be a good way to indicate to your fellow modelers (having a UML or SysML background) what were your intentions. But you can also use the right representation and make your classes derive directly from Node and Relationship , considered as implicitit stereotypes of Data Objet . In the rest of the page, we will more focus on tags that are not supposed to replace the stereotype specialization. Levels of nesting Basics In an Archimate model, it is quite common to model nested objects like shown in Figure 2. Figure 2 : Sample of nesting in the business layer In Figure 2, we can see that the Accountant role is assigned both to the Accounting business function and to the Closing business process. The Closing artifact is nested into the Accounting function that is, in our model, a business function of the highest level. Being the highest level of \"granularity\" of our processes (in some cases, this level can be the one of the quality system of the company), we can tag the Accounting artifact with the level (L1) to indicate its level. All the business processes and/or business functions that will be nested into it will be tagged (L2) for \"level 2\". All nested objects inside level 2 objects will be tagged level 3 (L3) and so on. An option is to flag the relationship with the level as shown in Figure 2. This can be superfluous if the artifacts are already tagged. Graph view This simple system enables to sort artifacts per level on the Archimate graph view (see Figure 3). Figure 3 : Archimate graph centered in the Accountant artifact When the diagram has many artifacts, it is possible to sort the level of vision that we want to have in the graph view. This kind of need appears for instance when we want to have a good vision of the full coverage of a certain point of view. In the graph view, helped with the tags, we can sort all processes attached to the Accountant role per level in order to determine in, in all our views, we cover what we imagine as being all (L1) processes. Figure 4 : Sorted Archimate graph centered in the Accountant artifact When the model has many views with many artifacts at various different levels, the graph view helps making the model consistent. Working at different levels In Figure 5, working with process levels enabled the enterprise architect to indicate that, at a certain level (L2) , the main role taking in charge the Closing process is the Accountant , but at a lower level, we have other roles taking in charge a part of the process. Figure 5 : Main role and secondary role For sure, all diagrams should be consistent, so possibly not to show the relationships that could be misleading in the view. For instance, in Figure 5, the assignment link between Accountant and (L2) Closing is misleading in the diagram. Because, it this diagram, we should only show the various roles that collaborate to the sub-processes of the (L2) Closing process. Figure 6 : Main role and secondary role (less ambiguous view) Figure 6 is showing a much less ambiguous view. For sure, as soon as we we look at those processes at the (L2) level, the Business controller role will become invisible. Note on derived relationships Semantically, we try to express an ambiguity. Figure 2 could say that the Accountant role is assigned to Closing and \"in a more general way\" to the Accounting function. But we know nothing about the other (L2) functions and if the Accountant role is really assigned to all (L2) functions that would compose the Accounting function. The definition of a relationship is not really helping: If two structural or dependency relationships r:R and s:S are permitted between elements a, b, and c such that r(a,b) and s(b,c), then a structural relationship t:T is also permitted, with t(a,c) and type T being the weakest of R and S. Source Let's say C is the composition relationship (\"composes\") and A the assignment relationship (\"is assigned to\"). We have: R1 = Closing C Accounting R2 = Accountant A Accounting R3 = Accountant A Closing So, in the context of structural relationships derivation rules, we could say that R3 + R1 &rarr; R2 . That means that R2 is a derived relationship. In the activity of modeling, derived relationships are not always interesting. Indeed, in Figure 5, we don't want to have Business controller assigned to Closing , even if Closing is composed with Validate provisions . We believe that, in such cases, it is clearer to use tags. When watching a role, we can see immediately at what level of process he is assigned. If we don't use the derive relationship, we can express complex situation of drill down that are, strictly speaking, not complete, but that \"fit\" better to the reality. Functional content of application components A very common requirement that we have in modeling the IT systems in Archimate is the requirement of modeling application functions at different levels. Figure 7 : Accounting system basic view The Figure 7 shows a basic accounting system with implicit assignment and aggregation links. By tagging the functions with their level, we can clarify at what level we are looking at the model. The problem is to represent a zoom on a particular function like shown in Figure 8. Figure 8 : The need to represent an ambiguous assignment This assignment relationship being flagged, the graph view enables a grouping of functions per levels (L1) and (L2) . If we think about coverage, e.g. to answer to the question \"what are, in all views of the model, all the functions of the accounting system?\", we'll have to consider only the (L1) . Figure 9 : The need to represent an ambiguous assignment Indeed the relationship Accounting System to Monthly closing management that we created in a certain viewpoint is a derived relationship, and the consequence of Monthly closing management being aggregated into General ledger management . Without tags, this relation could be ambiguous. With tags, we know right away in Figure 8 that the assignment relation is probably some kind of shortcut, used to highlight a specific point, and that (L1) functions are also assigned to Accounting System , the (L2) Monthly closing management being a (L2) function. We have to be vigilant of those relationships to (L2) elements because they will probably not enable the talk about coverage. If the (L1) functions present a full coverage representation of the application component, the (L2) level may not. Indeed, when using a (L2) level in in (L1) , we should think about the fact that the set of (L2) elements is representing correctly the (L1) . In this sample, the underlying hypothesis is that every application component will be assigned to a hierarchy of (L1) functions. Using tags in this way enables to have a more precise vision of the reality. Alternates solution for nesting An alternate solution was proposed by Philippe Augras in LinkedIn using hierarchy of meanings tagging the various levels of nesting (artifacts and relationships). Another alternate solution was propose by Nicolas Figay in LinkedIn using properties and specific views. Conclusion on nesting Nesting is an important part of big Archimate models. Generally, it is very useful to have certain artifacts (like active structures for instance) attached to several different levels of behaviors depending on the views. Most of the time, tagging artifacts with their level: Enable not to confuse the levels of artifacts, Get interested between the nested relationships between various levels of artifacts, Think about coverage (in our case, processes of a certain level per role, or sub-processes of a certain process). Several instances of the same software in different contexts Big software such as ERP often propose several modules. Documenting the proposed modules can be of interest to be able to capitalize on the descriptions, especially of some customizations were done inside the off-the-shelf software. The figure 10 shows an illustration of that case: The module 3 of the ERP was customized centrally to the company for the needs of the French subsidiary. Figure 10 : Standard and customized modules The (FR) tag will enable to see directly that the function is customized in a certain context. The complexity comes when we will try to represent the various instances of the ERP running for the various subsidiaries. Figure 11 : Several ERP instances In figure 11, we use the specialization relationship to define the \"instances\" of ERP used by several subsidiaries based on the \"central version\" of the ERP. This figure is ambiguous, however. It lets think that every subsidiary is using all modules of the ERP, whereas it is probably not the case. The (L1) ERP is the description of the \"central version\" of the ERP, the one that will be deployed per subsidiary, and the description of its sub-modules. But we also need sub-modules for the instances of that product, knowing that only the French subsidiary is using the customized (L2) ERP module 3 . Figure 12 : Linking sub modules The Figure 12 proposes a way to reuse the sub-modules defined at the (L1) ERP level. This can be sufficient if only the French subsidiary is using the (L2) ERP module 3 . But if the US subsidiary was to use also the module 3 without the French customization, we would have to reconsider the modeling in order to better represent the reality. Figure 13 : Specializing module 3 The figure 13 shows how to specialize module 3 in order to represent the reality. The US ERP will use the standard ERP module 3 with standard functions, whereas the French subsidiary will use a customized version of the module 3 containing the French customizations. By creating another module, specialized from the original module 3, we were able to express the reality. The tags also enable the identification of the various dimensions that we want to keep track of: The various level of nesting of components, The organizations running the software, The business domains. All the tags used in the model should be documented precisely in the model description. They represent a kind of specific taxonomy of the various artifacts and ease the understanding of complex situations. Managing time and scenarios Quite often in the digital transformation process, we need to establish the \"as-is\" situation and the \"to-be\" situation. This can be quite tricky if some naming conventions were not determined to make understand what are the \"as-is\" artifacts and the \"to-be\" ones. For instance, Figure 13 shows us a customized ERP module 3 for the French subsidiary. Let us suppose that this module is the \"to-be\" situation. Figure 14 : As-is situation The Figure 14 would describe the as-is situation. We know we need customized functions for the French subsidiary. Those functions will have to be implemented in some application component, being within the ERP module 3 or in an external module. That leads us to 2 scenarios. Figure 15 : 2 scenarios The Figure 15 shows a way of representing the 2 scenarios: One being a direct modification of module 3 with the new functions (scenario 1); One being a light modification of module 3 with the access to an external application implementing the new functions. In that case, we have 3 artifacts corresponding to the module 3, each of them having a different temporal tag: (as-is) for the current module 3, (s1) for the first scenario, (s2) for the second one. We decided to keep the link between the evolutions of the module 3 ( (s1) and (s2) tags) and their origin tagged (as-is) by using a specialization relationship tagged with the scenario number. This convention enables to work on alternate scenarios. Le's suppose now that we want to describe the following scenario: Step 1: Implementing the customized functions into the ERP module; Step 2: Getting those customized functions out of the ERP module. We would have the situation presented in Figure 16 with (s1) and (s2) representing the steps 1 and 2. Figure 16 : Evolution of solutions with time This solution has an advantage: the ERP content is always exact, because the tags \"as-is\" and \"to-be\" duplicated the \"ERP\" artifact. The drawback is that the ERP artifact is not unique. Another description choice would have been to keep only one artifact for the ERP component. The component would have aggregated the several various \"versions\" of the module 3. In that case, the links between the ERP and the version of sub-module should have been very explicit in order to keep track of the various modifications of the module 3. Using temporal tags enable to better formalize scenarios and successions of steps in a context of digital transformation. This use must be adapted to the purpose of the project. In all cases, the trade-off analysis result should be explained in the model itself. About tags In this article, we have introduced the use of tags in several cases: Nesting Multiple instances Time management The first case can be used for clarity and consistency. Using tags for nesting does not add semantic information to the model, but enables it to be more readable. The case of multiple instances is more complex, because we often want to represent several meanings at the same time, for instance two companies are using the same software but not really the same functions in it. In that case, a specific trade-off must be done in order to find the correct level of expressiveness. The case of time management is particularly useful to represent the various states of a digital transformation, or the various scenarios to go from one state to another. In that case also, the trade-off analysis is important and depends on what we really want to express. One simple rule can be to use different artifacts in the case when those artifacts can be estimated separately. For instance in Figure 15, the various versions of the module 3 will enable different sizing. The fact that we have only one ERP artifact would take the hypothesis that the version of the module 3 has no impact on the full ERP, whereas having 2 ERP artifacts will lead to think that depending on the version of the module 3, the ERP component could be impacted. We can note that those tags are not adding new semantic content (like UML stereotypes can do for instance). In this situation, tags can really be of help in quite a large number of situations. In Figure 11, we used tags to propagate two Archimate information in the tags of the software: Location Main organization using the software This use should be balanced and challenged. We did that in order to materialize the various instances of the software and the fact that those instances were really depending on the where and the who. On the other hand, native Archimate artifacts are existing which can be ambiguous. Sometimes, tags are just temporary ways to represent a complex reality before using a more standard way to do it. Remember that models are dynamic and often work in progress and that there is no definitive way of modeling complex reality. See also Introduction to Archimate Reports can be generated from the Archimate model. You can find here a report template slightly modified compared to the original Archi one. ( November 2019 )","title":"Archimate Recipes"},{"location":"articles/archimate-recipes/#archimate-recipes","text":"Modeling in Archimate can sometimes present some difficulties. In this article, we will expose some tips and tricks that can facilitate your daily life as an enterprise architect using Archimate in your modeling activities.","title":"Archimate recipes"},{"location":"articles/archimate-recipes/#prerequisites","text":"This article is using Archi . This tool is great. If you are using it, please consider making a donation.","title":"Prerequisites"},{"location":"articles/archimate-recipes/#introduction-to-tags","text":"","title":"Introduction to tags"},{"location":"articles/archimate-recipes/#about-tags","text":"Archimate is a very powerful language but sometimes, it can be very useful to create tags to distinguish artifacts or group them into sets. We will put some tags in the name of the artifact in some cases, and in other cases, we can benefit from the use of properties (for instance to process some calculation based on some property values). The convention we use in this article is to \"tag\" the names of the artifacts by placing a label between parenthesis. Tags are sometimes linked to derived relationships but often they answer to other needs.","title":"About tags"},{"location":"articles/archimate-recipes/#why-not-using-stereotypes","text":"Modeling languages like UML or SysML use stereotypes to specialialize model elements and to create new artifacts that derive from a master artifact (one from the standard). Example: If you want to model a graph of data, you may need 2 new artifacts: Node and Relationship instead of Data Objet . You can use the left representation of Figure 1 and create new data objects from the specializations with the UML convention <<stereotype>> . Figure 1 : Modeling a graph with and without stereotypes Stereotypes are creating new classes of objects. Mentioning stereotypes can be a good way to indicate to your fellow modelers (having a UML or SysML background) what were your intentions. But you can also use the right representation and make your classes derive directly from Node and Relationship , considered as implicitit stereotypes of Data Objet . In the rest of the page, we will more focus on tags that are not supposed to replace the stereotype specialization.","title":"Why not using stereotypes?"},{"location":"articles/archimate-recipes/#levels-of-nesting","text":"","title":"Levels of nesting"},{"location":"articles/archimate-recipes/#basics","text":"In an Archimate model, it is quite common to model nested objects like shown in Figure 2. Figure 2 : Sample of nesting in the business layer In Figure 2, we can see that the Accountant role is assigned both to the Accounting business function and to the Closing business process. The Closing artifact is nested into the Accounting function that is, in our model, a business function of the highest level. Being the highest level of \"granularity\" of our processes (in some cases, this level can be the one of the quality system of the company), we can tag the Accounting artifact with the level (L1) to indicate its level. All the business processes and/or business functions that will be nested into it will be tagged (L2) for \"level 2\". All nested objects inside level 2 objects will be tagged level 3 (L3) and so on. An option is to flag the relationship with the level as shown in Figure 2. This can be superfluous if the artifacts are already tagged.","title":"Basics"},{"location":"articles/archimate-recipes/#graph-view","text":"This simple system enables to sort artifacts per level on the Archimate graph view (see Figure 3). Figure 3 : Archimate graph centered in the Accountant artifact When the diagram has many artifacts, it is possible to sort the level of vision that we want to have in the graph view. This kind of need appears for instance when we want to have a good vision of the full coverage of a certain point of view. In the graph view, helped with the tags, we can sort all processes attached to the Accountant role per level in order to determine in, in all our views, we cover what we imagine as being all (L1) processes. Figure 4 : Sorted Archimate graph centered in the Accountant artifact When the model has many views with many artifacts at various different levels, the graph view helps making the model consistent.","title":"Graph view"},{"location":"articles/archimate-recipes/#working-at-different-levels","text":"In Figure 5, working with process levels enabled the enterprise architect to indicate that, at a certain level (L2) , the main role taking in charge the Closing process is the Accountant , but at a lower level, we have other roles taking in charge a part of the process. Figure 5 : Main role and secondary role For sure, all diagrams should be consistent, so possibly not to show the relationships that could be misleading in the view. For instance, in Figure 5, the assignment link between Accountant and (L2) Closing is misleading in the diagram. Because, it this diagram, we should only show the various roles that collaborate to the sub-processes of the (L2) Closing process. Figure 6 : Main role and secondary role (less ambiguous view) Figure 6 is showing a much less ambiguous view. For sure, as soon as we we look at those processes at the (L2) level, the Business controller role will become invisible.","title":"Working at different levels"},{"location":"articles/archimate-recipes/#note-on-derived-relationships","text":"Semantically, we try to express an ambiguity. Figure 2 could say that the Accountant role is assigned to Closing and \"in a more general way\" to the Accounting function. But we know nothing about the other (L2) functions and if the Accountant role is really assigned to all (L2) functions that would compose the Accounting function. The definition of a relationship is not really helping: If two structural or dependency relationships r:R and s:S are permitted between elements a, b, and c such that r(a,b) and s(b,c), then a structural relationship t:T is also permitted, with t(a,c) and type T being the weakest of R and S. Source Let's say C is the composition relationship (\"composes\") and A the assignment relationship (\"is assigned to\"). We have: R1 = Closing C Accounting R2 = Accountant A Accounting R3 = Accountant A Closing So, in the context of structural relationships derivation rules, we could say that R3 + R1 &rarr; R2 . That means that R2 is a derived relationship. In the activity of modeling, derived relationships are not always interesting. Indeed, in Figure 5, we don't want to have Business controller assigned to Closing , even if Closing is composed with Validate provisions . We believe that, in such cases, it is clearer to use tags. When watching a role, we can see immediately at what level of process he is assigned. If we don't use the derive relationship, we can express complex situation of drill down that are, strictly speaking, not complete, but that \"fit\" better to the reality.","title":"Note on derived relationships"},{"location":"articles/archimate-recipes/#functional-content-of-application-components","text":"A very common requirement that we have in modeling the IT systems in Archimate is the requirement of modeling application functions at different levels. Figure 7 : Accounting system basic view The Figure 7 shows a basic accounting system with implicit assignment and aggregation links. By tagging the functions with their level, we can clarify at what level we are looking at the model. The problem is to represent a zoom on a particular function like shown in Figure 8. Figure 8 : The need to represent an ambiguous assignment This assignment relationship being flagged, the graph view enables a grouping of functions per levels (L1) and (L2) . If we think about coverage, e.g. to answer to the question \"what are, in all views of the model, all the functions of the accounting system?\", we'll have to consider only the (L1) . Figure 9 : The need to represent an ambiguous assignment Indeed the relationship Accounting System to Monthly closing management that we created in a certain viewpoint is a derived relationship, and the consequence of Monthly closing management being aggregated into General ledger management . Without tags, this relation could be ambiguous. With tags, we know right away in Figure 8 that the assignment relation is probably some kind of shortcut, used to highlight a specific point, and that (L1) functions are also assigned to Accounting System , the (L2) Monthly closing management being a (L2) function. We have to be vigilant of those relationships to (L2) elements because they will probably not enable the talk about coverage. If the (L1) functions present a full coverage representation of the application component, the (L2) level may not. Indeed, when using a (L2) level in in (L1) , we should think about the fact that the set of (L2) elements is representing correctly the (L1) . In this sample, the underlying hypothesis is that every application component will be assigned to a hierarchy of (L1) functions. Using tags in this way enables to have a more precise vision of the reality.","title":"Functional content of application components"},{"location":"articles/archimate-recipes/#alternates-solution-for-nesting","text":"An alternate solution was proposed by Philippe Augras in LinkedIn using hierarchy of meanings tagging the various levels of nesting (artifacts and relationships). Another alternate solution was propose by Nicolas Figay in LinkedIn using properties and specific views.","title":"Alternates solution for nesting"},{"location":"articles/archimate-recipes/#conclusion-on-nesting","text":"Nesting is an important part of big Archimate models. Generally, it is very useful to have certain artifacts (like active structures for instance) attached to several different levels of behaviors depending on the views. Most of the time, tagging artifacts with their level: Enable not to confuse the levels of artifacts, Get interested between the nested relationships between various levels of artifacts, Think about coverage (in our case, processes of a certain level per role, or sub-processes of a certain process).","title":"Conclusion on nesting"},{"location":"articles/archimate-recipes/#several-instances-of-the-same-software-in-different-contexts","text":"Big software such as ERP often propose several modules. Documenting the proposed modules can be of interest to be able to capitalize on the descriptions, especially of some customizations were done inside the off-the-shelf software. The figure 10 shows an illustration of that case: The module 3 of the ERP was customized centrally to the company for the needs of the French subsidiary. Figure 10 : Standard and customized modules The (FR) tag will enable to see directly that the function is customized in a certain context. The complexity comes when we will try to represent the various instances of the ERP running for the various subsidiaries. Figure 11 : Several ERP instances In figure 11, we use the specialization relationship to define the \"instances\" of ERP used by several subsidiaries based on the \"central version\" of the ERP. This figure is ambiguous, however. It lets think that every subsidiary is using all modules of the ERP, whereas it is probably not the case. The (L1) ERP is the description of the \"central version\" of the ERP, the one that will be deployed per subsidiary, and the description of its sub-modules. But we also need sub-modules for the instances of that product, knowing that only the French subsidiary is using the customized (L2) ERP module 3 . Figure 12 : Linking sub modules The Figure 12 proposes a way to reuse the sub-modules defined at the (L1) ERP level. This can be sufficient if only the French subsidiary is using the (L2) ERP module 3 . But if the US subsidiary was to use also the module 3 without the French customization, we would have to reconsider the modeling in order to better represent the reality. Figure 13 : Specializing module 3 The figure 13 shows how to specialize module 3 in order to represent the reality. The US ERP will use the standard ERP module 3 with standard functions, whereas the French subsidiary will use a customized version of the module 3 containing the French customizations. By creating another module, specialized from the original module 3, we were able to express the reality. The tags also enable the identification of the various dimensions that we want to keep track of: The various level of nesting of components, The organizations running the software, The business domains. All the tags used in the model should be documented precisely in the model description. They represent a kind of specific taxonomy of the various artifacts and ease the understanding of complex situations.","title":"Several instances of the same software in different contexts"},{"location":"articles/archimate-recipes/#managing-time-and-scenarios","text":"Quite often in the digital transformation process, we need to establish the \"as-is\" situation and the \"to-be\" situation. This can be quite tricky if some naming conventions were not determined to make understand what are the \"as-is\" artifacts and the \"to-be\" ones. For instance, Figure 13 shows us a customized ERP module 3 for the French subsidiary. Let us suppose that this module is the \"to-be\" situation. Figure 14 : As-is situation The Figure 14 would describe the as-is situation. We know we need customized functions for the French subsidiary. Those functions will have to be implemented in some application component, being within the ERP module 3 or in an external module. That leads us to 2 scenarios. Figure 15 : 2 scenarios The Figure 15 shows a way of representing the 2 scenarios: One being a direct modification of module 3 with the new functions (scenario 1); One being a light modification of module 3 with the access to an external application implementing the new functions. In that case, we have 3 artifacts corresponding to the module 3, each of them having a different temporal tag: (as-is) for the current module 3, (s1) for the first scenario, (s2) for the second one. We decided to keep the link between the evolutions of the module 3 ( (s1) and (s2) tags) and their origin tagged (as-is) by using a specialization relationship tagged with the scenario number. This convention enables to work on alternate scenarios. Le's suppose now that we want to describe the following scenario: Step 1: Implementing the customized functions into the ERP module; Step 2: Getting those customized functions out of the ERP module. We would have the situation presented in Figure 16 with (s1) and (s2) representing the steps 1 and 2. Figure 16 : Evolution of solutions with time This solution has an advantage: the ERP content is always exact, because the tags \"as-is\" and \"to-be\" duplicated the \"ERP\" artifact. The drawback is that the ERP artifact is not unique. Another description choice would have been to keep only one artifact for the ERP component. The component would have aggregated the several various \"versions\" of the module 3. In that case, the links between the ERP and the version of sub-module should have been very explicit in order to keep track of the various modifications of the module 3. Using temporal tags enable to better formalize scenarios and successions of steps in a context of digital transformation. This use must be adapted to the purpose of the project. In all cases, the trade-off analysis result should be explained in the model itself.","title":"Managing time and scenarios"},{"location":"articles/archimate-recipes/#about-tags_1","text":"In this article, we have introduced the use of tags in several cases: Nesting Multiple instances Time management The first case can be used for clarity and consistency. Using tags for nesting does not add semantic information to the model, but enables it to be more readable. The case of multiple instances is more complex, because we often want to represent several meanings at the same time, for instance two companies are using the same software but not really the same functions in it. In that case, a specific trade-off must be done in order to find the correct level of expressiveness. The case of time management is particularly useful to represent the various states of a digital transformation, or the various scenarios to go from one state to another. In that case also, the trade-off analysis is important and depends on what we really want to express. One simple rule can be to use different artifacts in the case when those artifacts can be estimated separately. For instance in Figure 15, the various versions of the module 3 will enable different sizing. The fact that we have only one ERP artifact would take the hypothesis that the version of the module 3 has no impact on the full ERP, whereas having 2 ERP artifacts will lead to think that depending on the version of the module 3, the ERP component could be impacted. We can note that those tags are not adding new semantic content (like UML stereotypes can do for instance). In this situation, tags can really be of help in quite a large number of situations. In Figure 11, we used tags to propagate two Archimate information in the tags of the software: Location Main organization using the software This use should be balanced and challenged. We did that in order to materialize the various instances of the software and the fact that those instances were really depending on the where and the who. On the other hand, native Archimate artifacts are existing which can be ambiguous. Sometimes, tags are just temporary ways to represent a complex reality before using a more standard way to do it. Remember that models are dynamic and often work in progress and that there is no definitive way of modeling complex reality.","title":"About tags"},{"location":"articles/archimate-recipes/#see-also","text":"Introduction to Archimate Reports can be generated from the Archimate model. You can find here a report template slightly modified compared to the original Archi one. ( November 2019 )","title":"See also"},{"location":"articles/data-interop/","text":"The real nature of data \"Data is the new gold\". \"Data drives the world\". \"We are in a data driven world\". All medias, and not especially the IT medias, seem obsessed with data, especially big data, and the value they are representing or should represent. For decades now, we are running on data. Big data is just another step in a long-term trend that is just beginning. As always, the marketing speeches around big data are over-selling promises as they did in the past with relational databases, datawarehouses, datamarts, etc.). This article explains that data is not a simple absolute usable IT \"product\" that can be used, in all cases, as a source material . On the contrary, it is an elaborated product, fruit of a certain set of use cases and technical constraints, which makes its usability complex in many cases, which explains why some concepts like datalakes are not working in any case. Describing the business Software development is all about representation One of the main objectives of IT is to automate the business processes and enable a better business management. This is achieved by storing the business concepts and manage the business rules associated to them and that control their life cycle. In order to do a good job, the IT people must create a working representation of the business, both static and dynamic to be able to create the software applications that will automate the business. Application development is, consequently, all about representating . IT representations are like all representations: They are relative and changing. This fact has many consequences on data, as we will see. Business concepts and business processes In every business domain, we can exhibit business concepts. Those business concepts are linked together with various kinds of relationships. They establish a first representation of the business. Business processes are manipulating those business concepts, creating them, changing their state, linking or unlinking them together. Business processes establish the second dimension of the representation of business (to simplify, we will consider that business rules are part of the business processes). From this double representation, software designers can do many things: Create applications that manipulate those concepts and the concepts life cycles; Create databases that store those concepts; Create business workflows that will be a way to assist the business processes with software. For each business, we can define the set of all business processes and business concepts of the business domain as the business semantic space . This set is a theoretical concept that contains all possible IT representations of a particular business . Every software attached to a business domain is covering a part of the related business semantic space. Enriching the representations With time and business digital maturity, the business concepts and business processes are enriched. This phenomenon is a big constant of the IT world. Let us take an example: accounting. In the 80s, every company that had the financial means had their own accounting system, developed by themselves. The legislation was, often, not very detailed and each company had interpreted it in slightly different ways. With time, the legislation became more detailed and companies found it hard to implement the concepts and processes. Software companies provided generic accounting software that were taking in charge all concepts and processes of the accounting business and that could be parameterized to the special case of each company. Figure 1 : The evolution of the business concepts and processes With time and maturity, the considered business discovers progressively how to structure its business processes and business concepts: it is shaping its business by creating the most adequate representations at a certain point in time. That activity is called modeling . The Fig. 1 tries to represent this progressive representation enrichment in the case of a regulated business. At the beginning, both company 1 and company 2 have implemented their interpretation of the regulation: They both cover it (and more), but in different ways. At this stage, one company can have a competitive advantage on the other, if its interpretation is having better business impact. Then, the regulation expands (step 2 in Fig. 1): Company 1 covers it but company 2 has a hard time to do it. After a certain time, companies 1 and 2 will use an off-the-shelf software to cover the regulation (step n). Company 2 will still have a specific application connected to the off-the-shelf software. In that case, company 2 may have a better digitization of its business domain than company 1 by covering more of the business semantic space with a specific software. This maturity cycle is a crucial dynamic process in IT, and every IT architect should be aware of it. There are several signs that enable to to estimate the maturity of a business in terms of knowledge representation (business concepts and business processes). The starting point: A paper/Excel-based process As long as the business concepts are not known for what they really are, the business is not represented (at least from an IT standpoint): it is not conceptualized. At this moment, every user can get satisfied with generic multi-purpose tools like Excel. In that case, the business knowledge can be: Located in procedures, written in documents, and that everyone has to follow (but with no way to control the real application of those procedures); People-driven: everyone is doing the business as he wants, provided the end result is obtained (which is causing recurring troubles to work in teams). This kind of processes is both dangerous for the company because it relies on individuals and can be lost when knowledgeable people are no more in charge, and not scalable, so not able to run with more business activity. The business teams need a software to manage their business: it will be the first generation of software. This generation will embed the first IT representation of the business (business concepts and business processes). It will address the first business use cases of identified business roles. As soon as the business concepts are represented, they have a structure, they generally have fields of a certain type, they have links with one another and it is no more possible to manage them with Excel. The evolution of the business representation Progressively, the business have a better understanding of its business, and some concepts will become more detailed and new concepts will appear (or just will be \"named\"). In those cases, the business representation will evolve: instead of having one business concept, the business team realizes that 2 concepts are indeed hidden in the original one; or the reverse, that 2 different concepts are, at last, the same. This evolution enables to make business processes evolve and, quite often, gain in productivity. For sure, we have to insist: the new representation is different from the previous one, generally covering more space in the business semantic space. But it is still relative to the use cases and the roles taken in charge. It is still a representation, and a representation is not absolute. This phase is interesting because it can also exhibits a new kind of concept: management concepts , like \"task\", \"folder\", \"affair\", more generic concepts that could be used in another business domain, and that are used to manage things. Next generation software The next generation of software is generally built on both the experience of the previous one and on the new ways of working of the domain (for instance imposed by the regulation). Its perspective is often larger and encompasses a larger set of use cases. It becomes necessary when the previous system shows limitations in terms of evolutions. Indeed, in software, the way of representing the business concepts and business processes is largely sub-optimal (see our works on the graph-oriented programming ), which implies the necessity to rewrite the business application on a larger set of hypothesis, to keep on gaining in efficiency and productivity. The representation can be deeply modified, being on a business process level and on a business concept level. That means that: There is a need for change management to adapt the processes; There is a need for data migration to transform the old concepts into the new ones; There is a need to reconsider the interfaces between the system that evolves and its ecosystem. All mature businesses went to several generations of software. Let us take back the accounting sample. Many companies are now running their 3rd or 4th generation of accounting software. We can consider that the business is quite mature, and for sure, if we see the large functional scope of modern accounting software, we see that most concepts, along with the regulation, are quite mature. Connecting business domains An important sign of business maturity is the presence in the business of a multi-company standard exchange format , like the IATA transactions in the travel business, the Swift system in the banking area or the Step and ASD standards of the aerospace industry. Those standards are generally the fruits of impressive collective efforts to provide a multi-company shared vision of one or several business domains, in terms of business processes and in terms of business concepts. The presence of exchange standards are very important, not only because they define common business concepts and business processes, but because they define a frontier between the different business semantic spaces. Figure 2 : Connecting several semantic spaces The Fig. 2 is representing this situation of 2 business semantic spaces exchange concepts through an exchange format. Note that Fig. 2 represents 2 applications, application 1 and 2 respectively in the semantic spaces A and B. For them, for sure, the exchange format will be a subset of the concepts and processes they manipulate. Indeed, even if the business semantic space is hard to define in terms of exact scope, collective efforts to define interchange formats lead to the conclusion that several business: Are different; Need to share some concepts and be part of a larger process. These 2 very basic statements should not be taken lightly, because they have deep consequences on the shape of global IT systems. Indeed, there are several business domains in the world, each of them having their own concepts and processes. They sometimes have to be part of a larger process and have to share concepts in that purpose. But being able to exchange concepts does not mean that they are in the same semantic space. The optimal frontiers between different semantic spaces are the root problem behind an efficient set of IT systems, a global company performance, and an optimal business function placement (in French: \"urbanisation\"). Optimal business function placement is one of the most important role of the enterprise architect. If the systems are respecting the business semantic spaces, the company will be efficient, and every business will be able to evolve independently from the others provided the exchange formats are respected. If the business functions of various business semantic spaces are mixed together, there will be a huge confusion in processes and responsibilities and it will be very complicated for every business to evolve without impact the full company. Because, with time, business representations are changing, while staying in the same business semantic spaces, businesses deserve to have the capability to mature without being too tight to other businesses, even if they share a large number of business concepts (but not represented the same way). That is why exchange standards are so important: because they define the perimeter of the various business semantic spaces and enable to build large processes among them; and because this representation is the fruit of a collective effort of experts in the particular business domains. The real nature of data A attempt to define \"data\" As we explain, business concepts are representations. They can have many representations in the same semantic space. We can define \"data\" as follows: Data are projections of the business concepts on technical constraints . Figure 3 : Data are viewpoints on business concepts of the semantic space This simple representation of Fig. 3 is trying to put in focus several points: An application is defining a viewpoint on the business semantic space; This viewpoint is not absolute, it is a representation of business processes and business concepts that is the fruit of business roles and business use cases; Data are a projection of the business concepts in the space of technical constraints; This projection is a transformation of the business concepts; It is not absolute, and it depends upon the programming languages, the database technology, the performance requirements, etc. In other words, data are a technical transformation of a business representation of a business concept. The relativity of data As we saw in the Connecting domains section and in Fig. 2 , business domains can communicate together with the help of exchange formats. Let us examine this process at the data level by considering Fig. 4. Figure 4 : Data models are relative At the data level we can see 3 different representations of the same interchange concepts: The are 4 business objects in the data model of application 1; There are only 3 business objects in the data model of application 2; In the exchange format, there is no explicit mention of the links between A , B , C and D , but a grouping of those business objects and a cardinality grammar. Those 3 different representations of data are semantically equivalent , but they are different representations of the same reality. Two more comments can be added to that description: The data model we see for application 1 and application 2 is just a subset of their complete data model that will manipulate concepts that are not known from the other application (because they are in 2 different business domains); The interchange format is transported in a service way, within the concept of a verb (see our article on web services here and here . Aggregating data Let us suppose, taking the sample of Fig. 4 , that the application 1 publishes data to application 2. If we were to aggregate data from application 1 and application 2, we would be facing a problem: what data should we consider? It is not an easy problem. The application 1 offers more details in modeling A and C as different entities, whereas application 2 is using AunionC instead. But in terms of sequence, if data in application 2 is fresher than data in application 1 (because it is after in the flow), should not we take data in application 2 instead of data in application 1? But who is really the master in those data? And what about we need data connected to C ? Is it equivalent to connect those data to AunionC ? Figure 5 : What data should be in the data lake? Fig. 5 shows the problem graphically. It is possible to solve this issue on two conditions: To explicitly determine what we intend to do by aggregating the data from those two applications; What is the point in doing that? What treatments we want to implement? For what functional purpose? To have a minimum of functional and technical knowledge about applications 1 and 2, why are they communicating together, to what purpose, at what moment, etc. Aggregating data is not an end, it is a solution answering to a business question. So what is the business question? About data lakes The data lake for real use cases In some business, the interest of manipulating a huge amount of data is obvious: In Internet retail for instance: for big websites, you have a massive amount of data for each customer visiting your website with all information related to the consultations and to the purchases; Those data are being largely used for years; In the industry: Most production assembly lines generate a lot of data that are used to monitor the quality of the product and the productivity; In public services: Analyzing in details at the country level the various business trends, employment data and so on, can be done now from the real low level real data instead of using representative samples; Etc. What is common between all those samples is that the use cases of those big data are very clear and the big data technologies are just a way to answer to them. In a certain way, we are just talking about classical software applications using big data technology to store many more data than classical applications used to do. The data lake for no use cases The fact is most data lakes projects are not focusing on real use cases: they do the reverse. They are infrastructure projects to enable future use cases . They are called enablers which is a nice word to say: \"let's do a technical project and we'll see afterwards if it can be used on real use cases\" (1) . Big data vendors are so powerful (as datawarehouse vendors decades ago) and so trendy, that they convince enterprises to put in place big data infrastructure prior to having real use cases. The speech is seductive: In order to realize whatever use case you will want to implement in the future, your company needs to put all company data in a data lake; This will enable you to correlate all data you want and find value in your sleeping data. This speech has the advantage of seducing end users that understand the concept, and IT people that are thrilled by the introduction of new technology (2) . For sure, the fact that data lakes with real use cases work are confusing the situation. For data lakes like for other topics, providing technology without a real use case is a very risky operation, that generally ends badly: Millions invested for nothing. ure out what system is master at what point in time. Data lake as an application Indeed, working data lakes are applications (with big data). But, if this reasoning is true, that would mean that a data lake is an application rather than a data store , that it has some data ingestion business logic to be able to ingest data that fit in a global data landscape without introducing ambiguities or erroneous data links. But, if that is the case, that means that the data lake is also creating its own viewpoint on the semantic space, like shown if Fig. 6, which most of the time is built on the pivot models we find in exchange formats. Figure 6 : The data lake as another kind of application Semantic technology applied to data lakes The most recent flavor of the ever lasting concept of data warehouse is the semantic data lake . The idea can be summarized as follows: with the proper ontological description (ontologies being Semantic Web way of describing concepts), it is possible to aggregate all sorts of data and link them together. Fig. 4 is showing that, for the same concepts, we have 3 ways of representing it. The ontology approach will unfortunately not change this fact. For each representation, we can define a specific ontology that will not be the same and that will show knowledge representation choices (or modeling choices). For sure, we could define graph transformations that would make those 3 ontologies equivalent: but they are not the same. So Semantic Web technologies will not help in defining the \"data lake for no use cases\". As we saw, the modeling formalism is not important because business concepts are representations, and so are relative to the particular point of view of the application (roles and use cases). Conclusion The big data technologies are impressive. They come from the Internet age and from the solutions Cloud companies found to manage volumes of data that are much bigger than anything the humanity saw in the 20th century. Those technologies can be very useful for identified use cases, such as the industrial use cases (to better the product design, to monitor the product product, to manage the in-service life of products) or the Internet retail. But those technologies should not be envisaged outside of clearly defined use cases, first of all because all big data technologies cannot answer to all big data problems (e.g. large time series oriented data processing), and second of all because data is a relative object, quite often product of complex upstream processes. Furthermore, big data will always be expensive, and as all expensive technologies, the business case of using it should be carefully analyzed before investing. See Also Considerations About Rest And Web Services About GraphQL Notes (1) - The modern vocabulary of IT is a real regression compared to UML for instance. By talking about \"features\" and \"enablers\", it opens the door to develop \"features\" for unknown business roles and business processes, and to put in place \"enablers\", so technical infrastructure for the day when people will need it. This semantic shift is the result of a large-scale technical marketing of the large Cloud companies: instead of thinking about enhancing the business, most IT people and more and more business people think about technology. Back to text . (2) - Most IT people find it hard to enter into the knowledge of the business. They prefer working on technological assets. That is why, it is an important role of the IT management to continuously refocus IT people to answer business requirements or to help businesses use wisely the IT resources to solve their business problems. Without this continuous refocus, the gap between business and IT people is growing, opening the door to \"IT done by the business\" syndrome, sign of that impossibility to communicate. Back to text . ( February 2020 )","title":"The real nature of data"},{"location":"articles/data-interop/#the-real-nature-of-data","text":"\"Data is the new gold\". \"Data drives the world\". \"We are in a data driven world\". All medias, and not especially the IT medias, seem obsessed with data, especially big data, and the value they are representing or should represent. For decades now, we are running on data. Big data is just another step in a long-term trend that is just beginning. As always, the marketing speeches around big data are over-selling promises as they did in the past with relational databases, datawarehouses, datamarts, etc.). This article explains that data is not a simple absolute usable IT \"product\" that can be used, in all cases, as a source material . On the contrary, it is an elaborated product, fruit of a certain set of use cases and technical constraints, which makes its usability complex in many cases, which explains why some concepts like datalakes are not working in any case.","title":"The real nature of data"},{"location":"articles/data-interop/#describing-the-business","text":"","title":"Describing the business"},{"location":"articles/data-interop/#software-development-is-all-about-representation","text":"One of the main objectives of IT is to automate the business processes and enable a better business management. This is achieved by storing the business concepts and manage the business rules associated to them and that control their life cycle. In order to do a good job, the IT people must create a working representation of the business, both static and dynamic to be able to create the software applications that will automate the business. Application development is, consequently, all about representating . IT representations are like all representations: They are relative and changing. This fact has many consequences on data, as we will see.","title":"Software development is all about representation"},{"location":"articles/data-interop/#business-concepts-and-business-processes","text":"In every business domain, we can exhibit business concepts. Those business concepts are linked together with various kinds of relationships. They establish a first representation of the business. Business processes are manipulating those business concepts, creating them, changing their state, linking or unlinking them together. Business processes establish the second dimension of the representation of business (to simplify, we will consider that business rules are part of the business processes). From this double representation, software designers can do many things: Create applications that manipulate those concepts and the concepts life cycles; Create databases that store those concepts; Create business workflows that will be a way to assist the business processes with software. For each business, we can define the set of all business processes and business concepts of the business domain as the business semantic space . This set is a theoretical concept that contains all possible IT representations of a particular business . Every software attached to a business domain is covering a part of the related business semantic space.","title":"Business concepts and business processes"},{"location":"articles/data-interop/#enriching-the-representations","text":"With time and business digital maturity, the business concepts and business processes are enriched. This phenomenon is a big constant of the IT world. Let us take an example: accounting. In the 80s, every company that had the financial means had their own accounting system, developed by themselves. The legislation was, often, not very detailed and each company had interpreted it in slightly different ways. With time, the legislation became more detailed and companies found it hard to implement the concepts and processes. Software companies provided generic accounting software that were taking in charge all concepts and processes of the accounting business and that could be parameterized to the special case of each company. Figure 1 : The evolution of the business concepts and processes With time and maturity, the considered business discovers progressively how to structure its business processes and business concepts: it is shaping its business by creating the most adequate representations at a certain point in time. That activity is called modeling . The Fig. 1 tries to represent this progressive representation enrichment in the case of a regulated business. At the beginning, both company 1 and company 2 have implemented their interpretation of the regulation: They both cover it (and more), but in different ways. At this stage, one company can have a competitive advantage on the other, if its interpretation is having better business impact. Then, the regulation expands (step 2 in Fig. 1): Company 1 covers it but company 2 has a hard time to do it. After a certain time, companies 1 and 2 will use an off-the-shelf software to cover the regulation (step n). Company 2 will still have a specific application connected to the off-the-shelf software. In that case, company 2 may have a better digitization of its business domain than company 1 by covering more of the business semantic space with a specific software. This maturity cycle is a crucial dynamic process in IT, and every IT architect should be aware of it. There are several signs that enable to to estimate the maturity of a business in terms of knowledge representation (business concepts and business processes).","title":"Enriching the representations"},{"location":"articles/data-interop/#the-starting-point-a-paperexcel-based-process","text":"As long as the business concepts are not known for what they really are, the business is not represented (at least from an IT standpoint): it is not conceptualized. At this moment, every user can get satisfied with generic multi-purpose tools like Excel. In that case, the business knowledge can be: Located in procedures, written in documents, and that everyone has to follow (but with no way to control the real application of those procedures); People-driven: everyone is doing the business as he wants, provided the end result is obtained (which is causing recurring troubles to work in teams). This kind of processes is both dangerous for the company because it relies on individuals and can be lost when knowledgeable people are no more in charge, and not scalable, so not able to run with more business activity. The business teams need a software to manage their business: it will be the first generation of software. This generation will embed the first IT representation of the business (business concepts and business processes). It will address the first business use cases of identified business roles. As soon as the business concepts are represented, they have a structure, they generally have fields of a certain type, they have links with one another and it is no more possible to manage them with Excel.","title":"The starting point: A paper/Excel-based process"},{"location":"articles/data-interop/#the-evolution-of-the-business-representation","text":"Progressively, the business have a better understanding of its business, and some concepts will become more detailed and new concepts will appear (or just will be \"named\"). In those cases, the business representation will evolve: instead of having one business concept, the business team realizes that 2 concepts are indeed hidden in the original one; or the reverse, that 2 different concepts are, at last, the same. This evolution enables to make business processes evolve and, quite often, gain in productivity. For sure, we have to insist: the new representation is different from the previous one, generally covering more space in the business semantic space. But it is still relative to the use cases and the roles taken in charge. It is still a representation, and a representation is not absolute. This phase is interesting because it can also exhibits a new kind of concept: management concepts , like \"task\", \"folder\", \"affair\", more generic concepts that could be used in another business domain, and that are used to manage things.","title":"The evolution of the business representation"},{"location":"articles/data-interop/#next-generation-software","text":"The next generation of software is generally built on both the experience of the previous one and on the new ways of working of the domain (for instance imposed by the regulation). Its perspective is often larger and encompasses a larger set of use cases. It becomes necessary when the previous system shows limitations in terms of evolutions. Indeed, in software, the way of representing the business concepts and business processes is largely sub-optimal (see our works on the graph-oriented programming ), which implies the necessity to rewrite the business application on a larger set of hypothesis, to keep on gaining in efficiency and productivity. The representation can be deeply modified, being on a business process level and on a business concept level. That means that: There is a need for change management to adapt the processes; There is a need for data migration to transform the old concepts into the new ones; There is a need to reconsider the interfaces between the system that evolves and its ecosystem. All mature businesses went to several generations of software. Let us take back the accounting sample. Many companies are now running their 3rd or 4th generation of accounting software. We can consider that the business is quite mature, and for sure, if we see the large functional scope of modern accounting software, we see that most concepts, along with the regulation, are quite mature.","title":"Next generation software"},{"location":"articles/data-interop/#connecting-business-domains","text":"An important sign of business maturity is the presence in the business of a multi-company standard exchange format , like the IATA transactions in the travel business, the Swift system in the banking area or the Step and ASD standards of the aerospace industry. Those standards are generally the fruits of impressive collective efforts to provide a multi-company shared vision of one or several business domains, in terms of business processes and in terms of business concepts. The presence of exchange standards are very important, not only because they define common business concepts and business processes, but because they define a frontier between the different business semantic spaces. Figure 2 : Connecting several semantic spaces The Fig. 2 is representing this situation of 2 business semantic spaces exchange concepts through an exchange format. Note that Fig. 2 represents 2 applications, application 1 and 2 respectively in the semantic spaces A and B. For them, for sure, the exchange format will be a subset of the concepts and processes they manipulate. Indeed, even if the business semantic space is hard to define in terms of exact scope, collective efforts to define interchange formats lead to the conclusion that several business: Are different; Need to share some concepts and be part of a larger process. These 2 very basic statements should not be taken lightly, because they have deep consequences on the shape of global IT systems. Indeed, there are several business domains in the world, each of them having their own concepts and processes. They sometimes have to be part of a larger process and have to share concepts in that purpose. But being able to exchange concepts does not mean that they are in the same semantic space. The optimal frontiers between different semantic spaces are the root problem behind an efficient set of IT systems, a global company performance, and an optimal business function placement (in French: \"urbanisation\"). Optimal business function placement is one of the most important role of the enterprise architect. If the systems are respecting the business semantic spaces, the company will be efficient, and every business will be able to evolve independently from the others provided the exchange formats are respected. If the business functions of various business semantic spaces are mixed together, there will be a huge confusion in processes and responsibilities and it will be very complicated for every business to evolve without impact the full company. Because, with time, business representations are changing, while staying in the same business semantic spaces, businesses deserve to have the capability to mature without being too tight to other businesses, even if they share a large number of business concepts (but not represented the same way). That is why exchange standards are so important: because they define the perimeter of the various business semantic spaces and enable to build large processes among them; and because this representation is the fruit of a collective effort of experts in the particular business domains.","title":"Connecting business domains"},{"location":"articles/data-interop/#the-real-nature-of-data_1","text":"","title":"The real nature of data"},{"location":"articles/data-interop/#a-attempt-to-define-data","text":"As we explain, business concepts are representations. They can have many representations in the same semantic space. We can define \"data\" as follows: Data are projections of the business concepts on technical constraints . Figure 3 : Data are viewpoints on business concepts of the semantic space This simple representation of Fig. 3 is trying to put in focus several points: An application is defining a viewpoint on the business semantic space; This viewpoint is not absolute, it is a representation of business processes and business concepts that is the fruit of business roles and business use cases; Data are a projection of the business concepts in the space of technical constraints; This projection is a transformation of the business concepts; It is not absolute, and it depends upon the programming languages, the database technology, the performance requirements, etc. In other words, data are a technical transformation of a business representation of a business concept.","title":"A attempt to define \"data\""},{"location":"articles/data-interop/#the-relativity-of-data","text":"As we saw in the Connecting domains section and in Fig. 2 , business domains can communicate together with the help of exchange formats. Let us examine this process at the data level by considering Fig. 4. Figure 4 : Data models are relative At the data level we can see 3 different representations of the same interchange concepts: The are 4 business objects in the data model of application 1; There are only 3 business objects in the data model of application 2; In the exchange format, there is no explicit mention of the links between A , B , C and D , but a grouping of those business objects and a cardinality grammar. Those 3 different representations of data are semantically equivalent , but they are different representations of the same reality. Two more comments can be added to that description: The data model we see for application 1 and application 2 is just a subset of their complete data model that will manipulate concepts that are not known from the other application (because they are in 2 different business domains); The interchange format is transported in a service way, within the concept of a verb (see our article on web services here and here .","title":"The relativity of data"},{"location":"articles/data-interop/#aggregating-data","text":"Let us suppose, taking the sample of Fig. 4 , that the application 1 publishes data to application 2. If we were to aggregate data from application 1 and application 2, we would be facing a problem: what data should we consider? It is not an easy problem. The application 1 offers more details in modeling A and C as different entities, whereas application 2 is using AunionC instead. But in terms of sequence, if data in application 2 is fresher than data in application 1 (because it is after in the flow), should not we take data in application 2 instead of data in application 1? But who is really the master in those data? And what about we need data connected to C ? Is it equivalent to connect those data to AunionC ? Figure 5 : What data should be in the data lake? Fig. 5 shows the problem graphically. It is possible to solve this issue on two conditions: To explicitly determine what we intend to do by aggregating the data from those two applications; What is the point in doing that? What treatments we want to implement? For what functional purpose? To have a minimum of functional and technical knowledge about applications 1 and 2, why are they communicating together, to what purpose, at what moment, etc. Aggregating data is not an end, it is a solution answering to a business question. So what is the business question?","title":"Aggregating data"},{"location":"articles/data-interop/#about-data-lakes","text":"","title":"About data lakes"},{"location":"articles/data-interop/#the-data-lake-for-real-use-cases","text":"In some business, the interest of manipulating a huge amount of data is obvious: In Internet retail for instance: for big websites, you have a massive amount of data for each customer visiting your website with all information related to the consultations and to the purchases; Those data are being largely used for years; In the industry: Most production assembly lines generate a lot of data that are used to monitor the quality of the product and the productivity; In public services: Analyzing in details at the country level the various business trends, employment data and so on, can be done now from the real low level real data instead of using representative samples; Etc. What is common between all those samples is that the use cases of those big data are very clear and the big data technologies are just a way to answer to them. In a certain way, we are just talking about classical software applications using big data technology to store many more data than classical applications used to do.","title":"The data lake for real use cases"},{"location":"articles/data-interop/#the-data-lake-for-no-use-cases","text":"The fact is most data lakes projects are not focusing on real use cases: they do the reverse. They are infrastructure projects to enable future use cases . They are called enablers which is a nice word to say: \"let's do a technical project and we'll see afterwards if it can be used on real use cases\" (1) . Big data vendors are so powerful (as datawarehouse vendors decades ago) and so trendy, that they convince enterprises to put in place big data infrastructure prior to having real use cases. The speech is seductive: In order to realize whatever use case you will want to implement in the future, your company needs to put all company data in a data lake; This will enable you to correlate all data you want and find value in your sleeping data. This speech has the advantage of seducing end users that understand the concept, and IT people that are thrilled by the introduction of new technology (2) . For sure, the fact that data lakes with real use cases work are confusing the situation. For data lakes like for other topics, providing technology without a real use case is a very risky operation, that generally ends badly: Millions invested for nothing. ure out what system is master at what point in time.","title":"The data lake for no use cases"},{"location":"articles/data-interop/#data-lake-as-an-application","text":"Indeed, working data lakes are applications (with big data). But, if this reasoning is true, that would mean that a data lake is an application rather than a data store , that it has some data ingestion business logic to be able to ingest data that fit in a global data landscape without introducing ambiguities or erroneous data links. But, if that is the case, that means that the data lake is also creating its own viewpoint on the semantic space, like shown if Fig. 6, which most of the time is built on the pivot models we find in exchange formats. Figure 6 : The data lake as another kind of application","title":"Data lake as an application"},{"location":"articles/data-interop/#semantic-technology-applied-to-data-lakes","text":"The most recent flavor of the ever lasting concept of data warehouse is the semantic data lake . The idea can be summarized as follows: with the proper ontological description (ontologies being Semantic Web way of describing concepts), it is possible to aggregate all sorts of data and link them together. Fig. 4 is showing that, for the same concepts, we have 3 ways of representing it. The ontology approach will unfortunately not change this fact. For each representation, we can define a specific ontology that will not be the same and that will show knowledge representation choices (or modeling choices). For sure, we could define graph transformations that would make those 3 ontologies equivalent: but they are not the same. So Semantic Web technologies will not help in defining the \"data lake for no use cases\". As we saw, the modeling formalism is not important because business concepts are representations, and so are relative to the particular point of view of the application (roles and use cases).","title":"Semantic technology applied to data lakes"},{"location":"articles/data-interop/#conclusion","text":"The big data technologies are impressive. They come from the Internet age and from the solutions Cloud companies found to manage volumes of data that are much bigger than anything the humanity saw in the 20th century. Those technologies can be very useful for identified use cases, such as the industrial use cases (to better the product design, to monitor the product product, to manage the in-service life of products) or the Internet retail. But those technologies should not be envisaged outside of clearly defined use cases, first of all because all big data technologies cannot answer to all big data problems (e.g. large time series oriented data processing), and second of all because data is a relative object, quite often product of complex upstream processes. Furthermore, big data will always be expensive, and as all expensive technologies, the business case of using it should be carefully analyzed before investing.","title":"Conclusion"},{"location":"articles/data-interop/#see-also","text":"Considerations About Rest And Web Services About GraphQL","title":"See Also"},{"location":"articles/data-interop/#notes","text":"(1) - The modern vocabulary of IT is a real regression compared to UML for instance. By talking about \"features\" and \"enablers\", it opens the door to develop \"features\" for unknown business roles and business processes, and to put in place \"enablers\", so technical infrastructure for the day when people will need it. This semantic shift is the result of a large-scale technical marketing of the large Cloud companies: instead of thinking about enhancing the business, most IT people and more and more business people think about technology. Back to text . (2) - Most IT people find it hard to enter into the knowledge of the business. They prefer working on technological assets. That is why, it is an important role of the IT management to continuously refocus IT people to answer business requirements or to help businesses use wisely the IT resources to solve their business problems. Without this continuous refocus, the gap between business and IT people is growing, opening the door to \"IT done by the business\" syndrome, sign of that impossibility to communicate. Back to text . ( February 2020 )","title":"Notes"},{"location":"articles/five-levels/","text":"The Five Levels of Conceptual Maturity for IT Teams IT is a strange activity because it is an industry that has troubles getting out of a craft industry approach. Constantly moving to new exciting technologies and constantly influenced by massive technology centered marketing, IT people stay durably too technical and have recurring troubles to achieve IT projects. History of the Model Years ago, I was consulting a CEO and was trying to explain to him why his teams were failing big IT projects. A lot of executives today are digitally aware in their daily business practices but, when it comes to organizing new IT projects inside their company, they frequently discover that there is a huge gap between what they wanted to achieve and what is really done. Money is not the problem, and many big companies invest massive amounts of money in IT projects without getting the most of it. The fact is, when you invest too much in an IT project, your chances of success decrease drastically. I came to the CEO with this maturity model. I don't proclaim it to be perfect, far from it: it's just a representation of the problem. But the model is quite useful to explain and diagnose the capability of teams to perform IT projects and the levels of complexity attached to those projects. A Model of Conceptual Maturity IT is an abstract discipline manipulating abstract concepts. In IT, it is always possible to \"make the code work\", even if the cost and the delay to do that are unacceptable. The problem of IT is that you can pay the price of bad choices years (even decades) after you did the investment - and you pay each year, possibly more and more year after year for the exact same modification. Bad IT choices imply increasing the technical debt and jeopardize the capability of achieving future projects (at least you will continuously increase their cost). In that sense, IT is quite near from traditional industry. The fact is, in tradition industry, people are specialized in jobs: not every automotive worker will design the brakes or the engine. In IT, the situation is blurrier. Most IT people don't know what they are capable of doing, because specialization of jobs has not yet reached an industry agreement, and because it seems always possible to \"make the code work\" (despite of any consideration about quality/evolutions/money/timing/etc.). However, in IT, there are several levels of concerns that require different skills and different conceptual approaches. Over my 20 years in the IT business, I identified five levels in this conceptual world of software systems creation. Generally, in their professional life, people climb the rungs of the ladder in the same order, which makes the model a \"kind of\" maturity model. The Levels of Conceptual Maturity I will describe briefly the most important aspects of the various levels. I don't pretend to be exhaustive and I use more detailed materials and samples when I play the consultant for executives. Level 1: Development Most IT people know how to code, how to produce programs. For that purpose, IT people use programming languages (like Java, Cobol, C++, C, etc.). Behind the coding level, we can add the mastering of the development environment: development standards, debugging, versionning and branching management, build management, testing management, etc.. All those skills are crucial to good developments. Considering all that, if we rated people from 0 to 1 on this topic, I would say that most developers in the market are between 0.5 and 0.8 in development. Level 2: Design IT uses object orientation programming (OOP) commonly for more than two decades. However, OOP is conceptually complicated. To be explained to the non-IT people, design is the activity of building a set of programs structure and knowing where to put the code inside the program structure. Depending on how the programs are internally structured, you will face more or less troubles in the future maintenance and evolutions. Design activity uses lots of design patterns (level 2) which are generally badly mastered by IT teams because IT teams think too often at the \"code level \" (level 1). With years, and with Agile methodologies, IT people were pushed back to level 1. Design is complicated. IT people have to think before they code (very hard for developers); they have to forecast what will be the maintenance. Few people are really capable of good designs. Level 3: Application Architecture Architecting an application today is not easy, because they are a lot of trendy \"off-the-shelf\" components, a lot of competing \"sexy\" technologies. The hype around technologies is extreme and IT people can change frameworks very quickly to follow the last trend. Architecting an application is the skill to choose what components (reusable or not) will be used into an application. It enables the application to be developed correctly by several people, to isolate \"modules\", to manage customizable dependencies inside the IT systems. This is not an easy job. The objective is not just to realize software but to be able to maintain them on the long run and whatever their functional perimeter (that can become quite large with years). In order to build good application architecture, you have to think about \"component borders\" and be able to estimate the pros and cons of using a trendy component in the market. That's quite a hard job. Level 4: IT Architecture IT is everywhere and companies, even SMEs, have quite quickly a lot of IT systems. Being able to make those systems communicate together without being too hardly coupled, being able to make the IT systems as a whole evolving in a reasonable timing for reasonable costs, being able to communicate with the outside of the company in a safe manner, all that is part of IT architecture. In IT architecture, you also manage the positioning of features inside softwares: when a function is implemented in the wrong software, you can loose millions of dollars. In IT architecture problems are also the integration problems: a good IT architecture helps decreasing the QA costs and improving the TTM of the projects. Good IT architects are very rare in the market. It seems a pity because, for large companies, IT systems are one of the biggest problems, for instance in M&A phases. To add to the complexity, the IT market sells a lot of \"off-the-shelf solutions\" in that area, solutions that often bring confusion in IT systems, when put in place by non level 4 people. Level 5: Enterprise Architecture The term \"Enterprise Architecture\" (EA) is at the center of a lot of discussions because the industry is not clear about what an enterprise architect is supposed to do. I will provide my own pragmatic vision (which can be challenged). An enterprise architect must primarily ensure that company strategic intentions are concretely declined into the IT systems through auditable projects and programs. Secondly he/she must ensure that the changes brought to the softwares can be managed by end users, that software changes are not destroying efficient business processes but improving them. This is quite a work that requires a lot of IT skills but also a good business understanding and enhanced communication skills. The Usual Maturity Path One common path for IT-related designers or architects is to go from level 1 to level 5. Each level is requiring, following my experience, in average, at least 2 years of real life practice. Most often, when you work at a certain level, you have to consider also the impacts of what you are doing in all other lowest levels (that's why enterprise architects that never coded often make me uncomfortable, because they cannot imagine the consequences of their decisions). Most people will find happiness in a certain layer and will excel in that layer because it fits their mind and taste and it is perfectly adapted to the level of complexity they like to manage. Without a proper project experience and without training and/or coaching, it is quite hard for people to change levels without having to work for months in real life at the concerned level. A Maturity Model Related to Complexity The model describes the capability to solve IT problems at the proper level, to manage the complexity of the IT problem with the proper approach. For instance, you can take a level 4 problem with a level 1 approach but you have 70% of chances of never solving the problem completely and 95% of chances to spend millions of dollars fixing \"things that don't work\". The \"things that don't work\" were created by people because of their problem solving approach (level 1 instead of level 4). Using the proper approach would never have generated those costs and troubles. What is the Required Level of Maturity for my Project? Regarding this maturity model, executives should appreciate the level of maturity of their internal IT organization. They are then able to ask themselves the question: what's the adequate level of maturity required to achieve the project I want? Or, we can ask the question the other way round: what projects can I safely achieve considering the maturity of my internal IT organization? Those questions are crucial. When I look at failed projects, I often see people having a maturity level between 1 and 3 trying to master a multi-million dollars IT projects requiring a level 5. Integrating external consultants can do the job but not in all cases. If your business is complex and quite specific, it is preferable to bet on internal skills in the long term for IT people to understand your business in depth. Moreover, who chooses the casting of your external consultants? Your internal IT organization, and few organizations are able to hire consultants that know more than they do. This is exactly what happened when big companies got out of the mainframe. Mainframe teams were mostly level 1, some of them were level 2 (level 3 is not relevant in old mainframe application design and level 4 is often just about producing files for the outside or consuming them). Giving such a team an IT program with distributed applications in various technologies to build is doomed to failure, whatever the money and whatever the quality and dedication of people. For sure, with time and the appropriate training, most people can climb rungs in the ladder. But without time and training, people will keep on reproducing failures. Suppose the management is not aware of the core problem and people get fired for project failure, you can end up taking big outsourcing decisions based on a situation that nobody really understood. IT Industry Should Target Maturity Because it is only \"soft\"-ware and not hardware, the IT industry seems to like the level 1 so much that every serious attempt to raise to level 2 or 3 is pushed back to level 1 by the IT providers. IT providers love to focus their customers on the \"code level\": because they cannot see the bigger picture and buy their products even if they don't need them. If the IT industry created models for IT project management (like PMP), the increasing complexity of existing systems and new projects require the appropriate level of maturity in the project designers/architects teams. The problem today, in most companies, is not only to \"code\" nor only to \"manage projects\": it is to also to maintain the running systems and make the huge amount of legacy applications evolve without risks and conforming to the company strategic intentions. Most IT systems are a pile of geological layers of outdated technological hypes. The code of today becomes the legacy of tomorrow. If some industries (like aviation) forecast their investments for decades, IT should begin to do the same, to work on the adequate long-term skills that are required to ensure durable and quality IT services. IT industry should target maturity and maturity comes with the identification of our level of mastering of IT complexity. ( April 2015 )","title":"The Five Levels of Conceptual Maturity for IT Teams"},{"location":"articles/five-levels/#the-five-levels-of-conceptual-maturity-for-it-teams","text":"IT is a strange activity because it is an industry that has troubles getting out of a craft industry approach. Constantly moving to new exciting technologies and constantly influenced by massive technology centered marketing, IT people stay durably too technical and have recurring troubles to achieve IT projects.","title":"The Five Levels of Conceptual Maturity for IT Teams"},{"location":"articles/five-levels/#history-of-the-model","text":"Years ago, I was consulting a CEO and was trying to explain to him why his teams were failing big IT projects. A lot of executives today are digitally aware in their daily business practices but, when it comes to organizing new IT projects inside their company, they frequently discover that there is a huge gap between what they wanted to achieve and what is really done. Money is not the problem, and many big companies invest massive amounts of money in IT projects without getting the most of it. The fact is, when you invest too much in an IT project, your chances of success decrease drastically. I came to the CEO with this maturity model. I don't proclaim it to be perfect, far from it: it's just a representation of the problem. But the model is quite useful to explain and diagnose the capability of teams to perform IT projects and the levels of complexity attached to those projects.","title":"History of the Model"},{"location":"articles/five-levels/#a-model-of-conceptual-maturity","text":"IT is an abstract discipline manipulating abstract concepts. In IT, it is always possible to \"make the code work\", even if the cost and the delay to do that are unacceptable. The problem of IT is that you can pay the price of bad choices years (even decades) after you did the investment - and you pay each year, possibly more and more year after year for the exact same modification. Bad IT choices imply increasing the technical debt and jeopardize the capability of achieving future projects (at least you will continuously increase their cost). In that sense, IT is quite near from traditional industry. The fact is, in tradition industry, people are specialized in jobs: not every automotive worker will design the brakes or the engine. In IT, the situation is blurrier. Most IT people don't know what they are capable of doing, because specialization of jobs has not yet reached an industry agreement, and because it seems always possible to \"make the code work\" (despite of any consideration about quality/evolutions/money/timing/etc.). However, in IT, there are several levels of concerns that require different skills and different conceptual approaches. Over my 20 years in the IT business, I identified five levels in this conceptual world of software systems creation. Generally, in their professional life, people climb the rungs of the ladder in the same order, which makes the model a \"kind of\" maturity model.","title":"A Model of Conceptual Maturity"},{"location":"articles/five-levels/#the-levels-of-conceptual-maturity","text":"I will describe briefly the most important aspects of the various levels. I don't pretend to be exhaustive and I use more detailed materials and samples when I play the consultant for executives.","title":"The Levels of Conceptual Maturity"},{"location":"articles/five-levels/#level-1-development","text":"Most IT people know how to code, how to produce programs. For that purpose, IT people use programming languages (like Java, Cobol, C++, C, etc.). Behind the coding level, we can add the mastering of the development environment: development standards, debugging, versionning and branching management, build management, testing management, etc.. All those skills are crucial to good developments. Considering all that, if we rated people from 0 to 1 on this topic, I would say that most developers in the market are between 0.5 and 0.8 in development.","title":"Level 1: Development"},{"location":"articles/five-levels/#level-2-design","text":"IT uses object orientation programming (OOP) commonly for more than two decades. However, OOP is conceptually complicated. To be explained to the non-IT people, design is the activity of building a set of programs structure and knowing where to put the code inside the program structure. Depending on how the programs are internally structured, you will face more or less troubles in the future maintenance and evolutions. Design activity uses lots of design patterns (level 2) which are generally badly mastered by IT teams because IT teams think too often at the \"code level \" (level 1). With years, and with Agile methodologies, IT people were pushed back to level 1. Design is complicated. IT people have to think before they code (very hard for developers); they have to forecast what will be the maintenance. Few people are really capable of good designs.","title":"Level 2: Design"},{"location":"articles/five-levels/#level-3-application-architecture","text":"Architecting an application today is not easy, because they are a lot of trendy \"off-the-shelf\" components, a lot of competing \"sexy\" technologies. The hype around technologies is extreme and IT people can change frameworks very quickly to follow the last trend. Architecting an application is the skill to choose what components (reusable or not) will be used into an application. It enables the application to be developed correctly by several people, to isolate \"modules\", to manage customizable dependencies inside the IT systems. This is not an easy job. The objective is not just to realize software but to be able to maintain them on the long run and whatever their functional perimeter (that can become quite large with years). In order to build good application architecture, you have to think about \"component borders\" and be able to estimate the pros and cons of using a trendy component in the market. That's quite a hard job.","title":"Level 3: Application Architecture"},{"location":"articles/five-levels/#level-4-it-architecture","text":"IT is everywhere and companies, even SMEs, have quite quickly a lot of IT systems. Being able to make those systems communicate together without being too hardly coupled, being able to make the IT systems as a whole evolving in a reasonable timing for reasonable costs, being able to communicate with the outside of the company in a safe manner, all that is part of IT architecture. In IT architecture, you also manage the positioning of features inside softwares: when a function is implemented in the wrong software, you can loose millions of dollars. In IT architecture problems are also the integration problems: a good IT architecture helps decreasing the QA costs and improving the TTM of the projects. Good IT architects are very rare in the market. It seems a pity because, for large companies, IT systems are one of the biggest problems, for instance in M&A phases. To add to the complexity, the IT market sells a lot of \"off-the-shelf solutions\" in that area, solutions that often bring confusion in IT systems, when put in place by non level 4 people.","title":"Level 4: IT Architecture"},{"location":"articles/five-levels/#level-5-enterprise-architecture","text":"The term \"Enterprise Architecture\" (EA) is at the center of a lot of discussions because the industry is not clear about what an enterprise architect is supposed to do. I will provide my own pragmatic vision (which can be challenged). An enterprise architect must primarily ensure that company strategic intentions are concretely declined into the IT systems through auditable projects and programs. Secondly he/she must ensure that the changes brought to the softwares can be managed by end users, that software changes are not destroying efficient business processes but improving them. This is quite a work that requires a lot of IT skills but also a good business understanding and enhanced communication skills.","title":"Level 5: Enterprise Architecture"},{"location":"articles/five-levels/#the-usual-maturity-path","text":"One common path for IT-related designers or architects is to go from level 1 to level 5. Each level is requiring, following my experience, in average, at least 2 years of real life practice. Most often, when you work at a certain level, you have to consider also the impacts of what you are doing in all other lowest levels (that's why enterprise architects that never coded often make me uncomfortable, because they cannot imagine the consequences of their decisions). Most people will find happiness in a certain layer and will excel in that layer because it fits their mind and taste and it is perfectly adapted to the level of complexity they like to manage. Without a proper project experience and without training and/or coaching, it is quite hard for people to change levels without having to work for months in real life at the concerned level.","title":"The Usual Maturity Path"},{"location":"articles/five-levels/#a-maturity-model-related-to-complexity","text":"The model describes the capability to solve IT problems at the proper level, to manage the complexity of the IT problem with the proper approach. For instance, you can take a level 4 problem with a level 1 approach but you have 70% of chances of never solving the problem completely and 95% of chances to spend millions of dollars fixing \"things that don't work\". The \"things that don't work\" were created by people because of their problem solving approach (level 1 instead of level 4). Using the proper approach would never have generated those costs and troubles.","title":"A Maturity Model Related to Complexity"},{"location":"articles/five-levels/#what-is-the-required-level-of-maturity-for-my-project","text":"Regarding this maturity model, executives should appreciate the level of maturity of their internal IT organization. They are then able to ask themselves the question: what's the adequate level of maturity required to achieve the project I want? Or, we can ask the question the other way round: what projects can I safely achieve considering the maturity of my internal IT organization? Those questions are crucial. When I look at failed projects, I often see people having a maturity level between 1 and 3 trying to master a multi-million dollars IT projects requiring a level 5. Integrating external consultants can do the job but not in all cases. If your business is complex and quite specific, it is preferable to bet on internal skills in the long term for IT people to understand your business in depth. Moreover, who chooses the casting of your external consultants? Your internal IT organization, and few organizations are able to hire consultants that know more than they do. This is exactly what happened when big companies got out of the mainframe. Mainframe teams were mostly level 1, some of them were level 2 (level 3 is not relevant in old mainframe application design and level 4 is often just about producing files for the outside or consuming them). Giving such a team an IT program with distributed applications in various technologies to build is doomed to failure, whatever the money and whatever the quality and dedication of people. For sure, with time and the appropriate training, most people can climb rungs in the ladder. But without time and training, people will keep on reproducing failures. Suppose the management is not aware of the core problem and people get fired for project failure, you can end up taking big outsourcing decisions based on a situation that nobody really understood.","title":"What is the Required Level of Maturity for my Project?"},{"location":"articles/five-levels/#it-industry-should-target-maturity","text":"Because it is only \"soft\"-ware and not hardware, the IT industry seems to like the level 1 so much that every serious attempt to raise to level 2 or 3 is pushed back to level 1 by the IT providers. IT providers love to focus their customers on the \"code level\": because they cannot see the bigger picture and buy their products even if they don't need them. If the IT industry created models for IT project management (like PMP), the increasing complexity of existing systems and new projects require the appropriate level of maturity in the project designers/architects teams. The problem today, in most companies, is not only to \"code\" nor only to \"manage projects\": it is to also to maintain the running systems and make the huge amount of legacy applications evolve without risks and conforming to the company strategic intentions. Most IT systems are a pile of geological layers of outdated technological hypes. The code of today becomes the legacy of tomorrow. If some industries (like aviation) forecast their investments for decades, IT should begin to do the same, to work on the adequate long-term skills that are required to ensure durable and quality IT services. IT industry should target maturity and maturity comes with the identification of our level of mastering of IT complexity. ( April 2015 )","title":"IT Industry Should Target Maturity"},{"location":"articles/graphql-web-services/","text":"GraphQL And Classic Web Services Facebook produced the GraphQL specification in order to be an alternative to Rest. We already explained what we thought of Rest services . GraphQL appears to us as a new way to do the same thing that what the industry is doing for decades. This article is an analysis of GraphQL principles, their consequences and a comparison with traditional ways of doing web services. The Basics of Services Various Standards, Same Spirit The underlying basis of services is RPC (Remote Procedure Call, see the dedicated article on the subject ). For decades, this concept is reinvented again and again with various flavors. Apart from RPC protocols or proprietary protocols (like the Tuxedo ones), Edifact was for a long time an industry standard in some business domains like travel, banking, medical and logistics. Edifact was replaced by XML then by JSON . Every of those standards aimed to provide the same functionality: exchanging structured data between two different systems, probably implemented with different programming languages and/or paradigms, and having a client system benefiting from a service provided by a server system. Basic Requirements For a Service Oriented Language The first requirement is to have a \"verb\", meaning the name of the \"transaction\" we expect the remote system to process. The second requirement is to have a type system to be able to send structured data under the form of a tree of objects. This is generally achieved by proposing basic types (like string, integer, float, etc.) and the capability of defining fields that have those properties and that can be grouped into classes. Classes will generally have to indicate what fields are required and what fields are optional. This is generally done within the context of one particular transaction. Once classes are defined, they can be grouped in larger structures or other classes to define groups of object structures than define larger entities, most of the time, better suitable to express the business semantics. Optionally, those definitions are expressed in \"grammars\" or \"schemas\". This is the case in RPC systems, in Edifact and in XML, and this is optional for JSON (even if the JSON schema specification is very usable). Note that, even if there is no explicit schema mechanism, the fact of defining objects and structures leads to the use of implicit schemas. The difference is really what part of the code is \"validating\" that the messages are well formed: With an explicit schema definition, we can generate the validation code and create a kind of serializer/unserializer layer (wrapper/unwrapper or proxy/stub layer); Without an explicit schema definition, the validation code is generally hand written. All those requirements are defined in the OSI model in the \"presentation layer\". Don't forget the protocol The last requirement is to have some kind of underlying protocol to: Send and receive messages between systems; Create a naming convention that enables to use the name of the service (or name of the \"verb\"); Find the network configuration that enable to route the request message to the proper entry point; Possibly associate the request and the response with identifiers; Possibly include information about the sender and its security credentials. Once again, the OSI model defines a certain split of responsibilities between all those requirements. Quickly explained: The transport layer (layer 4) is defining the addressing scheme and primitives that enables several systems to communicate together; The session layer (layer 5) enables to manage the sessions and conversations between the client and the server; this layer manages the succession of names messages that the two peers will exchange; The presentation layer (layer 6) manages the content of data, their structure and their values. In the current JSON world, JSON-RPC is presenting a very basic protocol that can manage the basic single statement and request response messages. GraphQL Answers To Service Oriented Requirements GraphQL seems to us as a new flavor of the same old story. It brings some interesting new stuff (and we will come back on that), but the important point is that we can implement traditional services with this language. The \"Verb\" Each GraphQL service is an named http entry point. The get method will be used to access the query part of the service and the post method will access the mutation part of it. This looks like Rest but this is rather different, because the idea seems not to access to a single resource (even if it is possible), but to perform a service. The Type System GraphQL proposes an extended type system that proposes: Basic types, Classes, Groups of classes, Fragments (kind of filters on classes), Queries. With all the available semantics, it is very easy to implement all the existing web services that are used currently in the business. Moreover, it is possible to have a richer interface on existing services and a more adaptative way of performing transactions. This seems rather promising for mobile or chatbot accesses for instance. Those two UIs manipulate a small amount of data and may require some server services to adapt their reply to their particular use. For sure, we could argue saying: with standard web services in JSON, as a client, we can always extract the subset of the response that is interesting to us. The fact is this feature goes beyond the subset approach and enables to ask the server to adapt the contents of its response to the client. And The Protocol? In the Web, the protocol is http and the approach seems inspired by both Rest and Ajax kind of calls. A Promising Standard An Intermediate Between JSON-RPC and Rest? GraphQL seems to target the intermediate path between JSON-RPC (which is a simplified SOAP) and Rest. The fact of publishing a schema enables both approaches: Perform elaborated services (like before); Access business objects (like the Rest or CRUD approach). GraphQL seems to propose a way to benefit from both approaches. The fact of being able to perform complex queries (with a lot of flavours) also adds intelligence to the protocol, an intelligence driven by the client. But What Impact On The Server Design? This seems very interesting but the problem caused by this new protocol is located on the backend part. Some questions must be answered: What kind of models and/or application this protocol is targeting? All applications? Applications that should be able to disclose a part of their model? Do we know the extend of the server impacts in terms of design? Model Your Business Domain As A Graph We are entering here into delicate topics. For those how already performed some semantic modeling with RDF , RDFS or OWL , I think you're convinced about the fact that we can model a business domain with a graph. For those who already used graph databases such as Neo or Orient in Big Data use cases or fraud recognition or social applications, may be convinced that a business model can be modeled as a graph. But for the vast majority of IT people, this assertion is not obvious. Well, we'll come back on this particular topic in future articles, so we'll take for granted that a business domain can be modeled with a graph. Graph Data Means Graph Database What Facebook is proposing us is a protocol that enables to query the internal structure of the graph of data. So, the first consequences seem to be: Your data are modeled as a graph, You probably use a graph database of some sort. Consequently, your GraphQL schema is more or less the same thing as your graph database schema (when this sort of things exists such as in OrientDB ). Open Heart Data Model The second consequence is also double: You published your graph business model to your clients; You use your database model as the presentation layer. This second point is very problematic. Indeed, it explains why the protocol is proposing so much complexity in some of its part: because it is supposed to be, at the same time, a protocol and a graph query language (like Cypher ). In a certain sense, it is supposed to be the new SQL*Net or ODBC but for graphs. And this is where the approach is questionable. The Myth Of The Single Representation Of Reality There are many myths in IT, and this one is a big one. Many people believe that there is only one way of representing the reality-theirs, indeed. And the reality is, they are wrong. Note that, throughout the history of science, many great scientists had the same debate as the intuitionist debate of the beginning of the 20th Century . Any software designer that worked sufficiently in the domain should have seen several facts: Whatever the efforts and the design patterns used, you cannot predict how the business will evolve; Two designer confronted to the same problem will produce two different designs. Indeed, design is very subjective and, for a lot of reasons we won't explain here, it is not a bad thing that there is no unique model for a particular business domain, on the contrary. When Client and Server Share Everything GraphQL is proposing a way for the customer to be very near from its server, so near that its data model is perfectly known, right from the client. This can be very useful when you work with yourself, when you develop your own client and server in a kind of symbiosis. If you are on an Internet application, you also must know that you core model will be known from the outside, which can be a problems in terms of IP and in terms of security. So: GraphQL is OK if your API is internal ; GraphQL seems not OK if your API is external. Publishing a GraphQL API Well, if you want to publish a GraphQL API, you have to consider several things: You will impose to your client the graph model of your business domain, and maybe it is relevant and maybe not (see next part); You will disclose a graph representation of your business model, which is not the case in JSON-RPC where you only disclose the interoperability formats ; This can represent a potential threat on your IT because your software design is very often at the heart of your business value; You will have to have a complex security system, which is the security system of the graph exploration, and this will not be obvious to address; You will be bound to implement the protocol complexity that can open more security issues in your software. For sure, there are cases where all those arguments may be irrelevant: You can work in a domain where the graph model has no value or is very well known (for instance the social media); You can work for non profit organizations; You can have a system that will not cause any loss of money, loss of IP or loss of value if hacked. The Fundamental Principle Of Interoperability In complement to the article on REST , we will explain the fundamental principle of interoperability. The context of interoperability is the following: Systems communicate when they have some interest in doing so: interoperability is business-oriented. To establish system interconnection costs money and time, this in the project phase but also in the recurring phase: to realize those investments, the client and the server generally have a good business reason. When a server is publishing an API, it cannot imagine the list of clients he will have tomorrow, nor make assumptions on their programming language or even design approach. In this context, the fundamental principle of interoperability is that the client and the server contracting the exchange should define the common format that is proposing the less semantic constraints possible on both ends . Because the client and the server don't have to commit on their internal structure and functional aspects, the interchange is described with a semantic that must be the simplest possible and that can enable the client and server to define different modeling of the same \"exchanged data model\". The figure above shows the core problem of interoperability: The client and the server have no reason to model the interchange data the same way. Indeed, they don't. The consequence is that they must adapt their internal formats (memory or database) to the network presentation. The way they store the data is their problem, and their storage representation can be even quite different from their memory representation (that's why, generally, people use a data access layer to adapt their business objects to the database). Very commonly, in the industry, we have 3 representations of the data: the client's, the server's and the network presentation. Those adaptation are painful to develop, for sure, but they are the conditions of isolation, and who says isolation says separation of concerns. This is because the client and the server are different software parts that there is no need to impose more constraints. In this context, we do not recommend using GraphQL for an external business domain interface, but we can recommend it if the application architecture needs a kinf of ODBC or SQL*Net directly from the UI, and in a context where the API is not made to be exposed to unknown third parties. Note that we did not really enter into the core details of \"how do we really build a graph-oriented business application\"? This will come in later articles. A Correct Intuition? If we try to step back one minute, we can say that the Facebook team may have had an intuition that, with the graph-oriented modeling approach, this fundamental principle or interoperability could be declared obsolete (and consequently graphs could be in the client, in the server, in the database, with exactly the same structure). In some cases, that's true: if you master both ends of the wire, that's probably one very efficient way to do it. But it looks like ODBC or SQL*Net anyway. One thing is sure, the graph-oriented modeling of business domains will revolution the IT world, but perhaps not the way the Facebook team imagined it. We'll come back on that in the future. Conclusion GraphQL is a very interesting attempt to propose a middle term between REST and JSON-RPC, as the following diagram is showing it. However, this diagram is very misleading because the 3 inbound protocols have many different impacts on the server design, contrary to what's presented. Indeed, both REST and GraphQL imply a very specific programming model: REST is imposing a hard resource orientation that is unnatural to business applications (see here ), GraphQL proposes a graph-oriented ODBC-like protocol that will have the tendency to tie-up strongly the client and the server. The conclusion is it seems to us that, so far, only RPC enables to design and deliver reliable and secure business applications, and to do it the way you want. RPC defines a contract that can lead to many various programming paradigms, which is not the case for REST or for GraphQL. The GraphQL has, however, opened publicly the case of graph-orientation in the design of business applications. This area is really a core game changer for the IT business and it will be a topic described and explained in the future in this site. See also About Rest ( December 2017 )","title":"GraphQL And Classic Web Services"},{"location":"articles/graphql-web-services/#graphql-and-classic-web-services","text":"Facebook produced the GraphQL specification in order to be an alternative to Rest. We already explained what we thought of Rest services . GraphQL appears to us as a new way to do the same thing that what the industry is doing for decades. This article is an analysis of GraphQL principles, their consequences and a comparison with traditional ways of doing web services.","title":"GraphQL And Classic Web Services"},{"location":"articles/graphql-web-services/#the-basics-of-services","text":"","title":"The Basics of Services"},{"location":"articles/graphql-web-services/#various-standards-same-spirit","text":"The underlying basis of services is RPC (Remote Procedure Call, see the dedicated article on the subject ). For decades, this concept is reinvented again and again with various flavors. Apart from RPC protocols or proprietary protocols (like the Tuxedo ones), Edifact was for a long time an industry standard in some business domains like travel, banking, medical and logistics. Edifact was replaced by XML then by JSON . Every of those standards aimed to provide the same functionality: exchanging structured data between two different systems, probably implemented with different programming languages and/or paradigms, and having a client system benefiting from a service provided by a server system.","title":"Various Standards, Same Spirit"},{"location":"articles/graphql-web-services/#basic-requirements-for-a-service-oriented-language","text":"The first requirement is to have a \"verb\", meaning the name of the \"transaction\" we expect the remote system to process. The second requirement is to have a type system to be able to send structured data under the form of a tree of objects. This is generally achieved by proposing basic types (like string, integer, float, etc.) and the capability of defining fields that have those properties and that can be grouped into classes. Classes will generally have to indicate what fields are required and what fields are optional. This is generally done within the context of one particular transaction. Once classes are defined, they can be grouped in larger structures or other classes to define groups of object structures than define larger entities, most of the time, better suitable to express the business semantics. Optionally, those definitions are expressed in \"grammars\" or \"schemas\". This is the case in RPC systems, in Edifact and in XML, and this is optional for JSON (even if the JSON schema specification is very usable). Note that, even if there is no explicit schema mechanism, the fact of defining objects and structures leads to the use of implicit schemas. The difference is really what part of the code is \"validating\" that the messages are well formed: With an explicit schema definition, we can generate the validation code and create a kind of serializer/unserializer layer (wrapper/unwrapper or proxy/stub layer); Without an explicit schema definition, the validation code is generally hand written. All those requirements are defined in the OSI model in the \"presentation layer\".","title":"Basic Requirements For a Service Oriented Language"},{"location":"articles/graphql-web-services/#dont-forget-the-protocol","text":"The last requirement is to have some kind of underlying protocol to: Send and receive messages between systems; Create a naming convention that enables to use the name of the service (or name of the \"verb\"); Find the network configuration that enable to route the request message to the proper entry point; Possibly associate the request and the response with identifiers; Possibly include information about the sender and its security credentials. Once again, the OSI model defines a certain split of responsibilities between all those requirements. Quickly explained: The transport layer (layer 4) is defining the addressing scheme and primitives that enables several systems to communicate together; The session layer (layer 5) enables to manage the sessions and conversations between the client and the server; this layer manages the succession of names messages that the two peers will exchange; The presentation layer (layer 6) manages the content of data, their structure and their values. In the current JSON world, JSON-RPC is presenting a very basic protocol that can manage the basic single statement and request response messages.","title":"Don't forget the protocol"},{"location":"articles/graphql-web-services/#graphql-answers-to-service-oriented-requirements","text":"GraphQL seems to us as a new flavor of the same old story. It brings some interesting new stuff (and we will come back on that), but the important point is that we can implement traditional services with this language.","title":"GraphQL Answers To Service Oriented Requirements"},{"location":"articles/graphql-web-services/#the-verb","text":"Each GraphQL service is an named http entry point. The get method will be used to access the query part of the service and the post method will access the mutation part of it. This looks like Rest but this is rather different, because the idea seems not to access to a single resource (even if it is possible), but to perform a service.","title":"The \"Verb\""},{"location":"articles/graphql-web-services/#the-type-system","text":"GraphQL proposes an extended type system that proposes: Basic types, Classes, Groups of classes, Fragments (kind of filters on classes), Queries. With all the available semantics, it is very easy to implement all the existing web services that are used currently in the business. Moreover, it is possible to have a richer interface on existing services and a more adaptative way of performing transactions. This seems rather promising for mobile or chatbot accesses for instance. Those two UIs manipulate a small amount of data and may require some server services to adapt their reply to their particular use. For sure, we could argue saying: with standard web services in JSON, as a client, we can always extract the subset of the response that is interesting to us. The fact is this feature goes beyond the subset approach and enables to ask the server to adapt the contents of its response to the client.","title":"The Type System"},{"location":"articles/graphql-web-services/#and-the-protocol","text":"In the Web, the protocol is http and the approach seems inspired by both Rest and Ajax kind of calls.","title":"And The Protocol?"},{"location":"articles/graphql-web-services/#a-promising-standard","text":"","title":"A Promising Standard"},{"location":"articles/graphql-web-services/#an-intermediate-between-json-rpc-and-rest","text":"GraphQL seems to target the intermediate path between JSON-RPC (which is a simplified SOAP) and Rest. The fact of publishing a schema enables both approaches: Perform elaborated services (like before); Access business objects (like the Rest or CRUD approach). GraphQL seems to propose a way to benefit from both approaches. The fact of being able to perform complex queries (with a lot of flavours) also adds intelligence to the protocol, an intelligence driven by the client.","title":"An Intermediate Between JSON-RPC and Rest?"},{"location":"articles/graphql-web-services/#but-what-impact-on-the-server-design","text":"This seems very interesting but the problem caused by this new protocol is located on the backend part. Some questions must be answered: What kind of models and/or application this protocol is targeting? All applications? Applications that should be able to disclose a part of their model? Do we know the extend of the server impacts in terms of design?","title":"But What Impact On The Server Design?"},{"location":"articles/graphql-web-services/#model-your-business-domain-as-a-graph","text":"We are entering here into delicate topics. For those how already performed some semantic modeling with RDF , RDFS or OWL , I think you're convinced about the fact that we can model a business domain with a graph. For those who already used graph databases such as Neo or Orient in Big Data use cases or fraud recognition or social applications, may be convinced that a business model can be modeled as a graph. But for the vast majority of IT people, this assertion is not obvious. Well, we'll come back on this particular topic in future articles, so we'll take for granted that a business domain can be modeled with a graph.","title":"Model Your Business Domain As A Graph"},{"location":"articles/graphql-web-services/#graph-data-means-graph-database","text":"What Facebook is proposing us is a protocol that enables to query the internal structure of the graph of data. So, the first consequences seem to be: Your data are modeled as a graph, You probably use a graph database of some sort. Consequently, your GraphQL schema is more or less the same thing as your graph database schema (when this sort of things exists such as in OrientDB ).","title":"Graph Data Means Graph Database"},{"location":"articles/graphql-web-services/#open-heart-data-model","text":"The second consequence is also double: You published your graph business model to your clients; You use your database model as the presentation layer. This second point is very problematic. Indeed, it explains why the protocol is proposing so much complexity in some of its part: because it is supposed to be, at the same time, a protocol and a graph query language (like Cypher ). In a certain sense, it is supposed to be the new SQL*Net or ODBC but for graphs. And this is where the approach is questionable.","title":"Open Heart Data Model"},{"location":"articles/graphql-web-services/#the-myth-of-the-single-representation-of-reality","text":"There are many myths in IT, and this one is a big one. Many people believe that there is only one way of representing the reality-theirs, indeed. And the reality is, they are wrong. Note that, throughout the history of science, many great scientists had the same debate as the intuitionist debate of the beginning of the 20th Century . Any software designer that worked sufficiently in the domain should have seen several facts: Whatever the efforts and the design patterns used, you cannot predict how the business will evolve; Two designer confronted to the same problem will produce two different designs. Indeed, design is very subjective and, for a lot of reasons we won't explain here, it is not a bad thing that there is no unique model for a particular business domain, on the contrary.","title":"The Myth Of The Single Representation Of Reality"},{"location":"articles/graphql-web-services/#when-client-and-server-share-everything","text":"GraphQL is proposing a way for the customer to be very near from its server, so near that its data model is perfectly known, right from the client. This can be very useful when you work with yourself, when you develop your own client and server in a kind of symbiosis. If you are on an Internet application, you also must know that you core model will be known from the outside, which can be a problems in terms of IP and in terms of security. So: GraphQL is OK if your API is internal ; GraphQL seems not OK if your API is external.","title":"When Client and Server Share Everything"},{"location":"articles/graphql-web-services/#publishing-a-graphql-api","text":"Well, if you want to publish a GraphQL API, you have to consider several things: You will impose to your client the graph model of your business domain, and maybe it is relevant and maybe not (see next part); You will disclose a graph representation of your business model, which is not the case in JSON-RPC where you only disclose the interoperability formats ; This can represent a potential threat on your IT because your software design is very often at the heart of your business value; You will have to have a complex security system, which is the security system of the graph exploration, and this will not be obvious to address; You will be bound to implement the protocol complexity that can open more security issues in your software. For sure, there are cases where all those arguments may be irrelevant: You can work in a domain where the graph model has no value or is very well known (for instance the social media); You can work for non profit organizations; You can have a system that will not cause any loss of money, loss of IP or loss of value if hacked.","title":"Publishing a GraphQL API"},{"location":"articles/graphql-web-services/#the-fundamental-principle-of-interoperability","text":"In complement to the article on REST , we will explain the fundamental principle of interoperability. The context of interoperability is the following: Systems communicate when they have some interest in doing so: interoperability is business-oriented. To establish system interconnection costs money and time, this in the project phase but also in the recurring phase: to realize those investments, the client and the server generally have a good business reason. When a server is publishing an API, it cannot imagine the list of clients he will have tomorrow, nor make assumptions on their programming language or even design approach. In this context, the fundamental principle of interoperability is that the client and the server contracting the exchange should define the common format that is proposing the less semantic constraints possible on both ends . Because the client and the server don't have to commit on their internal structure and functional aspects, the interchange is described with a semantic that must be the simplest possible and that can enable the client and server to define different modeling of the same \"exchanged data model\". The figure above shows the core problem of interoperability: The client and the server have no reason to model the interchange data the same way. Indeed, they don't. The consequence is that they must adapt their internal formats (memory or database) to the network presentation. The way they store the data is their problem, and their storage representation can be even quite different from their memory representation (that's why, generally, people use a data access layer to adapt their business objects to the database). Very commonly, in the industry, we have 3 representations of the data: the client's, the server's and the network presentation. Those adaptation are painful to develop, for sure, but they are the conditions of isolation, and who says isolation says separation of concerns. This is because the client and the server are different software parts that there is no need to impose more constraints. In this context, we do not recommend using GraphQL for an external business domain interface, but we can recommend it if the application architecture needs a kinf of ODBC or SQL*Net directly from the UI, and in a context where the API is not made to be exposed to unknown third parties. Note that we did not really enter into the core details of \"how do we really build a graph-oriented business application\"? This will come in later articles.","title":"The Fundamental Principle Of Interoperability"},{"location":"articles/graphql-web-services/#a-correct-intuition","text":"If we try to step back one minute, we can say that the Facebook team may have had an intuition that, with the graph-oriented modeling approach, this fundamental principle or interoperability could be declared obsolete (and consequently graphs could be in the client, in the server, in the database, with exactly the same structure). In some cases, that's true: if you master both ends of the wire, that's probably one very efficient way to do it. But it looks like ODBC or SQL*Net anyway. One thing is sure, the graph-oriented modeling of business domains will revolution the IT world, but perhaps not the way the Facebook team imagined it. We'll come back on that in the future.","title":"A Correct Intuition?"},{"location":"articles/graphql-web-services/#conclusion","text":"GraphQL is a very interesting attempt to propose a middle term between REST and JSON-RPC, as the following diagram is showing it. However, this diagram is very misleading because the 3 inbound protocols have many different impacts on the server design, contrary to what's presented. Indeed, both REST and GraphQL imply a very specific programming model: REST is imposing a hard resource orientation that is unnatural to business applications (see here ), GraphQL proposes a graph-oriented ODBC-like protocol that will have the tendency to tie-up strongly the client and the server. The conclusion is it seems to us that, so far, only RPC enables to design and deliver reliable and secure business applications, and to do it the way you want. RPC defines a contract that can lead to many various programming paradigms, which is not the case for REST or for GraphQL. The GraphQL has, however, opened publicly the case of graph-orientation in the design of business applications. This area is really a core game changer for the IT business and it will be a topic described and explained in the future in this site.","title":"Conclusion"},{"location":"articles/graphql-web-services/#see-also","text":"About Rest ( December 2017 )","title":"See also"},{"location":"articles/mbse-vs-ea/","text":"Military frameworks, systems engineering and enterprise architecture (Courtesy of Pixaby on https://www.pexels.com) In the world of architecture modeling, the military organizations like the US Department of Defense (DoD), the UK Ministry of Defence (MoD) or the NATO organization created enterprise architecture frameworks adapted to their needs. On the market today, 3 main frameworks are existing and used in military projects: DoDAF , the US MoD Architecture Framework, that was historically the first military architecture framework ever (it is originated in the 90s); MoDAF , the UK MoD architecture framework, which is both an adaptation and an abstraction of DoDAF; NAF , the NATO Architecture Framework that is also an adaptation and an abstraction of DoDAF and MoDAF. Objectives and dimensions of the military frameworks The main objective searched by military organization is to be able to master a global project of several billions of dollars/euros from the design phase to the support and operational phase. Because those projects/programs can be very large and very complex, the idea was that everyone (military organization and supplier organizations) share the same set of \"views\" on the program in order to share understanding of what was to be done, how requirements were taken into account, what is the full lifecycle of the product and how the project should be managed. Table 1: Structure of military requirements # Dimension Details 1 Product definition Can be an aircraft, weapon, complex system, etc. and all the related IT systems to manage them. The objective is to use a systems engineering approach (model-based in that case), based on requirements. 2 Product support and operations The objective of this phase is to detail all the systems that will permit the support of the product. This is generally a wide topic including maintenance, parts procurement, etc. Moreover, the product use must be envisaged in operations preparation and in battle field in coordination with other systems and organizations. 3 Digital transformation The objective of this phase is to evaluate and deal with the impacts of the product and product support in the operational organization, processes and IT of the army. 4 Project plan The objective is to be able to describe the the plan to deliver the product conforming to the requirements (phases, milestones and so on), to permit project steering, and product support procurement. We can classify the military enterprise architecture frameworks as pursuing 4 objectives, as shown in table 1. Modeling domains Those 4 objectives are traditionally modeled with 3 different modeling techniques: Systems engineering for the product definition, Enterprise architecture for: Product support, Digital transformation, Project modeling tools for project management. Figure 1: The 3 domains Military framework concerns in the industrial V-model The figure 1 shows the 3 modeling modeling domains that they are traditionally covered in military frameworks. In terms of modeling, we could say that the core objective of the military frameworks is to have, in a single repository, the systems engineering (MBE/MBSE), the enterprise architecture and the project management gathered together . Positioning in the V-model Those 4 areas of concern can be mapped on an industrial V-model as shown in Figure 2. Figure 2: Military framework concerns in the industrial V-model The first dimension, product definition, is in the left part of the V and corresponds to the product architecture, from its operational conditions, requirements to its system design. The second dimension is corresponding to product support (we often speak about \"support concept\"). The two last dimensions are running all along the project, being at the project/program level or at the digital transformation level (integrate the new product into the current organizations). We can see in Figure 2 how an integrated modeling of those two highest branches of the V-model can be important for the military customer: It ensures a full lifecycle modeling of the product, It enables simulation scenarios linked to the requirement phase, It enables to anticipate the impact of the new project on army's existing organizations, It enables to manage the project/program. Most of all, on top of being a common modeling language, it enables complex tradeoffs, including performance, finance, supportability, etc. MBSE/MBE The RFLP process In manufacturing, the objective is to create products. The starting point of a product manufacturing is generally a set of requirements. To go from the requirements to the product is generally a long process involving the following phases: Requirements engineering : The objective of this phase is to work on requirements in order to make them non ambiguous, to sort them by importance and to remove inconsistency between them; In this phase, all the operational conditions of the product are described, which is a crucial phase of the process; Functional analysis : This phase objective is to determine the functions of the product, the information flows and dependencies between functions; Generally, the functions are worked at different levels of granularity (functions, sub-functions, sub-sub-functions and so on); Logical system analysis : Once the various functions at various levels are known, the assignment of functions to logical elements can be done; The objective of this phase is to create the appropriate logical systems to get the nearest possible from a possible industrial split of the realization of the product; When the logical analysis is finished, we have the detailed specification of each system ; Physical design : Once the product has been split on several components, several teams can work in parallel on component design (at the level of granularity of the component); The product architecture will ensure that all detailed components fit together conforming to the specifications. This view is, for sure, simplified and some industries (or specific businesses inside the industries like embedded software, electronics, or mechatronics) are often using more complex processes, with dedicated sub-processes for every discipline. However, seen from a high level standpoint, the various phases of the MBSE are the 4 we mentioned, leading some people to call the MBSE process a \"RFLP\" process (RFLP standing for Requirements, Functional, Logical, Physical). Currently, we find more and more people skipping the \"S\" of MBSE to talk about MBE, for model-based engineering. The term becomes larger because it does not have this embedded system connotation anymore. Maintaining the digital thread For sure, a RFLP process aims to create a digital \"thread\" or \"link\" between the various entities, at least between the requirements, the functions and the systems (plus all the connectivity semantics). The system entities are a solution matching the requirements, and so is their \"physical\" description; but this solution is the result of various levels of trade-off analysis. In other terms, solutions can (and will) evolve with time. The full digital thread of information will have to be maintained through changes. Changes in requirements (or operational conditions) will lead to changes in systems and changes in systems (like obsolescence) must be compliant with requirements. RFLP processes with many variable dimensions The RFLP processes that we can find in the industry are generally describing many parameters attached to the various elements (requirements, functions, systems, etc.): the performance, the price, the various criteria of manufacturability, the respect of regulation and standards are among the most important of parameters (the weight also in aerospace). Those parameters are crucial because, during the various design phases: They permit to use requirement quantification (with values and ranges) and not only requirement qualification; They enable to simulate the various elements working together (functional simulation, system simulation, or full product simulation in some known conditions); They enable complex trade-offs that lead to design the best product \"under a certain set of constraints\". Modeling in MBSE The topic is quite vast but we will, once again, try a simplification. Modeling can be done mainly with 3 different approaches: The first one is to do \"manual\" systems engineering with Office documents (Word and Excel mainly); This way of doing things is still very present in the industry; It is error prone, and very painful for the engineering teams; The second one is to use \"standard MBSE modeling language\" like SysML or Capella to run the \"RFL\" part of the MBSE process (the \"P\" part will be done with specific tools like 3D design tools or electricity engineering tools); The objective is to model graphically the \"RFL\" part of the process and to keep links with the \"P\" parts; The third one is to use a simulation/optimization-oriented domain-specific language (DSL, the domain being systems engineering) for requirements and systems like PMM or Form-L . In the first style of systems engineering, the digital thread is extremely difficult to maintain or trace, and generally, to be able to do it, a lot of manual and painful efforts are required. Most of the time, the requirements are vague, not measurable, and the trade-offs analysis are not recorded. With time, we see a growing consciousness of the advantages of the third solution to be able to do complex trade-offs very early in the product design phase (see the system engineering book of knowledge here ). MBE/MBSE seen from the military The MBE/MBSE modeling techniques are a way for military organizations to ensure that their requirements are correctly understood and are refined the right way. Especially, the requirements being tightly coupled with operational conditions (often called CONOPS for CONcept of OPerationS), the graphical modeling in a unified language is an enabler for common understanding, sharing of decisions in complex trade-offs and sharing of information. For sure, this is being true for the design phase of the system. About the complexity of modeling hybrid systems The fact is systems engineering is very adapted to design the early phases of a product and do the right structuring trade-offs. But the \"products\" of today are much more complex than the products of yesterday. If we take the sample of aerospace, the \"ground segment\", or the set of software that enable to manage the machines from the ground is much bigger and much more complex than before. Let us take a sample in a non military domain: the constellations of satellites. To be able to operate a constellation of more than 100 satellites in a crowded space, the ground segment complexity is very high. The thing is it is not easy to design the product (the satellite) as the set of software required to operate many instances of this very satellite. More and more military systems are facing the same kind of problems: they must tightly couple together the product definition and the product support design phases. How to ensure that all the requirements are taken in charge by all sub-systems in the global system, above all when the way of model management software systems is more related to IT techniques such as enterprise architecture, IT architecture or software architecture? Modeling enterprise architecture The enterprise architecture domain is very wide and our objective is not here to describe it extensively. We will just mention that this domain is mainly split in 2 concerns: What methodology should be used to create a good IT architecture serving the business process transformation according to the strategic intentions? What are the relevant views and artifacts to use to model the digital transformation, starting with the strategy, going through the business layer, the application layer and the infrastructure layer? There are many enterprise architecture methodology frameworks, but the most used nowadays is TOGAF . The most used enterprise architecture modeling language is Archimate (see our introductory article and our recommended recipes ). This language is particularly adapted to model most of the other dimensions of interest of the military project, especially the organizations, strategy, processes and IT systems, whatever the complexity. It proposes 13 basic viewpoints (see here ) but its global metamodel makes it adapted for many complex representations. Traditionally enterprise architecture is information systems oriented and aims at reintroducing the business common sense in IT. In order to tackle the strategic intentions of a company that wants to transform, process and IT transformation must be considered together as a system in order to avoid the 2 common traps: The biased process-driven project able to change the company processes but without any efficiency for the company (for instance by bringing more Excel spreadsheets, more blocking steps, multiple keying and bureaucracy than the company already has); The biased IT-driven project able to change the IT systems without being synchronized with new processes (and generating more troubles, data inconsistencies, process problems, double keying, etc.). Samples of those traps are numerous. For the first case, transformation projects led by process experts often lead to a huge loss of money because they create more Excel tools and often add layers of procedures to already overly complex environments. For the second case, a typical example is the \"tool project approach\": when users have a need, let's buy a tool and deploy it. Those two sample lead to the same result: inefficient, error prone and expensive processes, and inefficient, error prone and expensive IT systems. In one word: more recurring costs, less productivity and a future cost of change that increased. Enterprise architecture, by its way of considering the processes and IT systems together enables to perform the proper tradeoffs in projects and to plan a real digital transformation, meaning a process and IT systems joint transformation targeting a better efficiency (cost reduction or productivity enhancement). Various solutions for military projects Considering the military requirements of table 1 , let us dig into the various frameworks that enable to model military projects. The best of breed solutions If we look at the market and take the best of breed modeling solutions adapted to the requirements of table 1 , we could propose the solutions presented in Table 2. Table 2: Best of breed solutions for military programs # Dimension Solution 1 Product definition SysML systems engineering modeling language 2 Product support and operations Archimate enterprise architecture modeling language 3 Digital transformation Archimate enterprise architecture modeling language 4 Project plan Complex planning tools for programs (set of projects with dependencies) + Archimate with the implementation and migration package As we can see in the table 2, we have 3 ways of modeling the full program: SysML, Archimate and a planning tool. The 3 metamodels not being integrated together, we may have problems at the \"interfaces\" of each domain (see later in the article). SysML proposes 9 kinds of diagrams and Archimate 13 basic viewpoints, what corresponds to approximately 22 types of diagrams. If we had 2 or 3 viewpoints for planning, we would end up with a solution presenting around 25 viewpoints. The military frameworks DoDAF The military frameworks, for decades, defined a set of artifacts and views to have it all covered. The Figure 3 shows the structure of views of DoDAF version 2. Figure 3: DoDAF viewpoint structure (52 viewpoints) MoDAF The Figure 4 shows the structure of views of MoDAF. Figure 4: MoDAF viewpoint structure (46 viewpoints) NAF The Figure 5 shows the structure of views in NAF v4. Figure 5: NAF viewpoint structure (46 viewpoints) UAF An attempt of \"unification\" of those three frameworks was done by the OMG with the so-called Unified Architecture Framework ( UAF ). The Figure 6 shows the structure of views in UAF v1. Figure 6: UAF viewpoint structure (+70 viewpoints) Views and metamodel All frameworks propose a metamodel that is often very rich, and also very complex. With time, the metamodels got more abstract which makes their use quite difficult. Genericity, specialization and taxonomy The core advantage - and the core problem - of those frameworks is that they are generic . With the same semantics (or almost), it is possible to describe a project for a new military aircraft, or a project for a new military device or a project for a new IT application. The fact is that, in every of those projects, some notions can be called the same even if they don't cover the same reality. Let us take the sample of \"service\" (which is very emblematic of those frameworks). Some of those framework describe it as being a \"service in the broadest sense of the term\". An aircraft can provide a service, so is a military device, so is an application. For each of those \"products\", the notion of service will represent a different reality: the aircraft can provide a high level service to the army, the device a service to other devices and the IT application a service to a set of users. The connectivity between those services will depend on the context and on what we are talking about. If we imagine a more complex project such as an aircraft, its ground segment and all support processes, it will be very complex to represent all those various objects and their interconnectivity with generic concepts. To address this problem of \"specialization\" of very general concepts (or meta-concepts), the military frameworks propose the \"taxonomy\" approach. By defining for the various kind of views the appropriate taxonomy, the architects define the content of the views which removes the ambiguity. In this spirit, each project will \"instantiate\" the military architecture framework according to its needs: the taxonomy of artifacts will clarify the meaning of abstract artifacts in the context of the project. This way of working is very common in military projects where a \"guidance conference\" at project start defines the various implementation options that are taken by the project. This way of working can be quite easy in a simple project but can quickly become very complicated when the project is suppose to build several \"products\" of several kinds (which is the case in big military programs). For instance, to be able to create meaningful and useful views in a new aircraft project containing the aircraft, the ground segment and the support system, complex and different conventions will have to be taken for each big \"chunk\" of the program. But, if we specialize each part of the program with a specific meaning of artifacts per program part, why not using domain-specific modeling languages such as presented in the \"best of breed\" part above? Bridging several semantic domains together The advantage of having an integrated framework is the bridges between various domains. In a best of breed solution, the bridging are almost impossible unless the modeling tool is unifying several modeling languages (see later in the article). Bridges can be very important in the lifecycle of a program, because they can link the two parts of the V-model (as shown in figure 2 ). Samples are numerous and at the heart of many industrial projects: Interlinking product support to product definition enables a better product maturity; Interlinking all phases with an integrated planning is very interesting (product-based planning) especially if the product is an assembly of other subproducts; Interlinking digital transformation to the two branches of the V-model enables to prepare the efficient changes required in the organizations, their processes and their IT systems; Etc. When the modeling language and its metamodel enables to create those connections, the complexity of big programs can be described with many details [ 1 ]. A problem of semantics The problem, as we saw it in figure 2 , is that, under the military frameworks are \"concrete\" semantic domains underlying. We identified 4 semantic areas in table 1 and figure 1 and 2 : Product definition, Product support and operations, Digital transformation, Project management. We can add that the target of military frameworks is to support a certain kind of projects that are belonging to a limited number of categories: full device, partial device and IT systems, more or less. In those conditions, the advantages brought by the chosen framework must be balanced with its drawbacks. Among them we can name: A non natural way of modeling of already standardized semantic domains, which will make the information understanding and sharing quite difficult, even for experienced professionals; A complexity in the use of taxonomies and a risk of heterogeneity of framework interpretation, between various projects but also within the same project; The risk of introducing ambiguities that will have a negative impact on the project; The problem of finding appropriate skills to manipulate the complexity of the framework and its various interpretations; The cost of training people outside the domain-specific standards; The requirement of creating a tool implementing the metamodel of the framework (and maintaining it over time). Syncretism or unification? Evolutions and work-arounds It is important to note that, with time, the military frameworks opened to some modeling standards, while keeping their own core metamodel, which does not appear as fully consistent. Using Archimate for everything For instance, NAF v4 indicates that Archimate and UAF can be used for modeling. Clearly, Archimate can be used for product support and digital transformation (lines 2 and 3 of table 1 ) but it seems a bit light to model product definition (even with the physical package introduced in version 3). Concerning the project management Archimate has basic artifacts but that cannot enable the management of a complex program, in the way NAF or other military frameworks picture it. That means that Archimate can be used in NAF for certain kinds of projects, but cannot cover all that NAF intends to cover [ 2 ]. Figure 7 shows the classic use of Archimate which deliverables are processes and IT systems. Using SysML for everything Another option in the market is to consider that SysML can be used for everything. We propose to look at figure 7 a comparison between Archimate and SysML on an abstract RFLP process, declined respectively in a product definition process for SysML and on a set of processes and IT systems definition process for Archimate. Figure 7: SysML and Archimate comparison As we said earlier, the systems engineering area is introducing now more and more executable domain specific languages (DSL) to be able to simulate, optimize and perform early tradeoffs for system architecture. That is showing that SysML will never be the only systems engineering modeling language. Second of all, if we compare to Archimate, we can see basically that the two modeling languages are not targeting the same problem. As we said in the best of breed solution, each project which scope encompasses a physical product plus processes and IT would definitely need both approaches. Using UAF for everything UAF on the other hand pretends to cover it all. Its metamodel being an extension of SySML, it seems well suited for product definition. As the meta-model integrates many more artifacts, the coverage seems one of the most extensive of the market. The problem of NAF is, for us, the one we already described and that is attached to every military framework: in order to use it concretely, you have to instantiate it with the precise taxonomy of elements you will use. This is the way the military are doing things, but this way may not be adapted for every company. Using a custom architecture framework for everything Some companies are inventing their own modeling frameworks, quite often by sticking various semantically incompatible metamodels together, or by redefining from scratch a large set of artifacts in a brand new metamodel. In 25 years, we never saw this approach work: worse, we always saw this approach end badly. The drawbacks of this approach are numerous: The new framework is generally not documented enough, so architects are interpreting it in a chaotic and non consistent way; This implies that generally diagrams made by one architect are not understandable by another, which generates errors, misunderstanding and inconsistency in the rest of the chain; The new framework knowledge is concentrated in methodology gurus who know better ; They are on the critical path of everything and have, most of the time, no real experience in modeling on large scale projects (or they would not create a new metamodel on their own); They love methodology and metamodels but are not realizing that the quality of a metamodel is its semantic non ambiguity; They generally won't listen to any comment from operational people; The learning curve on the new framework is very long and complex, because there is not enough document, because the gurus are not reachable and because, as time passes, some errors in the metamodel are being corrected, which makes the old models incompatible with the new version of the framework; The new framework is generally based on a tool implementation with the metamodel; Generally, not everyone has access to the tool, which makes the adoption of the framework difficult for many users; There will always be experienced people that saw that the new framework is not working well (generally bringing semantic confusion) and that will keep on using standard methodologies and tools, that proved to be efficient and helpful in projects. This word is very important: helpful. The modeling framework aims to help architects to build better systems, not to glorify the ego of the gurus. The path of creating a new framework with a new metamodel is very dangerous and always ends badly, so we do not recommend it at all. It is a waste of time, energy and brains. That's why standardization groups were created: to put a lot of people with skills and large experience to define frameworks that work in many contexts and projects. You can look at our article on Archimate that explains why Archimate works . One or many? We have to say that we don't believe in unification, in the single modeling approach that covers any use case, in the \"one size fits all\" universal modeling language. That means we must use a syncretic approach: taking several modeling languages for what they do best. The problem of interlinking models from different modeling languages remains. It can be solved by two approaches: A tool based approach, integrating several metamodels together (with the risk of creating many semantic ambiguities by sticking together metamodels that have semantic overlap); A semantic approach where each modeling universe shares some concepts with other modeling universes, in an interoperability way (semantic web techniques can be very usefull in that area [ 3 ]). Figure 8: Sharing concepts between two modeling languages The second approach is shown in figure 8. Doing the exercise of concept sharing between different modeling languages is very healthy, because it will define what we want to really pilot at the project level. As the modeling languages are domain specific, they will be used at specific moments of the project and so the interfaces between the various modeling activities can be formalized. With the help of the common semantic concepts and relationships, we can define quite easily a maturity process of exchanges between the various poles of expertise of the project during all the phases of its execution. Conclusion Using military frameworks for military contracts When the military customer requires it, the use of those frameworks is mandatory. Here are pieces of advice for a pertinent use of those frameworks: Identify from the scope of the project what views are really necessary, what they represent, who will look at them and if they are here just to communicate between customer and supplier or if they have an operational impact; That means preparing a lot the guidance conference of the project start; In case of operational impact, try to see if the military framework cannot be mapped to another modeling language, better suited for the the particular views; propose this kind of adaptations in the guidance conference and maintain cautiously the taxonomy views explaining those mappings; Identify in the military frameworks the modeling intentions (artifacts and relationships) that are relevant to the project and have them translated in the language you propose to use; that would create a \"pattern library\" of important topics to keep in the radar, even if the chosen working modeling language is not proposing it; Don't hesitate to use inter-domain views, views located in the round rectangle of figure 8 ; keep track of that in the taxonomy; Be opened to concept duplication in various domains if needed but be sure to trace those choices and to create reconciliation views. In all cases, discuss, debate and negotiate in order to find the best compromise between the interests of the customer and the ones of the supplier. Big projects are successful when people share and work together on complexity. Using military frameworks in the industry Military frameworks are tuned to be used on the customer side: they rely on the hypothesis that the army is the customer and the industrial company (or companies) is (or are) the supplier(s). Table 1 explains the foundations of the military concerns. Military preoccupations are very concrete: features and performance, money, schedule, operational model. Those concerns are not the ones of the supplier company, as shown in Figure 2 in the V-model. The supplier company would target an integration at other levels to be able to perform other tradeoffs than the military, who is often an important customer but not the only one. All military frameworks, even UAF, are the result of the works and experience of the military organizations trying to manage projects of billions of dollars/euros. Those military frameworks are born from the military as a customer constraints. When the set of constraints is different, which is the case for the supplier company as an industrial company, the framework needed will not be the same. The industrial world becoming more and more complex, many industrial companies are searching to define or reuse big modeling frameworks to ensure that every aspect of theirs problems is covered, and that they can do all the panels of their required tradeoffs. Currently, in the market, there seem to be no obvious integrated enterprise framework suiting the full range of their needs. For the industry currently and unfortunately, syncretism of modeling languages seems the only solution. Notes [1] On the other hand, concerning the product lifecycle and specifically the links between engineering and support, some other industry-specific standards are covering the process aspect of such a challenge: the ASD standards - Back to text . [2] See the first NAF to Archimate mapping in the online article of Mark Lankhorst - Back to text . [3] Please see also the extensive works of Nicolas Figay on PLM interoperability and Archimate models interoperability - Back to text . ( November 2019 )","title":"Military frameworks, systems engineering and enterprise architecture"},{"location":"articles/mbse-vs-ea/#military-frameworks-systems-engineering-and-enterprise-architecture","text":"(Courtesy of Pixaby on https://www.pexels.com) In the world of architecture modeling, the military organizations like the US Department of Defense (DoD), the UK Ministry of Defence (MoD) or the NATO organization created enterprise architecture frameworks adapted to their needs. On the market today, 3 main frameworks are existing and used in military projects: DoDAF , the US MoD Architecture Framework, that was historically the first military architecture framework ever (it is originated in the 90s); MoDAF , the UK MoD architecture framework, which is both an adaptation and an abstraction of DoDAF; NAF , the NATO Architecture Framework that is also an adaptation and an abstraction of DoDAF and MoDAF.","title":"Military frameworks, systems engineering and enterprise architecture"},{"location":"articles/mbse-vs-ea/#objectives-and-dimensions-of-the-military-frameworks","text":"The main objective searched by military organization is to be able to master a global project of several billions of dollars/euros from the design phase to the support and operational phase. Because those projects/programs can be very large and very complex, the idea was that everyone (military organization and supplier organizations) share the same set of \"views\" on the program in order to share understanding of what was to be done, how requirements were taken into account, what is the full lifecycle of the product and how the project should be managed. Table 1: Structure of military requirements # Dimension Details 1 Product definition Can be an aircraft, weapon, complex system, etc. and all the related IT systems to manage them. The objective is to use a systems engineering approach (model-based in that case), based on requirements. 2 Product support and operations The objective of this phase is to detail all the systems that will permit the support of the product. This is generally a wide topic including maintenance, parts procurement, etc. Moreover, the product use must be envisaged in operations preparation and in battle field in coordination with other systems and organizations. 3 Digital transformation The objective of this phase is to evaluate and deal with the impacts of the product and product support in the operational organization, processes and IT of the army. 4 Project plan The objective is to be able to describe the the plan to deliver the product conforming to the requirements (phases, milestones and so on), to permit project steering, and product support procurement. We can classify the military enterprise architecture frameworks as pursuing 4 objectives, as shown in table 1.","title":"Objectives and dimensions of the military frameworks"},{"location":"articles/mbse-vs-ea/#modeling-domains","text":"Those 4 objectives are traditionally modeled with 3 different modeling techniques: Systems engineering for the product definition, Enterprise architecture for: Product support, Digital transformation, Project modeling tools for project management. Figure 1: The 3 domains Military framework concerns in the industrial V-model The figure 1 shows the 3 modeling modeling domains that they are traditionally covered in military frameworks. In terms of modeling, we could say that the core objective of the military frameworks is to have, in a single repository, the systems engineering (MBE/MBSE), the enterprise architecture and the project management gathered together .","title":"Modeling domains"},{"location":"articles/mbse-vs-ea/#positioning-in-the-v-model","text":"Those 4 areas of concern can be mapped on an industrial V-model as shown in Figure 2. Figure 2: Military framework concerns in the industrial V-model The first dimension, product definition, is in the left part of the V and corresponds to the product architecture, from its operational conditions, requirements to its system design. The second dimension is corresponding to product support (we often speak about \"support concept\"). The two last dimensions are running all along the project, being at the project/program level or at the digital transformation level (integrate the new product into the current organizations). We can see in Figure 2 how an integrated modeling of those two highest branches of the V-model can be important for the military customer: It ensures a full lifecycle modeling of the product, It enables simulation scenarios linked to the requirement phase, It enables to anticipate the impact of the new project on army's existing organizations, It enables to manage the project/program. Most of all, on top of being a common modeling language, it enables complex tradeoffs, including performance, finance, supportability, etc.","title":"Positioning in the V-model"},{"location":"articles/mbse-vs-ea/#mbsembe","text":"","title":"MBSE/MBE"},{"location":"articles/mbse-vs-ea/#the-rflp-process","text":"In manufacturing, the objective is to create products. The starting point of a product manufacturing is generally a set of requirements. To go from the requirements to the product is generally a long process involving the following phases: Requirements engineering : The objective of this phase is to work on requirements in order to make them non ambiguous, to sort them by importance and to remove inconsistency between them; In this phase, all the operational conditions of the product are described, which is a crucial phase of the process; Functional analysis : This phase objective is to determine the functions of the product, the information flows and dependencies between functions; Generally, the functions are worked at different levels of granularity (functions, sub-functions, sub-sub-functions and so on); Logical system analysis : Once the various functions at various levels are known, the assignment of functions to logical elements can be done; The objective of this phase is to create the appropriate logical systems to get the nearest possible from a possible industrial split of the realization of the product; When the logical analysis is finished, we have the detailed specification of each system ; Physical design : Once the product has been split on several components, several teams can work in parallel on component design (at the level of granularity of the component); The product architecture will ensure that all detailed components fit together conforming to the specifications. This view is, for sure, simplified and some industries (or specific businesses inside the industries like embedded software, electronics, or mechatronics) are often using more complex processes, with dedicated sub-processes for every discipline. However, seen from a high level standpoint, the various phases of the MBSE are the 4 we mentioned, leading some people to call the MBSE process a \"RFLP\" process (RFLP standing for Requirements, Functional, Logical, Physical). Currently, we find more and more people skipping the \"S\" of MBSE to talk about MBE, for model-based engineering. The term becomes larger because it does not have this embedded system connotation anymore.","title":"The RFLP process"},{"location":"articles/mbse-vs-ea/#maintaining-the-digital-thread","text":"For sure, a RFLP process aims to create a digital \"thread\" or \"link\" between the various entities, at least between the requirements, the functions and the systems (plus all the connectivity semantics). The system entities are a solution matching the requirements, and so is their \"physical\" description; but this solution is the result of various levels of trade-off analysis. In other terms, solutions can (and will) evolve with time. The full digital thread of information will have to be maintained through changes. Changes in requirements (or operational conditions) will lead to changes in systems and changes in systems (like obsolescence) must be compliant with requirements.","title":"Maintaining the digital thread"},{"location":"articles/mbse-vs-ea/#rflp-processes-with-many-variable-dimensions","text":"The RFLP processes that we can find in the industry are generally describing many parameters attached to the various elements (requirements, functions, systems, etc.): the performance, the price, the various criteria of manufacturability, the respect of regulation and standards are among the most important of parameters (the weight also in aerospace). Those parameters are crucial because, during the various design phases: They permit to use requirement quantification (with values and ranges) and not only requirement qualification; They enable to simulate the various elements working together (functional simulation, system simulation, or full product simulation in some known conditions); They enable complex trade-offs that lead to design the best product \"under a certain set of constraints\".","title":"RFLP processes with many variable dimensions"},{"location":"articles/mbse-vs-ea/#modeling-in-mbse","text":"The topic is quite vast but we will, once again, try a simplification. Modeling can be done mainly with 3 different approaches: The first one is to do \"manual\" systems engineering with Office documents (Word and Excel mainly); This way of doing things is still very present in the industry; It is error prone, and very painful for the engineering teams; The second one is to use \"standard MBSE modeling language\" like SysML or Capella to run the \"RFL\" part of the MBSE process (the \"P\" part will be done with specific tools like 3D design tools or electricity engineering tools); The objective is to model graphically the \"RFL\" part of the process and to keep links with the \"P\" parts; The third one is to use a simulation/optimization-oriented domain-specific language (DSL, the domain being systems engineering) for requirements and systems like PMM or Form-L . In the first style of systems engineering, the digital thread is extremely difficult to maintain or trace, and generally, to be able to do it, a lot of manual and painful efforts are required. Most of the time, the requirements are vague, not measurable, and the trade-offs analysis are not recorded. With time, we see a growing consciousness of the advantages of the third solution to be able to do complex trade-offs very early in the product design phase (see the system engineering book of knowledge here ).","title":"Modeling in MBSE"},{"location":"articles/mbse-vs-ea/#mbembse-seen-from-the-military","text":"The MBE/MBSE modeling techniques are a way for military organizations to ensure that their requirements are correctly understood and are refined the right way. Especially, the requirements being tightly coupled with operational conditions (often called CONOPS for CONcept of OPerationS), the graphical modeling in a unified language is an enabler for common understanding, sharing of decisions in complex trade-offs and sharing of information. For sure, this is being true for the design phase of the system.","title":"MBE/MBSE seen from the military"},{"location":"articles/mbse-vs-ea/#about-the-complexity-of-modeling-hybrid-systems","text":"The fact is systems engineering is very adapted to design the early phases of a product and do the right structuring trade-offs. But the \"products\" of today are much more complex than the products of yesterday. If we take the sample of aerospace, the \"ground segment\", or the set of software that enable to manage the machines from the ground is much bigger and much more complex than before. Let us take a sample in a non military domain: the constellations of satellites. To be able to operate a constellation of more than 100 satellites in a crowded space, the ground segment complexity is very high. The thing is it is not easy to design the product (the satellite) as the set of software required to operate many instances of this very satellite. More and more military systems are facing the same kind of problems: they must tightly couple together the product definition and the product support design phases. How to ensure that all the requirements are taken in charge by all sub-systems in the global system, above all when the way of model management software systems is more related to IT techniques such as enterprise architecture, IT architecture or software architecture?","title":"About the complexity of modeling hybrid systems"},{"location":"articles/mbse-vs-ea/#modeling-enterprise-architecture","text":"The enterprise architecture domain is very wide and our objective is not here to describe it extensively. We will just mention that this domain is mainly split in 2 concerns: What methodology should be used to create a good IT architecture serving the business process transformation according to the strategic intentions? What are the relevant views and artifacts to use to model the digital transformation, starting with the strategy, going through the business layer, the application layer and the infrastructure layer? There are many enterprise architecture methodology frameworks, but the most used nowadays is TOGAF . The most used enterprise architecture modeling language is Archimate (see our introductory article and our recommended recipes ). This language is particularly adapted to model most of the other dimensions of interest of the military project, especially the organizations, strategy, processes and IT systems, whatever the complexity. It proposes 13 basic viewpoints (see here ) but its global metamodel makes it adapted for many complex representations. Traditionally enterprise architecture is information systems oriented and aims at reintroducing the business common sense in IT. In order to tackle the strategic intentions of a company that wants to transform, process and IT transformation must be considered together as a system in order to avoid the 2 common traps: The biased process-driven project able to change the company processes but without any efficiency for the company (for instance by bringing more Excel spreadsheets, more blocking steps, multiple keying and bureaucracy than the company already has); The biased IT-driven project able to change the IT systems without being synchronized with new processes (and generating more troubles, data inconsistencies, process problems, double keying, etc.). Samples of those traps are numerous. For the first case, transformation projects led by process experts often lead to a huge loss of money because they create more Excel tools and often add layers of procedures to already overly complex environments. For the second case, a typical example is the \"tool project approach\": when users have a need, let's buy a tool and deploy it. Those two sample lead to the same result: inefficient, error prone and expensive processes, and inefficient, error prone and expensive IT systems. In one word: more recurring costs, less productivity and a future cost of change that increased. Enterprise architecture, by its way of considering the processes and IT systems together enables to perform the proper tradeoffs in projects and to plan a real digital transformation, meaning a process and IT systems joint transformation targeting a better efficiency (cost reduction or productivity enhancement).","title":"Modeling enterprise architecture"},{"location":"articles/mbse-vs-ea/#various-solutions-for-military-projects","text":"Considering the military requirements of table 1 , let us dig into the various frameworks that enable to model military projects.","title":"Various solutions for military projects"},{"location":"articles/mbse-vs-ea/#the-best-of-breed-solutions","text":"If we look at the market and take the best of breed modeling solutions adapted to the requirements of table 1 , we could propose the solutions presented in Table 2. Table 2: Best of breed solutions for military programs # Dimension Solution 1 Product definition SysML systems engineering modeling language 2 Product support and operations Archimate enterprise architecture modeling language 3 Digital transformation Archimate enterprise architecture modeling language 4 Project plan Complex planning tools for programs (set of projects with dependencies) + Archimate with the implementation and migration package As we can see in the table 2, we have 3 ways of modeling the full program: SysML, Archimate and a planning tool. The 3 metamodels not being integrated together, we may have problems at the \"interfaces\" of each domain (see later in the article). SysML proposes 9 kinds of diagrams and Archimate 13 basic viewpoints, what corresponds to approximately 22 types of diagrams. If we had 2 or 3 viewpoints for planning, we would end up with a solution presenting around 25 viewpoints.","title":"The best of breed solutions"},{"location":"articles/mbse-vs-ea/#the-military-frameworks","text":"","title":"The military frameworks"},{"location":"articles/mbse-vs-ea/#dodaf","text":"The military frameworks, for decades, defined a set of artifacts and views to have it all covered. The Figure 3 shows the structure of views of DoDAF version 2. Figure 3: DoDAF viewpoint structure (52 viewpoints)","title":"DoDAF"},{"location":"articles/mbse-vs-ea/#modaf","text":"The Figure 4 shows the structure of views of MoDAF. Figure 4: MoDAF viewpoint structure (46 viewpoints)","title":"MoDAF"},{"location":"articles/mbse-vs-ea/#naf","text":"The Figure 5 shows the structure of views in NAF v4. Figure 5: NAF viewpoint structure (46 viewpoints)","title":"NAF"},{"location":"articles/mbse-vs-ea/#uaf","text":"An attempt of \"unification\" of those three frameworks was done by the OMG with the so-called Unified Architecture Framework ( UAF ). The Figure 6 shows the structure of views in UAF v1. Figure 6: UAF viewpoint structure (+70 viewpoints)","title":"UAF"},{"location":"articles/mbse-vs-ea/#views-and-metamodel","text":"All frameworks propose a metamodel that is often very rich, and also very complex. With time, the metamodels got more abstract which makes their use quite difficult.","title":"Views and metamodel"},{"location":"articles/mbse-vs-ea/#genericity-specialization-and-taxonomy","text":"The core advantage - and the core problem - of those frameworks is that they are generic . With the same semantics (or almost), it is possible to describe a project for a new military aircraft, or a project for a new military device or a project for a new IT application. The fact is that, in every of those projects, some notions can be called the same even if they don't cover the same reality. Let us take the sample of \"service\" (which is very emblematic of those frameworks). Some of those framework describe it as being a \"service in the broadest sense of the term\". An aircraft can provide a service, so is a military device, so is an application. For each of those \"products\", the notion of service will represent a different reality: the aircraft can provide a high level service to the army, the device a service to other devices and the IT application a service to a set of users. The connectivity between those services will depend on the context and on what we are talking about. If we imagine a more complex project such as an aircraft, its ground segment and all support processes, it will be very complex to represent all those various objects and their interconnectivity with generic concepts. To address this problem of \"specialization\" of very general concepts (or meta-concepts), the military frameworks propose the \"taxonomy\" approach. By defining for the various kind of views the appropriate taxonomy, the architects define the content of the views which removes the ambiguity. In this spirit, each project will \"instantiate\" the military architecture framework according to its needs: the taxonomy of artifacts will clarify the meaning of abstract artifacts in the context of the project. This way of working is very common in military projects where a \"guidance conference\" at project start defines the various implementation options that are taken by the project. This way of working can be quite easy in a simple project but can quickly become very complicated when the project is suppose to build several \"products\" of several kinds (which is the case in big military programs). For instance, to be able to create meaningful and useful views in a new aircraft project containing the aircraft, the ground segment and the support system, complex and different conventions will have to be taken for each big \"chunk\" of the program. But, if we specialize each part of the program with a specific meaning of artifacts per program part, why not using domain-specific modeling languages such as presented in the \"best of breed\" part above?","title":"Genericity, specialization and taxonomy"},{"location":"articles/mbse-vs-ea/#bridging-several-semantic-domains-together","text":"The advantage of having an integrated framework is the bridges between various domains. In a best of breed solution, the bridging are almost impossible unless the modeling tool is unifying several modeling languages (see later in the article). Bridges can be very important in the lifecycle of a program, because they can link the two parts of the V-model (as shown in figure 2 ). Samples are numerous and at the heart of many industrial projects: Interlinking product support to product definition enables a better product maturity; Interlinking all phases with an integrated planning is very interesting (product-based planning) especially if the product is an assembly of other subproducts; Interlinking digital transformation to the two branches of the V-model enables to prepare the efficient changes required in the organizations, their processes and their IT systems; Etc. When the modeling language and its metamodel enables to create those connections, the complexity of big programs can be described with many details [ 1 ].","title":"Bridging several semantic domains together"},{"location":"articles/mbse-vs-ea/#a-problem-of-semantics","text":"The problem, as we saw it in figure 2 , is that, under the military frameworks are \"concrete\" semantic domains underlying. We identified 4 semantic areas in table 1 and figure 1 and 2 : Product definition, Product support and operations, Digital transformation, Project management. We can add that the target of military frameworks is to support a certain kind of projects that are belonging to a limited number of categories: full device, partial device and IT systems, more or less. In those conditions, the advantages brought by the chosen framework must be balanced with its drawbacks. Among them we can name: A non natural way of modeling of already standardized semantic domains, which will make the information understanding and sharing quite difficult, even for experienced professionals; A complexity in the use of taxonomies and a risk of heterogeneity of framework interpretation, between various projects but also within the same project; The risk of introducing ambiguities that will have a negative impact on the project; The problem of finding appropriate skills to manipulate the complexity of the framework and its various interpretations; The cost of training people outside the domain-specific standards; The requirement of creating a tool implementing the metamodel of the framework (and maintaining it over time).","title":"A problem of semantics"},{"location":"articles/mbse-vs-ea/#syncretism-or-unification","text":"","title":"Syncretism or unification?"},{"location":"articles/mbse-vs-ea/#evolutions-and-work-arounds","text":"It is important to note that, with time, the military frameworks opened to some modeling standards, while keeping their own core metamodel, which does not appear as fully consistent.","title":"Evolutions and work-arounds"},{"location":"articles/mbse-vs-ea/#using-archimate-for-everything","text":"For instance, NAF v4 indicates that Archimate and UAF can be used for modeling. Clearly, Archimate can be used for product support and digital transformation (lines 2 and 3 of table 1 ) but it seems a bit light to model product definition (even with the physical package introduced in version 3). Concerning the project management Archimate has basic artifacts but that cannot enable the management of a complex program, in the way NAF or other military frameworks picture it. That means that Archimate can be used in NAF for certain kinds of projects, but cannot cover all that NAF intends to cover [ 2 ]. Figure 7 shows the classic use of Archimate which deliverables are processes and IT systems.","title":"Using Archimate for everything"},{"location":"articles/mbse-vs-ea/#using-sysml-for-everything","text":"Another option in the market is to consider that SysML can be used for everything. We propose to look at figure 7 a comparison between Archimate and SysML on an abstract RFLP process, declined respectively in a product definition process for SysML and on a set of processes and IT systems definition process for Archimate. Figure 7: SysML and Archimate comparison As we said earlier, the systems engineering area is introducing now more and more executable domain specific languages (DSL) to be able to simulate, optimize and perform early tradeoffs for system architecture. That is showing that SysML will never be the only systems engineering modeling language. Second of all, if we compare to Archimate, we can see basically that the two modeling languages are not targeting the same problem. As we said in the best of breed solution, each project which scope encompasses a physical product plus processes and IT would definitely need both approaches.","title":"Using SysML for everything"},{"location":"articles/mbse-vs-ea/#using-uaf-for-everything","text":"UAF on the other hand pretends to cover it all. Its metamodel being an extension of SySML, it seems well suited for product definition. As the meta-model integrates many more artifacts, the coverage seems one of the most extensive of the market. The problem of NAF is, for us, the one we already described and that is attached to every military framework: in order to use it concretely, you have to instantiate it with the precise taxonomy of elements you will use. This is the way the military are doing things, but this way may not be adapted for every company.","title":"Using UAF for everything"},{"location":"articles/mbse-vs-ea/#using-a-custom-architecture-framework-for-everything","text":"Some companies are inventing their own modeling frameworks, quite often by sticking various semantically incompatible metamodels together, or by redefining from scratch a large set of artifacts in a brand new metamodel. In 25 years, we never saw this approach work: worse, we always saw this approach end badly. The drawbacks of this approach are numerous: The new framework is generally not documented enough, so architects are interpreting it in a chaotic and non consistent way; This implies that generally diagrams made by one architect are not understandable by another, which generates errors, misunderstanding and inconsistency in the rest of the chain; The new framework knowledge is concentrated in methodology gurus who know better ; They are on the critical path of everything and have, most of the time, no real experience in modeling on large scale projects (or they would not create a new metamodel on their own); They love methodology and metamodels but are not realizing that the quality of a metamodel is its semantic non ambiguity; They generally won't listen to any comment from operational people; The learning curve on the new framework is very long and complex, because there is not enough document, because the gurus are not reachable and because, as time passes, some errors in the metamodel are being corrected, which makes the old models incompatible with the new version of the framework; The new framework is generally based on a tool implementation with the metamodel; Generally, not everyone has access to the tool, which makes the adoption of the framework difficult for many users; There will always be experienced people that saw that the new framework is not working well (generally bringing semantic confusion) and that will keep on using standard methodologies and tools, that proved to be efficient and helpful in projects. This word is very important: helpful. The modeling framework aims to help architects to build better systems, not to glorify the ego of the gurus. The path of creating a new framework with a new metamodel is very dangerous and always ends badly, so we do not recommend it at all. It is a waste of time, energy and brains. That's why standardization groups were created: to put a lot of people with skills and large experience to define frameworks that work in many contexts and projects. You can look at our article on Archimate that explains why Archimate works .","title":"Using a custom architecture framework for everything"},{"location":"articles/mbse-vs-ea/#one-or-many","text":"We have to say that we don't believe in unification, in the single modeling approach that covers any use case, in the \"one size fits all\" universal modeling language. That means we must use a syncretic approach: taking several modeling languages for what they do best. The problem of interlinking models from different modeling languages remains. It can be solved by two approaches: A tool based approach, integrating several metamodels together (with the risk of creating many semantic ambiguities by sticking together metamodels that have semantic overlap); A semantic approach where each modeling universe shares some concepts with other modeling universes, in an interoperability way (semantic web techniques can be very usefull in that area [ 3 ]). Figure 8: Sharing concepts between two modeling languages The second approach is shown in figure 8. Doing the exercise of concept sharing between different modeling languages is very healthy, because it will define what we want to really pilot at the project level. As the modeling languages are domain specific, they will be used at specific moments of the project and so the interfaces between the various modeling activities can be formalized. With the help of the common semantic concepts and relationships, we can define quite easily a maturity process of exchanges between the various poles of expertise of the project during all the phases of its execution.","title":"One or many?"},{"location":"articles/mbse-vs-ea/#conclusion","text":"","title":"Conclusion"},{"location":"articles/mbse-vs-ea/#using-military-frameworks-for-military-contracts","text":"When the military customer requires it, the use of those frameworks is mandatory. Here are pieces of advice for a pertinent use of those frameworks: Identify from the scope of the project what views are really necessary, what they represent, who will look at them and if they are here just to communicate between customer and supplier or if they have an operational impact; That means preparing a lot the guidance conference of the project start; In case of operational impact, try to see if the military framework cannot be mapped to another modeling language, better suited for the the particular views; propose this kind of adaptations in the guidance conference and maintain cautiously the taxonomy views explaining those mappings; Identify in the military frameworks the modeling intentions (artifacts and relationships) that are relevant to the project and have them translated in the language you propose to use; that would create a \"pattern library\" of important topics to keep in the radar, even if the chosen working modeling language is not proposing it; Don't hesitate to use inter-domain views, views located in the round rectangle of figure 8 ; keep track of that in the taxonomy; Be opened to concept duplication in various domains if needed but be sure to trace those choices and to create reconciliation views. In all cases, discuss, debate and negotiate in order to find the best compromise between the interests of the customer and the ones of the supplier. Big projects are successful when people share and work together on complexity.","title":"Using military frameworks for military contracts"},{"location":"articles/mbse-vs-ea/#using-military-frameworks-in-the-industry","text":"Military frameworks are tuned to be used on the customer side: they rely on the hypothesis that the army is the customer and the industrial company (or companies) is (or are) the supplier(s). Table 1 explains the foundations of the military concerns. Military preoccupations are very concrete: features and performance, money, schedule, operational model. Those concerns are not the ones of the supplier company, as shown in Figure 2 in the V-model. The supplier company would target an integration at other levels to be able to perform other tradeoffs than the military, who is often an important customer but not the only one. All military frameworks, even UAF, are the result of the works and experience of the military organizations trying to manage projects of billions of dollars/euros. Those military frameworks are born from the military as a customer constraints. When the set of constraints is different, which is the case for the supplier company as an industrial company, the framework needed will not be the same. The industrial world becoming more and more complex, many industrial companies are searching to define or reuse big modeling frameworks to ensure that every aspect of theirs problems is covered, and that they can do all the panels of their required tradeoffs. Currently, in the market, there seem to be no obvious integrated enterprise framework suiting the full range of their needs. For the industry currently and unfortunately, syncretism of modeling languages seems the only solution.","title":"Using military frameworks in the industry"},{"location":"articles/mbse-vs-ea/#notes","text":"[1] On the other hand, concerning the product lifecycle and specifically the links between engineering and support, some other industry-specific standards are covering the process aspect of such a challenge: the ASD standards - Back to text . [2] See the first NAF to Archimate mapping in the online article of Mark Lankhorst - Back to text . [3] Please see also the extensive works of Nicolas Figay on PLM interoperability and Archimate models interoperability - Back to text . ( November 2019 )","title":"Notes"},{"location":"articles/portfolio/","text":"A Simple Meta-Model for Portfolio Management Image courtesy of freedigitalphotos.net . One of the main problems of companies is to manage many IT projects at the same time. Most often, the companies facing this problem are managing project portfolios. The main drawback of managing several projects in parallel is that is is not easy to: Understand the dependencies of the various projects together, and so the order in which they should be led; Identify and deal with the various scope overlaps between the various projects; Manage the relationship with the business people that want the various projects to happen. This article presents a very simple method to use to address all those points. We will present the original version of the meta-model. An Archimate version of this model will be presented in another article. This method mostly targets enterprise or IT architects, program managers or project portfolio managers. Due to its simplicity, many other profiles can use it. Meta-Model Presentation The first objective is to formalize the client of the IT project. Each organization has several departments in which we have several different customers. Our first artifact type will be Organization . We will assume that every organization can include several other organizations. Each organization will also have a customer name as an attribute. When the projects intentions are declared, the materials that are produced by the various customers inside the same organization will generally be a list of many projects, with a project name and a more or less developed functional description. We will define two artifact types to model that: Project to model the project, Function to model the functionality. In the existing IT systems, we have applications. With the projects to come, some of the current application will get new functions, some new applications will be created and some other will be decommissioned. We need one more artifact type: Application that models the application; if the application is new, we need a way to tag it as \"new\" or \"existing\". The Function artifact type will be used to attach functionality to Application s. For this purpose, we need an attribute on the Function that indicates if the function is already existing, if it is new or if it must evolve. As applications are part of a big part of the IT systems (for instance the financial systems), we will attach the application to an application domain; we will use the Domain artifact type for that purpose. In terms of relationships, we have the following semantics: An Organization can contain other Organization s; An Organization will be the customer of several Project s; Project s are aggregating Function s; Application s are attached to Domain s and Function s are attached to them. The resulting meta-model is presented on the following figure. Methodology Let's suppose we use this model during the budgeting process that usually happens once a year in companies. Budget time is often the time for difficult choices. The following method is easy to use, can neutralize affects and help the top level management to choose between projects and to create a feasible roadmap within the limits of a feasible budget. Step 1: Accept All Project Requests and Identify Functions The first step is consisting in gathering all projects intentions from all organizations. We will analyze organization per organization. For each customer within a particular organization, the project intentions are gathered. The objective is to get the best functional description of the requirements possible and to identify the required functions. Very often, the requirements are a list of features (warning: some of them may already have been implemented without the customer to know about it). In that phase, the customer must not be challenged, but the architect should try his best to understand the functional (and non functional) requirements. To do that, it can be necessary to distinguish between the solution imagined by the customer and the original requirement. As the model is simple, the requirement artifact does not exist in this version. The identification of the required Function s will conclude this phase. Once this phase is complete, we can promise the customer to: Analyze his requirements along with all other requirements and projects and do our best to develop what is required; Provide a detailed feedback on the demands when all demands have been captured. Step 2: Identify Common Functions to Highlight Dependencies Several customers could have defined different projects that are using the same subset of functions. Identifying those functions is very important, this for several reasons: To avoid several projects to develop several times the same function, or worse, variations of the same function; To identify in what projects this function can be required and to scope it carefully; Developing a reusable function will ease the success of many projects whereas not developing it may cause project delays or failure; To add this function early in the roadmap; To take a special care to carefully place this function at the proper spot in the rest of the IT systems (step 3); Indeed, a misplaced function will generate many useless client/server calls, will generate consistency problems and will generate more integration and QA works: Economically, function placement is crucial. Functions, once identified, should be assigned with a number characterizing the number of projects that require them. The highest number will lead the global roadmapping exercise. We will call this number the dependency number . Step 3: Associate Functions to Applications This is a back office step that will be done between IT people, most of the time by architects. It is time to review the main functions of the existing applications to be accurate in the exercise. Functions will be classified into 3 categories: Existing functions : We can plan to go back to the customer to assess if we missed something in the original requirement and if the existing function should not evolve. New functions that can be naturally attached to an existing application : Some of the functions will naturally find their place as an evolution of an existing application; New functions that don't seem to be a natural evolution of the existing applications : However, some new functions have to be studied in order to determine their best spot in the IT systems (or their \"less worst\" spot). The two last steps can give birth to more advanced architecture studies. Covering this topic is not in the scope of this article, but we can insist on leading careful analysis, because function misplacement is a problem that will cause painful and costly problems for years if not for decades. All the functions that are not in existing applications should be gathered and analyzed as a whole to determine the opportunity of developing new applications. Step 4: Analyze the Roadmap of Each Application, Size it Roughly and Compare With The Skills Available Once all functions were assigned to applications, we can size them roughly and have a view of the roadmap of each application. For sure, this roadmap is still theoretical and the pure consequence of the customer's requirements. The Case of Application Maintenance and Evolution All functions of an existing application can be prioritized in the best order of feasibility to create a first credible application roadmap, that is compliant with technical constraints. At this stage, we can also identify the functions that are nice to have compared to the ones that are must haves. This classification, even if it can be debated with the business may lead to some negotiation margins during the year to come. The skills available for each application is important because it will determine the real feasibility of the roadmap per application: We mean IT skills (project manager, product owner, developer, QA) but also business skills. Indeed, in that phase, we can identify the bottlenecks in terms of IT people, when not enough people are skilled to develop the expected application roadmap. Depending on the global priorities of the company, it is possible in that phase to plan people transfer, trainings or recruitments. The Case of New Applications Those projects are always more risky than the previous ones. In some cases, it is obvious for the customer that the new requirements will lead to a new application. In some other cases, it is not. Those cases must be dealt with carefully because the customer may argue that the new requirements are part of an existing application. In those cases, the objective is to avoid function misplacement. However, it is more difficult to create a new application: The project manager has to be found, and she/he has to be able to lead from-scratch projects; The team has to be found, or in the existing people (staff or consultants), or in new comers; The business expert(s) has to be named. Some business people may be a bit worried about the application creation projects because they cause HR troubles and they can face more delays or failures than evolution projects. End of the Phase At the end of the step 4, we should have: All functions identified; All application roadmap sized; All HR requirements. Step 5: Introduction of Dependencies to Create Project Visions To determine which projects to do first in the project portfolio, we need to take the reusable functions with the highest dependency number, in order to schedule them first. Then, the purpose is to play with the constraints to determine the less worst global path considering: The list of projects to develop; The skills available; The potential HR adjustments (people moving from one project to another, new people). Generally, the functions with the highest dependency number will drive the roadmap. That means that a function will be develop if and only if it is in the same project than a function with a high dependency number. That means also that generally a lot of new functions required by the business will not be able to be developed in the coming year, just because they are not part of an \"important\" project. Indeed, this method enables to show to the top level management what is important to the company throughout the functions that are required by many projects (i.e. that have the highest dependency numbers). Once everything is done, the exercise can be redone if the budget is bigger or lower in some parts. Limits of the Method We can argue that the functions with the highest dependency numbers may not be the most crucial functions. That can be true. We can use some tricks in using several levels of functions if needed. Indeed, a function (level 1) required by a certain project could be divided into 3 functions of a smaller granularity (level 2). The level 2 functions could be the functions implemented by the applications. With this model adjustment, we can have a high level view of the function with \"project functions\" (level 1) and an IT view of functions (level 2). The artifact type Function must then be able to aggregate a list of Function s. We let the reader update the meta-model. What Tools Can Help? Yed When this method was used for the first time, Yed was the tool used. A Yed model was realized per organization. The advantage of Yed is its simplicity and its powerful set of graph layout algorithms. The drawback of Yed is when the number of projects is big (we had to deal with the Digital Division and their 130 projects), the model is hardly usable. EMF and Sirius Indeed, when the number of projects is high, a multiple view design tool is required. A possible option is to create and EMF model and to create the edition views with Sirius . You can also use MetaEdit+ that is a powerful metamodeler. Roadmapping With Archimate Archimate is probably the best language to realize this kind of portfolio management. We will explain in a bit more details a possible use of Archimate enterprise architecture modeling language for portfolio roadmapping in another article. Conclusion With this meta-model, it is possible to create and operate a feasible, logic and auditable project portfolio and to have explanations about the constraints and the process of deciding what project should be done in priority. Most often, this kind of tools pacifies the battleground. Even if projects can go wrong during the year to come, with this method, many problems will have been anticipated and will, as per magic, not occur during the year. ( January 2018 )","title":"A Simple Meta-Model for Portfolio Management"},{"location":"articles/portfolio/#a-simple-meta-model-for-portfolio-management","text":"Image courtesy of freedigitalphotos.net . One of the main problems of companies is to manage many IT projects at the same time. Most often, the companies facing this problem are managing project portfolios. The main drawback of managing several projects in parallel is that is is not easy to: Understand the dependencies of the various projects together, and so the order in which they should be led; Identify and deal with the various scope overlaps between the various projects; Manage the relationship with the business people that want the various projects to happen. This article presents a very simple method to use to address all those points. We will present the original version of the meta-model. An Archimate version of this model will be presented in another article. This method mostly targets enterprise or IT architects, program managers or project portfolio managers. Due to its simplicity, many other profiles can use it.","title":"A Simple Meta-Model for Portfolio Management"},{"location":"articles/portfolio/#meta-model-presentation","text":"The first objective is to formalize the client of the IT project. Each organization has several departments in which we have several different customers. Our first artifact type will be Organization . We will assume that every organization can include several other organizations. Each organization will also have a customer name as an attribute. When the projects intentions are declared, the materials that are produced by the various customers inside the same organization will generally be a list of many projects, with a project name and a more or less developed functional description. We will define two artifact types to model that: Project to model the project, Function to model the functionality. In the existing IT systems, we have applications. With the projects to come, some of the current application will get new functions, some new applications will be created and some other will be decommissioned. We need one more artifact type: Application that models the application; if the application is new, we need a way to tag it as \"new\" or \"existing\". The Function artifact type will be used to attach functionality to Application s. For this purpose, we need an attribute on the Function that indicates if the function is already existing, if it is new or if it must evolve. As applications are part of a big part of the IT systems (for instance the financial systems), we will attach the application to an application domain; we will use the Domain artifact type for that purpose. In terms of relationships, we have the following semantics: An Organization can contain other Organization s; An Organization will be the customer of several Project s; Project s are aggregating Function s; Application s are attached to Domain s and Function s are attached to them. The resulting meta-model is presented on the following figure.","title":"Meta-Model Presentation"},{"location":"articles/portfolio/#methodology","text":"Let's suppose we use this model during the budgeting process that usually happens once a year in companies. Budget time is often the time for difficult choices. The following method is easy to use, can neutralize affects and help the top level management to choose between projects and to create a feasible roadmap within the limits of a feasible budget.","title":"Methodology"},{"location":"articles/portfolio/#step-1-accept-all-project-requests-and-identify-functions","text":"The first step is consisting in gathering all projects intentions from all organizations. We will analyze organization per organization. For each customer within a particular organization, the project intentions are gathered. The objective is to get the best functional description of the requirements possible and to identify the required functions. Very often, the requirements are a list of features (warning: some of them may already have been implemented without the customer to know about it). In that phase, the customer must not be challenged, but the architect should try his best to understand the functional (and non functional) requirements. To do that, it can be necessary to distinguish between the solution imagined by the customer and the original requirement. As the model is simple, the requirement artifact does not exist in this version. The identification of the required Function s will conclude this phase. Once this phase is complete, we can promise the customer to: Analyze his requirements along with all other requirements and projects and do our best to develop what is required; Provide a detailed feedback on the demands when all demands have been captured.","title":"Step 1: Accept All Project Requests and Identify Functions"},{"location":"articles/portfolio/#step-2-identify-common-functions-to-highlight-dependencies","text":"Several customers could have defined different projects that are using the same subset of functions. Identifying those functions is very important, this for several reasons: To avoid several projects to develop several times the same function, or worse, variations of the same function; To identify in what projects this function can be required and to scope it carefully; Developing a reusable function will ease the success of many projects whereas not developing it may cause project delays or failure; To add this function early in the roadmap; To take a special care to carefully place this function at the proper spot in the rest of the IT systems (step 3); Indeed, a misplaced function will generate many useless client/server calls, will generate consistency problems and will generate more integration and QA works: Economically, function placement is crucial. Functions, once identified, should be assigned with a number characterizing the number of projects that require them. The highest number will lead the global roadmapping exercise. We will call this number the dependency number .","title":"Step 2: Identify Common Functions to Highlight Dependencies"},{"location":"articles/portfolio/#step-3-associate-functions-to-applications","text":"This is a back office step that will be done between IT people, most of the time by architects. It is time to review the main functions of the existing applications to be accurate in the exercise. Functions will be classified into 3 categories: Existing functions : We can plan to go back to the customer to assess if we missed something in the original requirement and if the existing function should not evolve. New functions that can be naturally attached to an existing application : Some of the functions will naturally find their place as an evolution of an existing application; New functions that don't seem to be a natural evolution of the existing applications : However, some new functions have to be studied in order to determine their best spot in the IT systems (or their \"less worst\" spot). The two last steps can give birth to more advanced architecture studies. Covering this topic is not in the scope of this article, but we can insist on leading careful analysis, because function misplacement is a problem that will cause painful and costly problems for years if not for decades. All the functions that are not in existing applications should be gathered and analyzed as a whole to determine the opportunity of developing new applications.","title":"Step 3: Associate Functions to Applications"},{"location":"articles/portfolio/#step-4-analyze-the-roadmap-of-each-application-size-it-roughly-and-compare-with-the-skills-available","text":"Once all functions were assigned to applications, we can size them roughly and have a view of the roadmap of each application. For sure, this roadmap is still theoretical and the pure consequence of the customer's requirements.","title":"Step 4: Analyze the Roadmap of Each Application, Size it Roughly and Compare With The Skills Available"},{"location":"articles/portfolio/#the-case-of-application-maintenance-and-evolution","text":"All functions of an existing application can be prioritized in the best order of feasibility to create a first credible application roadmap, that is compliant with technical constraints. At this stage, we can also identify the functions that are nice to have compared to the ones that are must haves. This classification, even if it can be debated with the business may lead to some negotiation margins during the year to come. The skills available for each application is important because it will determine the real feasibility of the roadmap per application: We mean IT skills (project manager, product owner, developer, QA) but also business skills. Indeed, in that phase, we can identify the bottlenecks in terms of IT people, when not enough people are skilled to develop the expected application roadmap. Depending on the global priorities of the company, it is possible in that phase to plan people transfer, trainings or recruitments.","title":"The Case of Application Maintenance and Evolution"},{"location":"articles/portfolio/#the-case-of-new-applications","text":"Those projects are always more risky than the previous ones. In some cases, it is obvious for the customer that the new requirements will lead to a new application. In some other cases, it is not. Those cases must be dealt with carefully because the customer may argue that the new requirements are part of an existing application. In those cases, the objective is to avoid function misplacement. However, it is more difficult to create a new application: The project manager has to be found, and she/he has to be able to lead from-scratch projects; The team has to be found, or in the existing people (staff or consultants), or in new comers; The business expert(s) has to be named. Some business people may be a bit worried about the application creation projects because they cause HR troubles and they can face more delays or failures than evolution projects.","title":"The Case of New Applications"},{"location":"articles/portfolio/#end-of-the-phase","text":"At the end of the step 4, we should have: All functions identified; All application roadmap sized; All HR requirements.","title":"End of the Phase"},{"location":"articles/portfolio/#step-5-introduction-of-dependencies-to-create-project-visions","text":"To determine which projects to do first in the project portfolio, we need to take the reusable functions with the highest dependency number, in order to schedule them first. Then, the purpose is to play with the constraints to determine the less worst global path considering: The list of projects to develop; The skills available; The potential HR adjustments (people moving from one project to another, new people). Generally, the functions with the highest dependency number will drive the roadmap. That means that a function will be develop if and only if it is in the same project than a function with a high dependency number. That means also that generally a lot of new functions required by the business will not be able to be developed in the coming year, just because they are not part of an \"important\" project. Indeed, this method enables to show to the top level management what is important to the company throughout the functions that are required by many projects (i.e. that have the highest dependency numbers). Once everything is done, the exercise can be redone if the budget is bigger or lower in some parts.","title":"Step 5: Introduction of Dependencies to Create Project Visions"},{"location":"articles/portfolio/#limits-of-the-method","text":"We can argue that the functions with the highest dependency numbers may not be the most crucial functions. That can be true. We can use some tricks in using several levels of functions if needed. Indeed, a function (level 1) required by a certain project could be divided into 3 functions of a smaller granularity (level 2). The level 2 functions could be the functions implemented by the applications. With this model adjustment, we can have a high level view of the function with \"project functions\" (level 1) and an IT view of functions (level 2). The artifact type Function must then be able to aggregate a list of Function s. We let the reader update the meta-model.","title":"Limits of the Method"},{"location":"articles/portfolio/#what-tools-can-help","text":"","title":"What Tools Can Help?"},{"location":"articles/portfolio/#yed","text":"When this method was used for the first time, Yed was the tool used. A Yed model was realized per organization. The advantage of Yed is its simplicity and its powerful set of graph layout algorithms. The drawback of Yed is when the number of projects is big (we had to deal with the Digital Division and their 130 projects), the model is hardly usable.","title":"Yed"},{"location":"articles/portfolio/#emf-and-sirius","text":"Indeed, when the number of projects is high, a multiple view design tool is required. A possible option is to create and EMF model and to create the edition views with Sirius . You can also use MetaEdit+ that is a powerful metamodeler.","title":"EMF and Sirius"},{"location":"articles/portfolio/#roadmapping-with-archimate","text":"Archimate is probably the best language to realize this kind of portfolio management. We will explain in a bit more details a possible use of Archimate enterprise architecture modeling language for portfolio roadmapping in another article.","title":"Roadmapping With Archimate"},{"location":"articles/portfolio/#conclusion","text":"With this meta-model, it is possible to create and operate a feasible, logic and auditable project portfolio and to have explanations about the constraints and the process of deciding what project should be done in priority. Most often, this kind of tools pacifies the battleground. Even if projects can go wrong during the year to come, with this method, many problems will have been anticipated and will, as per magic, not occur during the year. ( January 2018 )","title":"Conclusion"},{"location":"articles/the-v2-vortex/","text":"The V2 Vortex A lot of companies enter, at a certain moment in their life, into the \"V2 Vortex\u201d. In this article, we're going to detail the pattern of the harsh replacement of the V1 of the application by the V2. The Case of Software Companies For a lot of software companies that enter into the V2 Vortex, the history is similar: the V1 of the product was created long ago, for instance in the late 90s or in the early 2000s. It was a success. The company found its market. It has customers. All is great except that in 15 years, technology changed. And so did the competition. In those software companies, we can often see a lack of software culture, and often, business-oriented people surfing on a tool that developed and gathered success. The R&D team is generally composed of IT people that preferred implementing new features for customers rather than making the development process change progressively to modernize slowly but surely the product. The problem looks like a pattern: after a large amount of time, even the best buildings get old when they are not maintained. With years, it becomes difficult to implement new features; evolutions are slower to much slower, and the software bugs are more and more complicated to fix [1]. Software is a complex activity because when things go wrong, the more customers, the more troubles, the more time and money spent in support, the less money to invest and the less time to fix. This is the dark side of the software moon, the one that enables selling plenty of times a software that was developed once. Multipliers work both ways. So, one day, the CEO has to face the facts. \"We need a V2\", he says. \u201cWe can invest, I have cash\u201d. That's when the trouble begins. Who Really Does The Maths? Does the company have the right skills? Enough people? The right money? As a consultant, I see that CEOs frequently don't run the maths, or maybe their calculator is bugged, or perhaps they are too optimistic. New trendy technologies make them dream their product can be rewritten in 2 years maximum (it's always 2 years). For sure, they think current technology is much more productive that the old one they keep on using. And new developers can do so many pretty web stuff. The argument I hear also a lot is that the current software was refactored several times and so its global costs is much lower that the R&D team multiplied by the age of the software (generally around 10 to 20 years). But, let's do the maths together: let's suppose the R&D development team was around 4 FTE during all those years. The gross product weight would be 60 men years (MY) for a team that worked 15 years on a product. Let's suppose 1/3 of the charge is not relevant and is corresponding to the refactorings. This leads to 40 useful MY. Globally, a good developer in Europe costs around 100 k\u20ac a year (with charges). That means that the software company should have approximately 4 M\u20ac available to realize this project. This investment is generally quite risky. 40 MY of effort is possible in two years provided you have a team of 30 people (taking ramp-ups into account). For sure, the CEO of the company (or the CTO) is used to manage only 4 R&D people. So he rarely has the (project) management skills for such a big project. Let's suppose he reduces the team and targets 10 people for 4 years, he still has to train them for them to learn the business and to understand the existing product in order to be productive. But where to find the right people? In IT service companies? This could be a bad move: IT service developer usually don't know how to produce good software; but they make it fast. Most CEOs will choose one of those options: Be optimistic and start with his 4 developers using new technologies. Things will probably not progressed as expected. Outsource because a service company can convince the CEO that with 1M\u20ac, you can create 2M\u20ac of software. Generally, the budget is consumed but the software is not produced (and not working). Believe software bandits or software magicians that will promise the CEO that with their code generation MDA approach, they can develop his V2 software in less than 2 years \u2013 provided he pays upfront. Develop a new product with a limited scope, for new customers, hoping to renew the old company success while fearing about the double team and the double recurring costs. But wouldn't it be a company startup within the company? Sell the company when it is still worth something and before it's too late. If we do the maths, we can be worried. Most of those CEOs will potentially try successively several options and fail. We can say they entered into the \"V2 Vortex\", a place where many software company go to die. Like the elephant cemetery, they cannot go back. Soon, they'll run out of cash and be obliged to find a more radical solution. The V2 Vortex in Big Companies In big companies, unfortunately, things are quite similar. Perhaps the application is much bigger (like a mainframe); perhaps it was not developed over 15 years but over 30; perhaps the development team is 100 people instead of 4; but the fact is here: they definitely are into the V2 Vortex. Doing the maths is really frightening on their side. 30 years with 100 FTE leads to 3,000 MY of effort. Let's apply the 1/3 optimistic ratio: the application weights 2,000 MY... For sure, perhaps they don't have the cash problem. But they have tried to replace their systems 3 or 4 times, and all projects failed. Indeed, they never really understood why. This is too scary to look into it. They also tried several options and spent a lot of cash, and the amount spent is at the size of the ambition. They, also, would like to get rid of the problem in 2 years, but that would mean 1,000 people in the project team... Nah, not possible. IT service companies are not a big help with this kind of problem. Those applications are core business ones and they are connected to dozens of systems inside and outside their company. It is quite hard to even grasp the full picture. They are in the V2 Vortex, and despite the fact that they're much bigger than the small software company we spoke about, the people in charge risk their job if they fail. There is No Magic I must say: there is no magic. I wish I could solve their problems with a lot of money but I cannot. However, there is a path - but they never like it. \u201cWhen you have applications with thousands of function points, you have to think about it as an asset\u201d, I say to them. Ok, it is old, they did not maintain it enough, it is bugged, they are locked in ancient design constraints and old technologies, yes, it is the same for everybody. But it is their asset. Most of time, they can still sell it to customers (one day, they won't), or the application is still massively used (because it is core business). It still solves business issues. It still runs everyday for tons of people that can hate it while not being able to live without it. So, as they would do with an old unmaintained asset that they have to keep, they have to modernize it. They cannot let it go. They have to do what they should have done, a long time ago. And it becomes urgent. Rewrite by Parts and Refactor New technologies brought new tools and new ways of managing legacy code. My advice is to upgrade to new tools, to new source management, to continuous build and integration, to pure local development workstations, to automated testing. The old code must absolutely be ported on recent compilers and OS. Surprisingly, after all the previous failures, their development process is often the same old unproductive non agile process. The hardest thing to do is to rewrite the software by parts, which implies code refactoring. The final users of the applications will have to cope with a hybrid product for years, with some modernized parts and some old ones, the time for the team in place to modernize it completely. Enterprise Architecture and Transition Architecture Building The Functional Map, the Map to Other Systems and the Transformation Steps Modernizing a complex software must be planned through the use of enterprise architecture techniques. As the system can be big in terms of functionality and/or can be connected to a lot of other systems, getting a complete map of the functionality and interfaces seems the best starting point. Archimate is the language for the task [2]. With Archimate, it is possible to represent the software progressive evolutions and the impacts on the user's processes. This is particularly important because some application modifications will have a serious impact on processes whereas others won't. In terms of software, it is hard to migrate the core of an old application right away. Generally, the peripheric functions will be the easiest to replace. Basically, those functions often use data that, in the database, are not lined to any other data (leaves of the tree of tables). By migrating function by function, and refactoring continuously the interfaces between the new part of the system and the old part of the system, the old part will become thinner while the new part will grow. Transition Architecture Technically, a transition architecture must be determined. The current one was supposed to be abandonned for a new one, but if we suppose the application will be composed by both technologies at the same time, it is crucial to have a generic way of moving functions from one environment to the other. Identifying what process is creating the data and what processes are consuming the data are important knowledge to get in order to do a smooth migration. The migration should enable to transfer the control of data from the old system to the new one progressively. Software is Made of People The HR can be a problem. Let's face it: it is not so easy to convert people that were used for years to work in a certain paradigm in another. Some people will adapt and some won't. Moreover, developing in that phase of the life of the application requires skills that are generally not present in the original team. Recruitment is generally getting on the critical path. Because software is made of people. If people are closed, the software will be closed; if they are open, it will be open. If their people are messy, the software will be a mess; if they are structured, so will their software. So, to create a great software, you need to hire great people. And, once you got them, you have to cherish them because bright people can be insecure. And you don't want to lose them. This change is hard is many context: In small companies, it can bring a lot of issues due to feelings towards people (generally the original CTO is part of the founders and should be replaced); In big companies, the application manager, even if he took in charge for decades an application, must generally be changed in order for new motivated people to take over and do the work. Tabula Rasa (Rewriting) Versus Transformation Transformation Is Not Popular In the software industry, we generally damage our software assets when maintaining them. Then comes a time when we must transform the big software. Software transformation and refactoring is not very popular, for a lot of reasons: It requires a lot of work on the existing system, to really analyze what it does, how it was designed, and what are his good and bad points; It requires to enter into the fucntional details of the business, which many software engineers are reluctant to do, because they are more interested by technology than by functional aspects; It implies mastering two environments for a long period of time, and one of them may be very old and not very appealing on a r\u00e9sum\u00e9; It implies thinking at the software and IT architecture levels (3 and 4), even in a single application, which is not possible for many people [3]. For all those reasons, most people and managers prefer the \"tabula rasa\" solution. But, if we do the maths, we see that it is very often not an option. The Rewriting Path The rewriting path is not impossible, and it is sometimes the only option. It can be a success (even if many rewriting projects fail) but the conditions are numerous for it to succeed: The budget must be very big, and so must be the business case to get a ROI in a reasonable delay; The rewriting project must take into account the migration from the old system to the new one, the earlier possible in the project; The architecture of the V2 product should respect the semantics of the business, because may old systems do; The team must be great and work closely with the old system one; Evolutions should be limited in the old system; The architecture of the new system must enable parallel developments. Conclusion After decades of projects and auditing, most of the companies I saw facing the V2 vortex should have considered the transformation path rather than the rewriting path. Notes [1] - We will come back on technical debt in other parts of this book. [2] - Archimate resources can be found here: http://www.opengroup.org/subjectareas/enterprise/archimate. [3] - Refer to The Five Levels Of Conceptual Maturity for IT Teams . ( June 2015, corrected November 2017 )","title":"The V2 Vortex"},{"location":"articles/the-v2-vortex/#the-v2-vortex","text":"A lot of companies enter, at a certain moment in their life, into the \"V2 Vortex\u201d. In this article, we're going to detail the pattern of the harsh replacement of the V1 of the application by the V2.","title":"The V2 Vortex"},{"location":"articles/the-v2-vortex/#the-case-of-software-companies","text":"For a lot of software companies that enter into the V2 Vortex, the history is similar: the V1 of the product was created long ago, for instance in the late 90s or in the early 2000s. It was a success. The company found its market. It has customers. All is great except that in 15 years, technology changed. And so did the competition. In those software companies, we can often see a lack of software culture, and often, business-oriented people surfing on a tool that developed and gathered success. The R&D team is generally composed of IT people that preferred implementing new features for customers rather than making the development process change progressively to modernize slowly but surely the product. The problem looks like a pattern: after a large amount of time, even the best buildings get old when they are not maintained. With years, it becomes difficult to implement new features; evolutions are slower to much slower, and the software bugs are more and more complicated to fix [1]. Software is a complex activity because when things go wrong, the more customers, the more troubles, the more time and money spent in support, the less money to invest and the less time to fix. This is the dark side of the software moon, the one that enables selling plenty of times a software that was developed once. Multipliers work both ways. So, one day, the CEO has to face the facts. \"We need a V2\", he says. \u201cWe can invest, I have cash\u201d. That's when the trouble begins.","title":"The Case of Software Companies"},{"location":"articles/the-v2-vortex/#who-really-does-the-maths","text":"Does the company have the right skills? Enough people? The right money? As a consultant, I see that CEOs frequently don't run the maths, or maybe their calculator is bugged, or perhaps they are too optimistic. New trendy technologies make them dream their product can be rewritten in 2 years maximum (it's always 2 years). For sure, they think current technology is much more productive that the old one they keep on using. And new developers can do so many pretty web stuff. The argument I hear also a lot is that the current software was refactored several times and so its global costs is much lower that the R&D team multiplied by the age of the software (generally around 10 to 20 years). But, let's do the maths together: let's suppose the R&D development team was around 4 FTE during all those years. The gross product weight would be 60 men years (MY) for a team that worked 15 years on a product. Let's suppose 1/3 of the charge is not relevant and is corresponding to the refactorings. This leads to 40 useful MY. Globally, a good developer in Europe costs around 100 k\u20ac a year (with charges). That means that the software company should have approximately 4 M\u20ac available to realize this project. This investment is generally quite risky. 40 MY of effort is possible in two years provided you have a team of 30 people (taking ramp-ups into account). For sure, the CEO of the company (or the CTO) is used to manage only 4 R&D people. So he rarely has the (project) management skills for such a big project. Let's suppose he reduces the team and targets 10 people for 4 years, he still has to train them for them to learn the business and to understand the existing product in order to be productive. But where to find the right people? In IT service companies? This could be a bad move: IT service developer usually don't know how to produce good software; but they make it fast. Most CEOs will choose one of those options: Be optimistic and start with his 4 developers using new technologies. Things will probably not progressed as expected. Outsource because a service company can convince the CEO that with 1M\u20ac, you can create 2M\u20ac of software. Generally, the budget is consumed but the software is not produced (and not working). Believe software bandits or software magicians that will promise the CEO that with their code generation MDA approach, they can develop his V2 software in less than 2 years \u2013 provided he pays upfront. Develop a new product with a limited scope, for new customers, hoping to renew the old company success while fearing about the double team and the double recurring costs. But wouldn't it be a company startup within the company? Sell the company when it is still worth something and before it's too late. If we do the maths, we can be worried. Most of those CEOs will potentially try successively several options and fail. We can say they entered into the \"V2 Vortex\", a place where many software company go to die. Like the elephant cemetery, they cannot go back. Soon, they'll run out of cash and be obliged to find a more radical solution.","title":"Who Really Does The Maths?"},{"location":"articles/the-v2-vortex/#the-v2-vortex-in-big-companies","text":"In big companies, unfortunately, things are quite similar. Perhaps the application is much bigger (like a mainframe); perhaps it was not developed over 15 years but over 30; perhaps the development team is 100 people instead of 4; but the fact is here: they definitely are into the V2 Vortex. Doing the maths is really frightening on their side. 30 years with 100 FTE leads to 3,000 MY of effort. Let's apply the 1/3 optimistic ratio: the application weights 2,000 MY... For sure, perhaps they don't have the cash problem. But they have tried to replace their systems 3 or 4 times, and all projects failed. Indeed, they never really understood why. This is too scary to look into it. They also tried several options and spent a lot of cash, and the amount spent is at the size of the ambition. They, also, would like to get rid of the problem in 2 years, but that would mean 1,000 people in the project team... Nah, not possible. IT service companies are not a big help with this kind of problem. Those applications are core business ones and they are connected to dozens of systems inside and outside their company. It is quite hard to even grasp the full picture. They are in the V2 Vortex, and despite the fact that they're much bigger than the small software company we spoke about, the people in charge risk their job if they fail.","title":"The V2 Vortex in Big Companies"},{"location":"articles/the-v2-vortex/#there-is-no-magic","text":"I must say: there is no magic. I wish I could solve their problems with a lot of money but I cannot. However, there is a path - but they never like it. \u201cWhen you have applications with thousands of function points, you have to think about it as an asset\u201d, I say to them. Ok, it is old, they did not maintain it enough, it is bugged, they are locked in ancient design constraints and old technologies, yes, it is the same for everybody. But it is their asset. Most of time, they can still sell it to customers (one day, they won't), or the application is still massively used (because it is core business). It still solves business issues. It still runs everyday for tons of people that can hate it while not being able to live without it. So, as they would do with an old unmaintained asset that they have to keep, they have to modernize it. They cannot let it go. They have to do what they should have done, a long time ago. And it becomes urgent.","title":"There is No Magic"},{"location":"articles/the-v2-vortex/#rewrite-by-parts-and-refactor","text":"New technologies brought new tools and new ways of managing legacy code. My advice is to upgrade to new tools, to new source management, to continuous build and integration, to pure local development workstations, to automated testing. The old code must absolutely be ported on recent compilers and OS. Surprisingly, after all the previous failures, their development process is often the same old unproductive non agile process. The hardest thing to do is to rewrite the software by parts, which implies code refactoring. The final users of the applications will have to cope with a hybrid product for years, with some modernized parts and some old ones, the time for the team in place to modernize it completely.","title":"Rewrite by Parts and Refactor"},{"location":"articles/the-v2-vortex/#enterprise-architecture-and-transition-architecture","text":"","title":"Enterprise Architecture and Transition Architecture"},{"location":"articles/the-v2-vortex/#building-the-functional-map-the-map-to-other-systems-and-the-transformation-steps","text":"Modernizing a complex software must be planned through the use of enterprise architecture techniques. As the system can be big in terms of functionality and/or can be connected to a lot of other systems, getting a complete map of the functionality and interfaces seems the best starting point. Archimate is the language for the task [2]. With Archimate, it is possible to represent the software progressive evolutions and the impacts on the user's processes. This is particularly important because some application modifications will have a serious impact on processes whereas others won't. In terms of software, it is hard to migrate the core of an old application right away. Generally, the peripheric functions will be the easiest to replace. Basically, those functions often use data that, in the database, are not lined to any other data (leaves of the tree of tables). By migrating function by function, and refactoring continuously the interfaces between the new part of the system and the old part of the system, the old part will become thinner while the new part will grow.","title":"Building The Functional Map, the Map to Other Systems and the Transformation Steps"},{"location":"articles/the-v2-vortex/#transition-architecture","text":"Technically, a transition architecture must be determined. The current one was supposed to be abandonned for a new one, but if we suppose the application will be composed by both technologies at the same time, it is crucial to have a generic way of moving functions from one environment to the other. Identifying what process is creating the data and what processes are consuming the data are important knowledge to get in order to do a smooth migration. The migration should enable to transfer the control of data from the old system to the new one progressively.","title":"Transition Architecture"},{"location":"articles/the-v2-vortex/#software-is-made-of-people","text":"The HR can be a problem. Let's face it: it is not so easy to convert people that were used for years to work in a certain paradigm in another. Some people will adapt and some won't. Moreover, developing in that phase of the life of the application requires skills that are generally not present in the original team. Recruitment is generally getting on the critical path. Because software is made of people. If people are closed, the software will be closed; if they are open, it will be open. If their people are messy, the software will be a mess; if they are structured, so will their software. So, to create a great software, you need to hire great people. And, once you got them, you have to cherish them because bright people can be insecure. And you don't want to lose them. This change is hard is many context: In small companies, it can bring a lot of issues due to feelings towards people (generally the original CTO is part of the founders and should be replaced); In big companies, the application manager, even if he took in charge for decades an application, must generally be changed in order for new motivated people to take over and do the work.","title":"Software is Made of People"},{"location":"articles/the-v2-vortex/#tabula-rasa-40rewriting41-versus-transformation","text":"","title":"Tabula Rasa (Rewriting) Versus Transformation"},{"location":"articles/the-v2-vortex/#transformation-is-not-popular","text":"In the software industry, we generally damage our software assets when maintaining them. Then comes a time when we must transform the big software. Software transformation and refactoring is not very popular, for a lot of reasons: It requires a lot of work on the existing system, to really analyze what it does, how it was designed, and what are his good and bad points; It requires to enter into the fucntional details of the business, which many software engineers are reluctant to do, because they are more interested by technology than by functional aspects; It implies mastering two environments for a long period of time, and one of them may be very old and not very appealing on a r\u00e9sum\u00e9; It implies thinking at the software and IT architecture levels (3 and 4), even in a single application, which is not possible for many people [3]. For all those reasons, most people and managers prefer the \"tabula rasa\" solution. But, if we do the maths, we see that it is very often not an option.","title":"Transformation Is Not Popular"},{"location":"articles/the-v2-vortex/#the-rewriting-path","text":"The rewriting path is not impossible, and it is sometimes the only option. It can be a success (even if many rewriting projects fail) but the conditions are numerous for it to succeed: The budget must be very big, and so must be the business case to get a ROI in a reasonable delay; The rewriting project must take into account the migration from the old system to the new one, the earlier possible in the project; The architecture of the V2 product should respect the semantics of the business, because may old systems do; The team must be great and work closely with the old system one; Evolutions should be limited in the old system; The architecture of the new system must enable parallel developments.","title":"The Rewriting Path"},{"location":"articles/the-v2-vortex/#conclusion","text":"After decades of projects and auditing, most of the companies I saw facing the V2 vortex should have considered the transformation path rather than the rewriting path.","title":"Conclusion"},{"location":"articles/the-v2-vortex/#notes","text":"[1] - We will come back on technical debt in other parts of this book. [2] - Archimate resources can be found here: http://www.opengroup.org/subjectareas/enterprise/archimate. [3] - Refer to The Five Levels Of Conceptual Maturity for IT Teams . ( June 2015, corrected November 2017 )","title":"Notes"},{"location":"articles/various-stages/","text":"The Various Stages of Digital Transformation Presentation done in Airbus Helicopters for the PMI France Chapter. Click to see the presentation . ( June 2015 )","title":"The Various Stages of Digital Transformation"},{"location":"articles/various-stages/#the-various-stages-of-digital-transformation","text":"Presentation done in Airbus Helicopters for the PMI France Chapter. Click to see the presentation . ( June 2015 )","title":"The Various Stages of Digital Transformation"},{"location":"graph/first-article/","text":"First article on graph-oriented programming In 2013, I started a personal project around directed attributed graph databases. The objective was to find a programming model that would enable the software to be as \"soft\" as the database. First works The concept was defined and setup during 2014 and 2015. Two articles on graph transformations were a crucial inspiration to the project. The AGG Aproach published in the volume 2 of the \"Handbook of Graph Grammars\". AGG was particularly inspiring. Ermel, Claudia, Michael Rudolf, and Gabriele Taentzer. \"The AGG approach: Language and environment.\" Handbook Of Graph Grammars And Computing By Graph Transformation: Volume 2: Applications, Languages and Tools. 1999. 551-603. Another article from the same book was also a source of inspiration, \"The Progres approach\" (which doesn't seem to be available online). Sch\u00fcrr, Andy, Andreas J. Winter, and Albert Z\u00fcndorf. \"The PROGRES approach: Language and environment.\" Handbook Of Graph Grammars And Computing By Graph Transformation: Volume 2: Applications, Languages and Tools. 1999. 487-550. Beginning of 2015, the graph-oriented programming concept was completed, and its capability of solving the useless technical debt was theoretically proven. The graph transformations are playing a core role in graph-oriented programming. Closed-source prototype In 2016, it was prototyped in a company called GraphApps in a closed source approach. I am no longer part of this company. Global article on graph-oriented programming A very first article was written in 2016 to explain all the concepts of this programming model. This article is far from being perfect and didactic but it browses a lot of topics that would deserve special attention (especially in the part concerning graph transformations). The original article can be found here: The graph-oriented programming paradigm . (July 2018)","title":"First article on graph-oriented programming"},{"location":"graph/first-article/#first-article-on-graph-oriented-programming","text":"In 2013, I started a personal project around directed attributed graph databases. The objective was to find a programming model that would enable the software to be as \"soft\" as the database.","title":"First article on graph-oriented programming"},{"location":"graph/first-article/#first-works","text":"The concept was defined and setup during 2014 and 2015. Two articles on graph transformations were a crucial inspiration to the project. The AGG Aproach published in the volume 2 of the \"Handbook of Graph Grammars\". AGG was particularly inspiring. Ermel, Claudia, Michael Rudolf, and Gabriele Taentzer. \"The AGG approach: Language and environment.\" Handbook Of Graph Grammars And Computing By Graph Transformation: Volume 2: Applications, Languages and Tools. 1999. 551-603. Another article from the same book was also a source of inspiration, \"The Progres approach\" (which doesn't seem to be available online). Sch\u00fcrr, Andy, Andreas J. Winter, and Albert Z\u00fcndorf. \"The PROGRES approach: Language and environment.\" Handbook Of Graph Grammars And Computing By Graph Transformation: Volume 2: Applications, Languages and Tools. 1999. 487-550. Beginning of 2015, the graph-oriented programming concept was completed, and its capability of solving the useless technical debt was theoretically proven. The graph transformations are playing a core role in graph-oriented programming.","title":"First works"},{"location":"graph/first-article/#closed-source-prototype","text":"In 2016, it was prototyped in a company called GraphApps in a closed source approach. I am no longer part of this company.","title":"Closed-source prototype"},{"location":"graph/first-article/#global-article-on-graph-oriented-programming","text":"A very first article was written in 2016 to explain all the concepts of this programming model. This article is far from being perfect and didactic but it browses a lot of topics that would deserve special attention (especially in the part concerning graph transformations). The original article can be found here: The graph-oriented programming paradigm . (July 2018)","title":"Global article on graph-oriented programming"},{"location":"graph/staf-icgt2018/","text":"Conference at the STAF/ICGT 2018 in Toulouse The prototyping works of the GraphApps company were stopped in December 2016 for economic reasons, the products (Design and Web Framework) staying closed source. However, I kept on working on the graph-oriented programming topic, in particular in the perspective of solving the technical debt. Marburg University During Fall 2017, some conference calls were organized with Prof. Dr. Gabriele Taentzer . Those discussions were about graph-oriented programming but also about Henshin . Beginning of 2018, Prof. Dr. Taentzer proposed that I was keynote speaker at the STAF/ICGT 2018 in Toulouse, conference organized by Dr. Leen Lambers and Prof. Dr. Jens Weber . I thank all of them for their invitation and support. Introductory papers For the conference, two introductory papers were produced: An abstract of the presentation for the Springer proceedings ; A non published introduction to graph-oriented programming. Slides of the presentation The slides of the presentation can be found hereafter: Introduction to graph-oriented programming A copy of those slides can also be found on the conference website here . What's next? My works on graph-oriented programming will go on, but with probably less time that I had in 2016. (July 2018)","title":"Conference at the STAF/ICGT 2018 in Toulouse"},{"location":"graph/staf-icgt2018/#conference-at-the-staficgt-2018-in-toulouse","text":"The prototyping works of the GraphApps company were stopped in December 2016 for economic reasons, the products (Design and Web Framework) staying closed source. However, I kept on working on the graph-oriented programming topic, in particular in the perspective of solving the technical debt.","title":"Conference at the STAF/ICGT 2018 in Toulouse"},{"location":"graph/staf-icgt2018/#marburg-university","text":"During Fall 2017, some conference calls were organized with Prof. Dr. Gabriele Taentzer . Those discussions were about graph-oriented programming but also about Henshin . Beginning of 2018, Prof. Dr. Taentzer proposed that I was keynote speaker at the STAF/ICGT 2018 in Toulouse, conference organized by Dr. Leen Lambers and Prof. Dr. Jens Weber . I thank all of them for their invitation and support.","title":"Marburg University"},{"location":"graph/staf-icgt2018/#introductory-papers","text":"For the conference, two introductory papers were produced: An abstract of the presentation for the Springer proceedings ; A non published introduction to graph-oriented programming.","title":"Introductory papers"},{"location":"graph/staf-icgt2018/#slides-of-the-presentation","text":"The slides of the presentation can be found hereafter: Introduction to graph-oriented programming A copy of those slides can also be found on the conference website here .","title":"Slides of the presentation"},{"location":"graph/staf-icgt2018/#whats-next","text":"My works on graph-oriented programming will go on, but with probably less time that I had in 2016. (July 2018)","title":"What's next?"},{"location":"research/DSL-for-graph-topology-checks/","text":"A DSL for checking the topology of the graph before transforming it Two kinds of graph transformations (GT) GTs that start with a root node. GTs that aim at making several modifications in the graph (generally using pattern matching expressions). We are focused on the first kind of GT. A DSL to chack the topology It is possible to create a DSL to check the basic applicability of a GT. The DSL can propose a set of primitives to make assertions on the graph, typically seen as starting at a certain root node (so we have an artefact for root node, let's says it is \"root\"). For instance root:A -[:]-> :B -[:R]-> :C means that the root node which is an instance of A should be connected through whatever relationship to an instance of B which is connected by a relationship of type R to an instance of C. This syntax is inspired by Open Cypher . In this DSL, it is possible to express complex checks like: ASSERT { ( RN:A -[:]-> x:B -[:R]-> :C ) AND ( ( x:B -[NOT]-> :D) OR ( (x:B <-[:]- y:E) AND ( y.att12 == 'foo') ) ) } The DSL could not manage: Variables, Attribute value checks. It would be a pure graph structure analysis. But, even if the topology conditions are met, if the DSL is too simple, we have to recode the graph navigation in the GT in order, for instance, to make a check on the instance of C. Advantages and drawbacks This DSL works. However, I am not entirely satisfied about it, in the same way that I am not completely satisfied about the rules description in general. Being a kind of pseudo-code (even if more generic), the DSL: Is a sort of new programming language; The frontier between what we express in this DSL and what must be in the code is vague; The topology checks are often depending on complex business conditions, which means that in real code, you may have topology conditions that you must code manually and that we cannot express really by this DSL. So, the DSL is reduced to a kind of \"helper\" that is addressing many cases in simple applications (or let's say in applications where the rules are simple), but cannot be used as a new \"structural way of coding\" graph transformations. Limitations This limitation opens the problem of theoretical check of GT applicabilities in case of GT forks (see the fork concept .","title":"A DSL for checking the topology of the graph before transforming it"},{"location":"research/DSL-for-graph-topology-checks/#a-dsl-for-checking-the-topology-of-the-graph-before-transforming-it","text":"","title":"A DSL for checking the topology of the graph before transforming it"},{"location":"research/DSL-for-graph-topology-checks/#two-kinds-of-graph-transformations-gt","text":"GTs that start with a root node. GTs that aim at making several modifications in the graph (generally using pattern matching expressions). We are focused on the first kind of GT.","title":"Two kinds of graph transformations (GT)"},{"location":"research/DSL-for-graph-topology-checks/#a-dsl-to-chack-the-topology","text":"It is possible to create a DSL to check the basic applicability of a GT. The DSL can propose a set of primitives to make assertions on the graph, typically seen as starting at a certain root node (so we have an artefact for root node, let's says it is \"root\"). For instance root:A -[:]-> :B -[:R]-> :C means that the root node which is an instance of A should be connected through whatever relationship to an instance of B which is connected by a relationship of type R to an instance of C. This syntax is inspired by Open Cypher . In this DSL, it is possible to express complex checks like: ASSERT { ( RN:A -[:]-> x:B -[:R]-> :C ) AND ( ( x:B -[NOT]-> :D) OR ( (x:B <-[:]- y:E) AND ( y.att12 == 'foo') ) ) } The DSL could not manage: Variables, Attribute value checks. It would be a pure graph structure analysis. But, even if the topology conditions are met, if the DSL is too simple, we have to recode the graph navigation in the GT in order, for instance, to make a check on the instance of C.","title":"A DSL to chack the topology"},{"location":"research/DSL-for-graph-topology-checks/#advantages-and-drawbacks","text":"This DSL works. However, I am not entirely satisfied about it, in the same way that I am not completely satisfied about the rules description in general. Being a kind of pseudo-code (even if more generic), the DSL: Is a sort of new programming language; The frontier between what we express in this DSL and what must be in the code is vague; The topology checks are often depending on complex business conditions, which means that in real code, you may have topology conditions that you must code manually and that we cannot express really by this DSL. So, the DSL is reduced to a kind of \"helper\" that is addressing many cases in simple applications (or let's say in applications where the rules are simple), but cannot be used as a new \"structural way of coding\" graph transformations.","title":"Advantages and drawbacks"},{"location":"research/DSL-for-graph-topology-checks/#limitations","text":"This limitation opens the problem of theoretical check of GT applicabilities in case of GT forks (see the fork concept .","title":"Limitations"},{"location":"research/basic-graph-transformations/","text":"Basic graph transformations This page is written in the context of multi-labeled nodes as they are implemented in standard graph databases. Node structure Basic Python structure features: Id with hash and eq function Type Domain Attributes are in a dictionary (real implementation of the multi-labelled graph, knowing that it is also the way Neo represents it) Questions: Link to a grammar or not Position semantics (in the case for instance we unserialize elements in a CSV file in which the line number is important relatively to the previous or the next one) Relationship Attributes dictionary also To be thought through: a graph based on arrays of neighborhood may be sufficient Graph Node dictionary -> Neighbors dictionary -> Dictionary of attribute links Unitary GTs GT have two modes: Destructive mode Copy mode Gt1: filter Remove void attributes/columns Gt2: a column is a type Change the type and remove the attribute Gt3: foreign key column Remove attribute and create a link Gt4: references hidden in a label Remove attribute and create a link Gt5 : references hidden towards a concept that does not exist Gt6 : link in the past PREVIOUS GT Interface graph, rootnode, options = GT(graph, rootnode, options) rootnode is in the graph options = {key : value, etc.} For instance: Gt1 (graph, None, { \"pattern\" : \"01001110\"}) Gt1 (graph, root, { \"depth\" : 3, \"pattern\" : \"0100110\"}) GT can forward parameters such as: CloneGt(None, root, params) Gt6(None, root, params) -> None, root, params clonegt(Gt6) -> None, root, params By forwarding the rest, we can compose GTs. Edited: better interface graph2, rootnode2 = GT1(graph1, rootnode1, options1) graph3, rootnode3 = GT2(*GT1(graph1, rootnode1, options1), option2) This enables to set the options at each stage. For sure, depending on the case, the graph or the rootnode may be None . Flexible interface: Graph, None => Enables to address all nodes issues Graph, root => Considers the graph from the root, meaning, we can request any node or relationship in the graph in complement than accessing the graph through the root node None, root => Or we consider a GT only touching the node, or the GT will access the graph through the root node only. Questions Deal with subgraphs?","title":"Basic graph transformations"},{"location":"research/basic-graph-transformations/#basic-graph-transformations","text":"This page is written in the context of multi-labeled nodes as they are implemented in standard graph databases.","title":"Basic graph transformations"},{"location":"research/basic-graph-transformations/#node-structure","text":"Basic Python structure features: Id with hash and eq function Type Domain Attributes are in a dictionary (real implementation of the multi-labelled graph, knowing that it is also the way Neo represents it) Questions: Link to a grammar or not Position semantics (in the case for instance we unserialize elements in a CSV file in which the line number is important relatively to the previous or the next one)","title":"Node structure"},{"location":"research/basic-graph-transformations/#relationship","text":"Attributes dictionary also To be thought through: a graph based on arrays of neighborhood may be sufficient","title":"Relationship"},{"location":"research/basic-graph-transformations/#graph","text":"Node dictionary -> Neighbors dictionary -> Dictionary of attribute links","title":"Graph"},{"location":"research/basic-graph-transformations/#unitary-gts","text":"GT have two modes: Destructive mode Copy mode Gt1: filter Remove void attributes/columns Gt2: a column is a type Change the type and remove the attribute Gt3: foreign key column Remove attribute and create a link Gt4: references hidden in a label Remove attribute and create a link Gt5 : references hidden towards a concept that does not exist Gt6 : link in the past PREVIOUS","title":"Unitary GTs"},{"location":"research/basic-graph-transformations/#gt-interface","text":"graph, rootnode, options = GT(graph, rootnode, options) rootnode is in the graph options = {key : value, etc.} For instance: Gt1 (graph, None, { \"pattern\" : \"01001110\"}) Gt1 (graph, root, { \"depth\" : 3, \"pattern\" : \"0100110\"}) GT can forward parameters such as: CloneGt(None, root, params) Gt6(None, root, params) -> None, root, params clonegt(Gt6) -> None, root, params By forwarding the rest, we can compose GTs. Edited: better interface graph2, rootnode2 = GT1(graph1, rootnode1, options1) graph3, rootnode3 = GT2(*GT1(graph1, rootnode1, options1), option2) This enables to set the options at each stage. For sure, depending on the case, the graph or the rootnode may be None . Flexible interface: Graph, None => Enables to address all nodes issues Graph, root => Considers the graph from the root, meaning, we can request any node or relationship in the graph in complement than accessing the graph through the root node None, root => Or we consider a GT only touching the node, or the GT will access the graph through the root node only.","title":"GT Interface"},{"location":"research/basic-graph-transformations/#questions","text":"Deal with subgraphs?","title":"Questions"},{"location":"research/basic-semantic-graph-transformations/","text":"Basic semantic graph transformations This page is following Basic graph transformations but with a semantic graph perspective. Rich/poor level of information A single triple is a \"poor piece of information\", but other triples with the same subjects can build a rich set of information. Poor piece of information: s p o . Rich piece of information: s p o ; a S . o a O . Time management Rich piece of information: adding time information: s p_23DEC2018 o ; a S . o a O . p_23DEC2018 a P . P a Time_Predicate . This is practical because s P o . can be deduced easily even if s p_23DEC2018 o . is more precise. The statement P a Time-Predicate . indicates that P is a time-enabled predicate. Version management We can have a variation of what we saw with version tagging. s p_V2 o ; a S . o a O . p_V2 a P . P a Version_Predicate . Managing life-cycle Case of subject modification and history keeping. Step 1. We have: s1 p o . Step 2: s1 becomes s2. We have: s1 p o . s2 previous s1 . s2 p o . // \"Rewiring\" This is ambiguous because the 3rd statement was made after the first. Let's use a time predicate. s1 p_12DEC2018 o . p_12DEC2018 a P . P a Time_Predicate . s2 previous s1 . s2 p_23DEC2018 o . // \"Rewiring\" p_23DEC2018 a P . We can look at the graph at various moments. Indeed, we still have: s1 P o . s2 previous s1 . s2 P o . // \"Rewiring\" But we also encoded a more precise information. We could think about removing s1 p o . after s2 is created but as the semantic web is a cumulative system, this does not seem very interesting. Shortcuts Let's consider the pattern: q = p(1) o p(2) o ... o p(n) with p(i) a set of predicates. s q a . if x(n) p(n) a . and x(n-1) p(n-1) x(n) . and .. and s p(1) x(2) . q id just a new \"predicate name\". This can be very useful to present the same reality in another perspective/point of view. Filters If we have: s p o ; q a . We can define a subgraph by \"removing\" the q predicate: graph(s , depth=1) \\ {q} => s p o . Classical inferences The use of classical inferences is very important also. Temporal inferences If we have: s p(t1) a . s p(t2) b . p(t1) a P . p(t2) a P . P a Time_Predicate . and a and b were not existing before the predicates were created, we could deduce: a before b . Not sure it is useful. Simpler like that: a2 previous a1 . a3 previous a2 . => a3 previous a1 . Some predicates can have special transitivity features. Special predicates Transitive predicates: a r b . and b r c . => a r c . Commutative predicates: a r b . => b r a . Semantic inversion (very used in Owl): p is the inverse of r , so if a r b . => b p a . To do Re-analyze the list notion in the RDF standard Re-analyze the Gruf temporal display Formalization of grammar in table parsing => grammar being a semantic graph List graph transformations in Sparql-like","title":"Basic semantic graph transformations"},{"location":"research/basic-semantic-graph-transformations/#basic-semantic-graph-transformations","text":"This page is following Basic graph transformations but with a semantic graph perspective.","title":"Basic semantic graph transformations"},{"location":"research/basic-semantic-graph-transformations/#richpoor-level-of-information","text":"A single triple is a \"poor piece of information\", but other triples with the same subjects can build a rich set of information. Poor piece of information: s p o . Rich piece of information: s p o ; a S . o a O .","title":"Rich/poor level of information"},{"location":"research/basic-semantic-graph-transformations/#time-management","text":"Rich piece of information: adding time information: s p_23DEC2018 o ; a S . o a O . p_23DEC2018 a P . P a Time_Predicate . This is practical because s P o . can be deduced easily even if s p_23DEC2018 o . is more precise. The statement P a Time-Predicate . indicates that P is a time-enabled predicate.","title":"Time management"},{"location":"research/basic-semantic-graph-transformations/#version-management","text":"We can have a variation of what we saw with version tagging. s p_V2 o ; a S . o a O . p_V2 a P . P a Version_Predicate .","title":"Version management"},{"location":"research/basic-semantic-graph-transformations/#managing-life-cycle","text":"Case of subject modification and history keeping. Step 1. We have: s1 p o . Step 2: s1 becomes s2. We have: s1 p o . s2 previous s1 . s2 p o . // \"Rewiring\" This is ambiguous because the 3rd statement was made after the first. Let's use a time predicate. s1 p_12DEC2018 o . p_12DEC2018 a P . P a Time_Predicate . s2 previous s1 . s2 p_23DEC2018 o . // \"Rewiring\" p_23DEC2018 a P . We can look at the graph at various moments. Indeed, we still have: s1 P o . s2 previous s1 . s2 P o . // \"Rewiring\" But we also encoded a more precise information. We could think about removing s1 p o . after s2 is created but as the semantic web is a cumulative system, this does not seem very interesting.","title":"Managing life-cycle"},{"location":"research/basic-semantic-graph-transformations/#shortcuts","text":"Let's consider the pattern: q = p(1) o p(2) o ... o p(n) with p(i) a set of predicates. s q a . if x(n) p(n) a . and x(n-1) p(n-1) x(n) . and .. and s p(1) x(2) . q id just a new \"predicate name\". This can be very useful to present the same reality in another perspective/point of view.","title":"Shortcuts"},{"location":"research/basic-semantic-graph-transformations/#filters","text":"If we have: s p o ; q a . We can define a subgraph by \"removing\" the q predicate: graph(s , depth=1) \\ {q} => s p o .","title":"Filters"},{"location":"research/basic-semantic-graph-transformations/#classical-inferences","text":"The use of classical inferences is very important also.","title":"Classical inferences"},{"location":"research/basic-semantic-graph-transformations/#temporal-inferences","text":"If we have: s p(t1) a . s p(t2) b . p(t1) a P . p(t2) a P . P a Time_Predicate . and a and b were not existing before the predicates were created, we could deduce: a before b . Not sure it is useful. Simpler like that: a2 previous a1 . a3 previous a2 . => a3 previous a1 . Some predicates can have special transitivity features.","title":"Temporal inferences"},{"location":"research/basic-semantic-graph-transformations/#special-predicates","text":"Transitive predicates: a r b . and b r c . => a r c . Commutative predicates: a r b . => b r a . Semantic inversion (very used in Owl): p is the inverse of r , so if a r b . => b p a .","title":"Special predicates"},{"location":"research/basic-semantic-graph-transformations/#to-do","text":"Re-analyze the list notion in the RDF standard Re-analyze the Gruf temporal display Formalization of grammar in table parsing => grammar being a semantic graph List graph transformations in Sparql-like","title":"To do"},{"location":"research/data-mig/","text":"Semantic data migration project Presentation You can read the following article : Using semantic web technologies for aerospace industrial data migration, Madics 2019 Elements Why? Arguments for using the semantic graph technology to complex data conversion: It is relatively easy to turn whatever table into sets of triples. The semantic of each column must be named and reused as a \"semantic dictionary\" between the multiples sources. This step is much easier to accomplish than in standard data lakes where the complete big definitions must be set in one single movement. The design actions, that were at the heart of the problems in GraphApps , are less important and can be limited to a correct understanding of the data. The semantic databases (For instance AllegroGraph or Apache Jena) are working in an incremental way: if a triple already exists, the attempt to import it again will do nothing, which \"by design\" eliminates redundant information. Sparql enables easy graph transformations, first of all to visualize data (Gruff on AllegroGraph is a good triplestore visualization tool), and then to transform them. Timed of life-cycled data Data versions (being time-based versions or life-cycle versions) can be managed with a link timestamp or a link version stamp. The timestamp relation will have to be a rdfs:SubClassOf the theoretical link type. Basic semantic graph transformation (in work) In the context of graph transformations , the page Basic semantic graph transformations aims at defining a set of basic graph transformations. Industry data The page Graphs and semantic data in industry talks about the opportunity of using RDF approaches to convert efficiently industrial data. (Last update: June 2020)","title":"Semantic data migration project page"},{"location":"research/data-mig/#semantic-data-migration-project","text":"","title":"Semantic data migration project"},{"location":"research/data-mig/#presentation","text":"You can read the following article : Using semantic web technologies for aerospace industrial data migration, Madics 2019","title":"Presentation"},{"location":"research/data-mig/#elements","text":"","title":"Elements"},{"location":"research/data-mig/#why","text":"Arguments for using the semantic graph technology to complex data conversion: It is relatively easy to turn whatever table into sets of triples. The semantic of each column must be named and reused as a \"semantic dictionary\" between the multiples sources. This step is much easier to accomplish than in standard data lakes where the complete big definitions must be set in one single movement. The design actions, that were at the heart of the problems in GraphApps , are less important and can be limited to a correct understanding of the data. The semantic databases (For instance AllegroGraph or Apache Jena) are working in an incremental way: if a triple already exists, the attempt to import it again will do nothing, which \"by design\" eliminates redundant information. Sparql enables easy graph transformations, first of all to visualize data (Gruff on AllegroGraph is a good triplestore visualization tool), and then to transform them.","title":"Why?"},{"location":"research/data-mig/#timed-of-life-cycled-data","text":"Data versions (being time-based versions or life-cycle versions) can be managed with a link timestamp or a link version stamp. The timestamp relation will have to be a rdfs:SubClassOf the theoretical link type.","title":"Timed of life-cycled data"},{"location":"research/data-mig/#basic-semantic-graph-transformation-in-work","text":"In the context of graph transformations , the page Basic semantic graph transformations aims at defining a set of basic graph transformations.","title":"Basic semantic graph transformation (in work)"},{"location":"research/data-mig/#industry-data","text":"The page Graphs and semantic data in industry talks about the opportunity of using RDF approaches to convert efficiently industrial data. (Last update: June 2020)","title":"Industry data"},{"location":"research/grammar-graph-transformation/","text":"Grammar graph transformation Graphs in general, and semantic graphs in particular, can be used to migrate very efficiently data from one format to another. Grammar By grammar, we mean several possible things: A description of the schema of a RDBMS, especially of tables, specifying the type of each table and the types of each columns. In that case, the grammar corresponds to the information contained in the database schema. The grammar that we can have to generate triples from a structured table format source. See the page on industry data for a sample. In that page, the description of the types of all data in a particular semantic space can be called the \"grammar\" of the semantic space. Transforming grammars with reusable basic graph transformations Grammars can be expressed in graphs. Let's take the very simple example of the following RDMS schema (see Figure 1). Figure 1: Simple grammar for a table This grammar can be expressed in basic RDF. T1 a Table . T1 attribute C1 . C1 SubClassOf Type1 . ... Tj attribute Cj . Cj SubClassOf Typej . ... Tn attribute Cn . Cn SubClassOf Typen . Basic RDBMS schema transformation in a multi-attributes graph database The following transformations are basic RDBMS schema transformations. Note that graph transformations are not commutative and their order is important. Transforming foreign key into a relationship Let's suppose Ck is a foreign key of Table1 to Table2 , referencing column Kl . # Before T1 attribute Ck . Ck SubClassOf T2 . T2 attribute Kl . Kl SubClassOf Typel . # Grammar graph transformation 1 Transform_foreign_key { # 1. Link the two table colums with a particular semantic predicate # 2. Remove Ck attribute from T1 } # After T1 foreign_key T2 . T2 attribute Kl . Kl SubClassOf Typel . The predicate foreign_key will enable real data transformation later. Simple class split and simple class merge Those graph transformations are exploiting the 1-1 relationship. In case of split, semantically the table T aggregates 2 concepts that are implicitly linked together. It is possible to reorder the attributes in that case. Let's consider a table T with 4 attributes C1 , C2 , C3 and C4 . The graph transformation will create a new resource T' and use a new semantic predicate (here linked_to ) so that: # Before T attribute C1 . T attribute C2 . T attribute C3 . T attribute C4 . # Grammar graph transformation 2 Split_class { # 1. Create the new resource type for T' (here a Table but could be some other semantic class) # 2. Link to source T # 3. Get attributes of T # 4. Remove attributes from T } # After T attribute C1 . T attribute C2 . T linked_to T' . T' a Table . T' attribute C3 . T' attribute C4 . Symmetrically, we have the merge. # Before T attribute C1 . T attribute C2 . T' attribute C3 . T' attribute C4 . # Grammar graph transformation 3 Merge_classes { # 1. Get attributes of T' to wire them in T # 2. Remove attributes from T' # 3. Remove T' } # After T attribute C1 . T attribute C2 . T attribute C3 . T attribute C4 . For sure, if T' is removed, we have to ensure before that, in the semantic space, the type is not required elsewhere. Complex class split One problem of table design is that we have sometimes different types hiding in the same table. Those cases are frequent in RDBMS and can be due to: A compromise between the object oriented approach and the RDMS (see Figure 2); Ease of code reuse for some different concepts. Note: Those practices generate a lot of technical debt and may paralyze the evolution of a software. Figure 2: Many types hiding in a table The class split will exhibit new concepts Type1 , Type2 and Type3 , all those concepts not having anymore the attribute of type indication ( C2 in Figure 2). The transformation will have to manage the various values in C2 and, depending on those values, choose what attributes to move from one resource to another. Non destructiveness and changing the semantic domain If we don't want to be destructive on the original grammar, we might want to contextualize the grammar to a semantic space describing the source world (here the RDBMS world). This could be done by using a specific URI prefix, like rdbms . The consequence is that we need basic transformations to transform rdbms:Table artifact into graph:Concept artifact, and rdms:attribute into graph:attribute . Those transformations are like \"translations\". Figure 3: Various levels of graph transformations In that case, we will have global (inter-semantic domain) graph transformations for translations, and intra grammar semantic space (graph) for the various transformations we just saw before. Using the time approach described in the basic semantic graph transformation page , we could imagine chaining the graph transformations for the global transformation of the RDBMS model into a richer, \"re-semantized\", version of the model. From that chain of graph transformations, we can generate code to transform the real data from the grammar transformation process. Complementary readings: Basic semantic graph transformations Basic graph transformations Page under the GNU FDL license.","title":"Grammar graph transformation"},{"location":"research/grammar-graph-transformation/#grammar-graph-transformation","text":"Graphs in general, and semantic graphs in particular, can be used to migrate very efficiently data from one format to another.","title":"Grammar graph transformation"},{"location":"research/grammar-graph-transformation/#grammar","text":"By grammar, we mean several possible things: A description of the schema of a RDBMS, especially of tables, specifying the type of each table and the types of each columns. In that case, the grammar corresponds to the information contained in the database schema. The grammar that we can have to generate triples from a structured table format source. See the page on industry data for a sample. In that page, the description of the types of all data in a particular semantic space can be called the \"grammar\" of the semantic space.","title":"Grammar"},{"location":"research/grammar-graph-transformation/#transforming-grammars-with-reusable-basic-graph-transformations","text":"Grammars can be expressed in graphs. Let's take the very simple example of the following RDMS schema (see Figure 1). Figure 1: Simple grammar for a table This grammar can be expressed in basic RDF. T1 a Table . T1 attribute C1 . C1 SubClassOf Type1 . ... Tj attribute Cj . Cj SubClassOf Typej . ... Tn attribute Cn . Cn SubClassOf Typen .","title":"Transforming grammars with reusable basic graph transformations"},{"location":"research/grammar-graph-transformation/#basic-rdbms-schema-transformation-in-a-multi-attributes-graph-database","text":"The following transformations are basic RDBMS schema transformations. Note that graph transformations are not commutative and their order is important.","title":"Basic RDBMS schema transformation in a multi-attributes graph database"},{"location":"research/grammar-graph-transformation/#transforming-foreign-key-into-a-relationship","text":"Let's suppose Ck is a foreign key of Table1 to Table2 , referencing column Kl . # Before T1 attribute Ck . Ck SubClassOf T2 . T2 attribute Kl . Kl SubClassOf Typel . # Grammar graph transformation 1 Transform_foreign_key { # 1. Link the two table colums with a particular semantic predicate # 2. Remove Ck attribute from T1 } # After T1 foreign_key T2 . T2 attribute Kl . Kl SubClassOf Typel . The predicate foreign_key will enable real data transformation later.","title":"Transforming foreign key into a relationship"},{"location":"research/grammar-graph-transformation/#simple-class-split-and-simple-class-merge","text":"Those graph transformations are exploiting the 1-1 relationship. In case of split, semantically the table T aggregates 2 concepts that are implicitly linked together. It is possible to reorder the attributes in that case. Let's consider a table T with 4 attributes C1 , C2 , C3 and C4 . The graph transformation will create a new resource T' and use a new semantic predicate (here linked_to ) so that: # Before T attribute C1 . T attribute C2 . T attribute C3 . T attribute C4 . # Grammar graph transformation 2 Split_class { # 1. Create the new resource type for T' (here a Table but could be some other semantic class) # 2. Link to source T # 3. Get attributes of T # 4. Remove attributes from T } # After T attribute C1 . T attribute C2 . T linked_to T' . T' a Table . T' attribute C3 . T' attribute C4 . Symmetrically, we have the merge. # Before T attribute C1 . T attribute C2 . T' attribute C3 . T' attribute C4 . # Grammar graph transformation 3 Merge_classes { # 1. Get attributes of T' to wire them in T # 2. Remove attributes from T' # 3. Remove T' } # After T attribute C1 . T attribute C2 . T attribute C3 . T attribute C4 . For sure, if T' is removed, we have to ensure before that, in the semantic space, the type is not required elsewhere.","title":"Simple class split and simple class merge"},{"location":"research/grammar-graph-transformation/#complex-class-split","text":"One problem of table design is that we have sometimes different types hiding in the same table. Those cases are frequent in RDBMS and can be due to: A compromise between the object oriented approach and the RDMS (see Figure 2); Ease of code reuse for some different concepts. Note: Those practices generate a lot of technical debt and may paralyze the evolution of a software. Figure 2: Many types hiding in a table The class split will exhibit new concepts Type1 , Type2 and Type3 , all those concepts not having anymore the attribute of type indication ( C2 in Figure 2). The transformation will have to manage the various values in C2 and, depending on those values, choose what attributes to move from one resource to another.","title":"Complex class split"},{"location":"research/grammar-graph-transformation/#non-destructiveness-and-changing-the-semantic-domain","text":"If we don't want to be destructive on the original grammar, we might want to contextualize the grammar to a semantic space describing the source world (here the RDBMS world). This could be done by using a specific URI prefix, like rdbms . The consequence is that we need basic transformations to transform rdbms:Table artifact into graph:Concept artifact, and rdms:attribute into graph:attribute . Those transformations are like \"translations\". Figure 3: Various levels of graph transformations In that case, we will have global (inter-semantic domain) graph transformations for translations, and intra grammar semantic space (graph) for the various transformations we just saw before. Using the time approach described in the basic semantic graph transformation page , we could imagine chaining the graph transformations for the global transformation of the RDBMS model into a richer, \"re-semantized\", version of the model. From that chain of graph transformations, we can generate code to transform the real data from the grammar transformation process. Complementary readings: Basic semantic graph transformations Basic graph transformations Page under the GNU FDL license.","title":"Non destructiveness and changing the semantic domain"},{"location":"research/graph-oriented-pl/","text":"Graph-Oriented programming language This document is gathering some reflections about what the syntax of a programming language could be. Constructs (graph local lg local-graph) (graph remote rg remote-graph url user pwd)","title":"Graph-Oriented programming language"},{"location":"research/graph-oriented-pl/#graph-oriented-programming-language","text":"This document is gathering some reflections about what the syntax of a programming language could be.","title":"Graph-Oriented programming language"},{"location":"research/graph-oriented-pl/#constructs","text":"(graph local lg local-graph) (graph remote rg remote-graph url user pwd)","title":"Constructs"},{"location":"research/graph-transfo/","text":"Graph transformations Graph transformation is an essential concept to understand to understand those projects. Signature The graph transformations signature is homogeneous: Let g be a graph transformation. g(G) = G' If G &sub; G', then g is said to be non destructive. If G &nsub; G', then g is said to be destructive. Implementation notes a. A root node is often very practical to associate the applicability of a transformation from a certain root node. b.Domain and range of graph transformations are not so easily defined (lots of works in the mathematical fields on that topic). In order to create a workable model, we will define the special value NOT_APPLICABLE . Consequently, we can write: g(G1) = NOT_APPLICABLE &forall; g, graph transformation, g(NOT_APPLICABLE) = NOT_APPLICABLE Graph transformations can be composed Let g and h graph transformations. g o h and h o g are graph transformations. This defines: The basis of reusability; The possible existence of a \"graph transformation base\" of basic graph transformations. Basic set of graph transformations See the special pages: Basic graph transformations Basic semantic graph transformations Applicability of graph transformations See: DSL for graph topology check Graph transformation applicability Note: to be declined in Sparql. Grammar for graph transformation In the context of the page on industry data , please have a look at the grammar of graph transformation . (Last update: June 2020)","title":"Graph transformations"},{"location":"research/graph-transfo/#graph-transformations","text":"Graph transformation is an essential concept to understand to understand those projects.","title":"Graph transformations"},{"location":"research/graph-transfo/#signature","text":"The graph transformations signature is homogeneous: Let g be a graph transformation. g(G) = G' If G &sub; G', then g is said to be non destructive. If G &nsub; G', then g is said to be destructive. Implementation notes a. A root node is often very practical to associate the applicability of a transformation from a certain root node. b.Domain and range of graph transformations are not so easily defined (lots of works in the mathematical fields on that topic). In order to create a workable model, we will define the special value NOT_APPLICABLE . Consequently, we can write: g(G1) = NOT_APPLICABLE &forall; g, graph transformation, g(NOT_APPLICABLE) = NOT_APPLICABLE","title":"Signature"},{"location":"research/graph-transfo/#graph-transformations-can-be-composed","text":"Let g and h graph transformations. g o h and h o g are graph transformations. This defines: The basis of reusability; The possible existence of a \"graph transformation base\" of basic graph transformations.","title":"Graph transformations can be composed"},{"location":"research/graph-transfo/#basic-set-of-graph-transformations","text":"See the special pages: Basic graph transformations Basic semantic graph transformations","title":"Basic set of graph transformations"},{"location":"research/graph-transfo/#applicability-of-graph-transformations","text":"See: DSL for graph topology check Graph transformation applicability Note: to be declined in Sparql.","title":"Applicability of graph transformations"},{"location":"research/graph-transfo/#grammar-for-graph-transformation","text":"In the context of the page on industry data , please have a look at the grammar of graph transformation . (Last update: June 2020)","title":"Grammar for graph transformation"},{"location":"research/graph-transformation-applicability/","text":"Graph transformation applicability The note about the DSL opens the problem of GT applicability. This is a theoretical problem but rather interesting. We are in the context of GTs that are attached to root nodes. Dictionary of applicable GTs per type It is possible to record in a dictionary the list of GTs applicable from a certain root node type. Once recorded, we can find the rules that enable to determine if each of the GT, that is potentially applicable is really applicable. Position of the applicability code When a GT is created and then forked, we can consider we have 2 versions of the GT: GTv1 and GTv2 for instance. GTv1 will be at the beginning applicable forever until it is forked. There seem to be 3 cases: The forking of the GT is necessary when there is a topological change, and in that case, the GTv2 will not apply to the samle topology than GTv1 . In a way, the applicability information is encoded into the graph topology itself. There are unfortunately many cases where GTv2 exists because the rule changes but not the topology. We identified this case as being difficult, because there is no real standard rule to decide if we had to fork or not. If we did, the 2 versions of the GT may apply at the same time, if we consider their topology checks only (that are the same in that case). The 2 cases are the following: We encode the applicability knowledge into the GT, Or we encode the applicability knowledge outside the GT. The first case will require a possible modification of the GTv1 when GTv2 appears, because it will need to chack its version or a certain date of applicability from which it becomes not applicable. In the second case, we can imagine a controler that knows what rule to apply considering, for instance, the age of data we are looking for. In that case, that would be, for instance, the age of the root node that would be a good first indicator to determine what version is applicable. Heterogeneous encoding of knowledge The core problem of the graph-oriented programming works was the placement of the rules in transient structures representing the graph. Here also, we have the same problem. It is not easy to encode a certain knowledge right inside the various artifacts that we have. Quite often, this is a design choice, which implies that we cannot find a rule that answers to all cases. Knowledge adequate/optimized/relevant encoding is very complex. Maybe the GT should be analyzed in semantic graphs that are simpler than directed multi-labeled graphs.","title":"Graph transformation applicability"},{"location":"research/graph-transformation-applicability/#graph-transformation-applicability","text":"The note about the DSL opens the problem of GT applicability. This is a theoretical problem but rather interesting. We are in the context of GTs that are attached to root nodes.","title":"Graph transformation applicability"},{"location":"research/graph-transformation-applicability/#dictionary-of-applicable-gts-per-type","text":"It is possible to record in a dictionary the list of GTs applicable from a certain root node type. Once recorded, we can find the rules that enable to determine if each of the GT, that is potentially applicable is really applicable.","title":"Dictionary of applicable GTs per type"},{"location":"research/graph-transformation-applicability/#position-of-the-applicability-code","text":"When a GT is created and then forked, we can consider we have 2 versions of the GT: GTv1 and GTv2 for instance. GTv1 will be at the beginning applicable forever until it is forked. There seem to be 3 cases: The forking of the GT is necessary when there is a topological change, and in that case, the GTv2 will not apply to the samle topology than GTv1 . In a way, the applicability information is encoded into the graph topology itself. There are unfortunately many cases where GTv2 exists because the rule changes but not the topology. We identified this case as being difficult, because there is no real standard rule to decide if we had to fork or not. If we did, the 2 versions of the GT may apply at the same time, if we consider their topology checks only (that are the same in that case). The 2 cases are the following: We encode the applicability knowledge into the GT, Or we encode the applicability knowledge outside the GT. The first case will require a possible modification of the GTv1 when GTv2 appears, because it will need to chack its version or a certain date of applicability from which it becomes not applicable. In the second case, we can imagine a controler that knows what rule to apply considering, for instance, the age of data we are looking for. In that case, that would be, for instance, the age of the root node that would be a good first indicator to determine what version is applicable.","title":"Position of the applicability code"},{"location":"research/graph-transformation-applicability/#heterogeneous-encoding-of-knowledge","text":"The core problem of the graph-oriented programming works was the placement of the rules in transient structures representing the graph. Here also, we have the same problem. It is not easy to encode a certain knowledge right inside the various artifacts that we have. Quite often, this is a design choice, which implies that we cannot find a rule that answers to all cases. Knowledge adequate/optimized/relevant encoding is very complex. Maybe the GT should be analyzed in semantic graphs that are simpler than directed multi-labeled graphs.","title":"Heterogeneous encoding of knowledge"},{"location":"research/graphapps/","text":"GraphApps project What is GraphApps? GraphApps was a research project (2013-2018) that aims to use graphs and graph transformations in the context of software engineering, like application building. It was focused on attributed directed graphs. Graph-oriented programming and technical debt You can read the following articles: First article on graph oriented programming Slides from the ICGT 2018 conference The code that was produced during this period is under copyright and so, unfortunately, this code is not available. Important points Those works brought several results. Object-oriented and RDBMS-based software engineering generates a lot of couplings (structural and temporal). Those couplings are, for us, at the center of the technical debt problem. The way we represent knowledge in current software engineering is largely sub-optimal , and we believe that the technical debt is a problem created by bad engineering practices and tools (OOP/RDBMS) and not attached to the business semantics. A new programming model called \"graph-oriented programming\" enables to limit the technical debt to its minimal expression (semantic couplings). This programming model is using: Graph databases (attributed directed graph databases); Graph transformations to model business logic. This programming model is an intermediate between object-orientation and functional programming. Shortly said, it takes the best out of the two programming models while being totally consistent because based on the business semantics and not on technical considerations. (June 2020) Second period (2018-now): Exploring (semantic) graph transformations for data conversion Basic graph transformations As graph transformations can be composed easily, the idea is to build a set of basic graph transformations as a foundation for applications. Two repos are currently under development to study those dimensions: The graph repo: The paper Basic graph transformations explains the intention of this repo. This repo is in Python. See also a page in the repo . The graphappsjs repo, which is targeting the same objective but analyzes the link of the programming language in that problem. The underlying topic under those repos is also the capability of building a graph-oriented programming language with adapted constructs. A direction to explore would be to propotype a DSL of a graph-oriented programming language with Common Lisp. Semantic data conversion draft papers The following draft papers are exploring the semantic data conversion approach: The article Arguments for semantic use in data conversion gathers the various reasons why using a semantic approach can be interesting. The paper Basic semantic graph transformations aims at defining a set of basic graph transformations. The paper Graphs and semantic data in industry talks about the opportunity of using RDF approaches to convert efficiently industrial data. Graph data visualization Converting CSV data into RDF Before being able to use semantic graph transformation on data, we must be able to convert CSV data to RDF. This is the objective of the csv2rdf.py tool available in the csv2rdf repo. There are two options proposed by this tool to convert CSV data: The default conversion option, The semantic grammar option, which enables to determine precisely the triple construction. By reusing the same semantic grammar, multiple sources of data can be gathered in the same semantic database (current tests are using Apache Jena ). Visualizing RDF data Per se RDF data are quite complicated to represent because of the hierachy of predicates. The following tools do not consider this hierarchy and generate one new edge object per predicate, while generating only one node for the subject and object across the full set of triples. The rdfviz repo contains two options of visualization: GML conversion, to be imported in tools like Yed or Cytoscape ; dot conversion to be displayed with GraphViz . Exploring RDF graph data through neighborhoods In the original GraphApps project, a lot of efforts were put in the use of node neighborhood as a basic element of knowledge navigation. The ontovisu repo aims at enabling the neighborhood-based visual navigation in RDF graph data. It is based on : Node , Apache Jena ), Cytoscape js graph library . Development of this repo is ongoing. Various notes linked to graphs Using graph transformations at the grammar level, the case of RDBMS schema transformation for re-semantization: Grammar graph transformations A DSL to check the graph topology Graph transformation applicability Also ongoing: it is quite difficult to properly design in the context of RDF/RDFS (without even talking Owl). This ongoing paper RDF design patterns is a first attempt. Notes on Description Logic See also The papers on orey.github.io . Those documents are available under the GNU FDL license.","title":"GraphApps project page"},{"location":"research/graphapps/#graphapps-project","text":"","title":"GraphApps project"},{"location":"research/graphapps/#what-is-graphapps","text":"GraphApps was a research project (2013-2018) that aims to use graphs and graph transformations in the context of software engineering, like application building. It was focused on attributed directed graphs.","title":"What is GraphApps?"},{"location":"research/graphapps/#graph-oriented-programming-and-technical-debt","text":"You can read the following articles: First article on graph oriented programming Slides from the ICGT 2018 conference The code that was produced during this period is under copyright and so, unfortunately, this code is not available.","title":"Graph-oriented programming and technical debt"},{"location":"research/graphapps/#important-points","text":"Those works brought several results. Object-oriented and RDBMS-based software engineering generates a lot of couplings (structural and temporal). Those couplings are, for us, at the center of the technical debt problem. The way we represent knowledge in current software engineering is largely sub-optimal , and we believe that the technical debt is a problem created by bad engineering practices and tools (OOP/RDBMS) and not attached to the business semantics. A new programming model called \"graph-oriented programming\" enables to limit the technical debt to its minimal expression (semantic couplings). This programming model is using: Graph databases (attributed directed graph databases); Graph transformations to model business logic. This programming model is an intermediate between object-orientation and functional programming. Shortly said, it takes the best out of the two programming models while being totally consistent because based on the business semantics and not on technical considerations. (June 2020)","title":"Important points"},{"location":"research/graphapps/#second-period-2018-now-exploring-semantic-graph-transformations-for-data-conversion","text":"","title":"Second period (2018-now): Exploring (semantic) graph transformations for data conversion"},{"location":"research/graphapps/#basic-graph-transformations","text":"As graph transformations can be composed easily, the idea is to build a set of basic graph transformations as a foundation for applications. Two repos are currently under development to study those dimensions: The graph repo: The paper Basic graph transformations explains the intention of this repo. This repo is in Python. See also a page in the repo . The graphappsjs repo, which is targeting the same objective but analyzes the link of the programming language in that problem. The underlying topic under those repos is also the capability of building a graph-oriented programming language with adapted constructs. A direction to explore would be to propotype a DSL of a graph-oriented programming language with Common Lisp.","title":"Basic graph transformations"},{"location":"research/graphapps/#semantic-data-conversion-draft-papers","text":"The following draft papers are exploring the semantic data conversion approach: The article Arguments for semantic use in data conversion gathers the various reasons why using a semantic approach can be interesting. The paper Basic semantic graph transformations aims at defining a set of basic graph transformations. The paper Graphs and semantic data in industry talks about the opportunity of using RDF approaches to convert efficiently industrial data.","title":"Semantic data conversion draft papers"},{"location":"research/graphapps/#graph-data-visualization","text":"","title":"Graph data visualization"},{"location":"research/graphapps/#converting-csv-data-into-rdf","text":"Before being able to use semantic graph transformation on data, we must be able to convert CSV data to RDF. This is the objective of the csv2rdf.py tool available in the csv2rdf repo. There are two options proposed by this tool to convert CSV data: The default conversion option, The semantic grammar option, which enables to determine precisely the triple construction. By reusing the same semantic grammar, multiple sources of data can be gathered in the same semantic database (current tests are using Apache Jena ).","title":"Converting CSV data into RDF"},{"location":"research/graphapps/#visualizing-rdf-data","text":"Per se RDF data are quite complicated to represent because of the hierachy of predicates. The following tools do not consider this hierarchy and generate one new edge object per predicate, while generating only one node for the subject and object across the full set of triples. The rdfviz repo contains two options of visualization: GML conversion, to be imported in tools like Yed or Cytoscape ; dot conversion to be displayed with GraphViz .","title":"Visualizing RDF data"},{"location":"research/graphapps/#exploring-rdf-graph-data-through-neighborhoods","text":"In the original GraphApps project, a lot of efforts were put in the use of node neighborhood as a basic element of knowledge navigation. The ontovisu repo aims at enabling the neighborhood-based visual navigation in RDF graph data. It is based on : Node , Apache Jena ), Cytoscape js graph library . Development of this repo is ongoing.","title":"Exploring RDF graph data through neighborhoods"},{"location":"research/graphapps/#various-notes-linked-to-graphs","text":"Using graph transformations at the grammar level, the case of RDBMS schema transformation for re-semantization: Grammar graph transformations A DSL to check the graph topology Graph transformation applicability Also ongoing: it is quite difficult to properly design in the context of RDF/RDFS (without even talking Owl). This ongoing paper RDF design patterns is a first attempt. Notes on Description Logic See also The papers on orey.github.io . Those documents are available under the GNU FDL license.","title":"Various notes linked to graphs"},{"location":"research/index-research/","text":"Personal Research Main Page Main Ideas All research projects are founded on main ideas. Here are the main ideas that are underlying to the GraphApps project. Idea 1 : In the software engineering world, our ways of representing knowledge are bad, inefficient and non evolutive , being at the programmaning level or at the data level. This fact generates massive costs and pain in evolution of software or evolution of data storage. In one formula: Software is far from beware \"soft\". Idea 2 : Graphs enable better knowledge representation , both static and within the evolution dynamics of programs and data. For instance, graphs can be used to address the technical debt problem (see here and here ) or to address complex data migration (see here ). Idea 3 : Graph transformations are at the heart of new software engineering paradigms , such as graph-oriented programming (see here and here ) or semantic data migration (see here ). For sure, a new generation of tools is required to be efficient with those paradigms (design, programming languages, data storage). Idea 4 : With a real evolutive software and data, the total worldwide software effort could be divided by more than 2, probably 3 , which means: * An enormous worldwide gain of productivity; * A capability of evolving the business processes faster than ever before; * A convergence of the design processes of software creation, software evolution and refactoring; * A different mindset based on graph representation of knowledge and graph transformations for business rules. The Graph Transformations at the center See the Graph transformations page. Industrial Projects 2 main industrial projects were developed based on the 4 ideas and are implmeenting concepts described in the graph transformation page: GraphApps project project (2013-2018) Semantic data migration project (2018-now) Open-source projects Here is a list: The graph repo: tests on graph transformations (Python project). The graphappsjs repo, which is targeting the same objective but analyzes the link of the programming language in that problem (JS project). The csv2rdf repo: able to turn a CSV file into RDF with a graph transformation grammar (Python project). The rdfviz repo proposes to visualize rdf graph data (Python project). The ontovisu repo aims at enabling the neighborhood-based visual navigation in RDF graph data (JS project). More Reflections on industry data RDF design patterns - ongoing, important in a professional context Towards a graph-oriented programming language? Resources (June 2020)","title":"Index"},{"location":"research/index-research/#personal-research-main-page","text":"","title":"Personal Research Main Page"},{"location":"research/index-research/#main-ideas","text":"All research projects are founded on main ideas. Here are the main ideas that are underlying to the GraphApps project. Idea 1 : In the software engineering world, our ways of representing knowledge are bad, inefficient and non evolutive , being at the programmaning level or at the data level. This fact generates massive costs and pain in evolution of software or evolution of data storage. In one formula: Software is far from beware \"soft\". Idea 2 : Graphs enable better knowledge representation , both static and within the evolution dynamics of programs and data. For instance, graphs can be used to address the technical debt problem (see here and here ) or to address complex data migration (see here ). Idea 3 : Graph transformations are at the heart of new software engineering paradigms , such as graph-oriented programming (see here and here ) or semantic data migration (see here ). For sure, a new generation of tools is required to be efficient with those paradigms (design, programming languages, data storage). Idea 4 : With a real evolutive software and data, the total worldwide software effort could be divided by more than 2, probably 3 , which means: * An enormous worldwide gain of productivity; * A capability of evolving the business processes faster than ever before; * A convergence of the design processes of software creation, software evolution and refactoring; * A different mindset based on graph representation of knowledge and graph transformations for business rules.","title":"Main Ideas"},{"location":"research/index-research/#the-graph-transformations-at-the-center","text":"See the Graph transformations page.","title":"The Graph Transformations at the center"},{"location":"research/index-research/#industrial-projects","text":"2 main industrial projects were developed based on the 4 ideas and are implmeenting concepts described in the graph transformation page: GraphApps project project (2013-2018) Semantic data migration project (2018-now)","title":"Industrial Projects"},{"location":"research/index-research/#open-source-projects","text":"Here is a list: The graph repo: tests on graph transformations (Python project). The graphappsjs repo, which is targeting the same objective but analyzes the link of the programming language in that problem (JS project). The csv2rdf repo: able to turn a CSV file into RDF with a graph transformation grammar (Python project). The rdfviz repo proposes to visualize rdf graph data (Python project). The ontovisu repo aims at enabling the neighborhood-based visual navigation in RDF graph data (JS project).","title":"Open-source projects"},{"location":"research/index-research/#more","text":"Reflections on industry data RDF design patterns - ongoing, important in a professional context Towards a graph-oriented programming language? Resources (June 2020)","title":"More"},{"location":"research/industry-data/","text":"Reflections on industry data The Graal of the single data model In the IT world, the idea of interoperability seems OK for a long time. It is largely accepted that many systems have to collaborate around the same data and that a standard neutral shared data format is enabling: Every system to do manage its own business; All system to share a minimal set of semantic data. In the industry, things are a bit more complicated for the following reasons: Data have complex life-cycles; Data are interconnected to each other; Data may be transformed in order to be used (for instance in simulation). For sure, data outside the industry have also complex lifecycles, but there are specific applications designed to manage them with their own business rules. Due to the fact that the core element of industrial data was the \"part\", standard software like PDMs manipulated those parts and made everyone believe that it was easy to have a single representation of things based on parts. The Requirements, Functional, Logical and Physical (RFLP) model The RFLP model, largely accepted now in the industry, showed that many artifacts were required to be able to create and manage the lifecycle of a product, each of them being part of a certain level of representation and being interconnected to another level. This approach is already largely known in enterprise architecture where different levels of representation of the enterprise enable complex multi-level modelings (cf. Archimate). The semantic graphs at the center of the game At the center of those multiple representation views are the semantic graphs. Those graphs can be seen as a two level semantic universe (see Fig. 1): Each semantic domain owns its own set of artifacts that are interconnected through various kinds of links (predicates); The domains are interconnected together by other kind of predicates (inter-domain predicates). For sure, those predicates are touching source and destination nodes from several semantic domains but enable to attach the various semantic domains through a set of semantic links. Figure 1: Semantic spaces in industry data Graph transformations The formalism of graph transformations enables to transform the data inside a semantic space but also outside (see Fig. 2). Figure 2: Graph transformations in semantic spaces (Last update: June 2020)","title":"Reflections on industry data"},{"location":"research/industry-data/#reflections-on-industry-data","text":"","title":"Reflections on industry data"},{"location":"research/industry-data/#the-graal-of-the-single-data-model","text":"In the IT world, the idea of interoperability seems OK for a long time. It is largely accepted that many systems have to collaborate around the same data and that a standard neutral shared data format is enabling: Every system to do manage its own business; All system to share a minimal set of semantic data. In the industry, things are a bit more complicated for the following reasons: Data have complex life-cycles; Data are interconnected to each other; Data may be transformed in order to be used (for instance in simulation). For sure, data outside the industry have also complex lifecycles, but there are specific applications designed to manage them with their own business rules. Due to the fact that the core element of industrial data was the \"part\", standard software like PDMs manipulated those parts and made everyone believe that it was easy to have a single representation of things based on parts.","title":"The Graal of the single data model"},{"location":"research/industry-data/#the-requirements-functional-logical-and-physical-rflp-model","text":"The RFLP model, largely accepted now in the industry, showed that many artifacts were required to be able to create and manage the lifecycle of a product, each of them being part of a certain level of representation and being interconnected to another level. This approach is already largely known in enterprise architecture where different levels of representation of the enterprise enable complex multi-level modelings (cf. Archimate).","title":"The Requirements, Functional, Logical and Physical (RFLP) model"},{"location":"research/industry-data/#the-semantic-graphs-at-the-center-of-the-game","text":"At the center of those multiple representation views are the semantic graphs. Those graphs can be seen as a two level semantic universe (see Fig. 1): Each semantic domain owns its own set of artifacts that are interconnected through various kinds of links (predicates); The domains are interconnected together by other kind of predicates (inter-domain predicates). For sure, those predicates are touching source and destination nodes from several semantic domains but enable to attach the various semantic domains through a set of semantic links. Figure 1: Semantic spaces in industry data","title":"The semantic graphs at the center of the game"},{"location":"research/industry-data/#graph-transformations","text":"The formalism of graph transformations enables to transform the data inside a semantic space but also outside (see Fig. 2). Figure 2: Graph transformations in semantic spaces (Last update: June 2020)","title":"Graph transformations"},{"location":"research/rdf-design-patterns/","text":"RDF design patterns This page describe some design patterns to create application basic structures. The objective is less ambitious than to create complex ontologies. It aims to be an intermediate between graph-oriented design (or object-oriented design) and ontology design with tools such as Prot\u00e9g\u00e9. Sources The resources listed in the ./resources are original and interpreted basic semantic web resources. They can be models to understand how the semantic web is really working. Classes and instances The semantic web frontier between classes and instances is not always clear, especially when the set of triples is representing a \"grammar\", so a set of structures that will be instanciated to create real data triples. One of the core ambiguity is represented by the rdf:type or a in Turtle. This relation is defining an instance of the class. The standard defines rdf:type a rdf:Property . which indicates that rdf:type is a \"relation\". However, things can quickly become complicated. Let's take the following sample slightly simplified extracted from the standard: rdf:Property a rdfs:Class ; rdfs:comment \"The class of RDF properties.\" ; rdfs:subClassOf rdfs:Resource . rdfs:Resource a rdfs:Class ; rdfs:label \"Resource\" ; rdfs:comment \"The class resource, everything.\" . rdf:Property and rdfs:Resource are rdfs:Class , but rdfs:Class a rdfs:Class ; rdfs:label \"Class\" ; rdfs:comment \"The class of classes.\" ; rdfs:subClassOf rdfs:Resource .","title":"RDF design patterns"},{"location":"research/rdf-design-patterns/#rdf-design-patterns","text":"This page describe some design patterns to create application basic structures. The objective is less ambitious than to create complex ontologies. It aims to be an intermediate between graph-oriented design (or object-oriented design) and ontology design with tools such as Prot\u00e9g\u00e9.","title":"RDF design patterns"},{"location":"research/rdf-design-patterns/#sources","text":"The resources listed in the ./resources are original and interpreted basic semantic web resources. They can be models to understand how the semantic web is really working.","title":"Sources"},{"location":"research/rdf-design-patterns/#classes-and-instances","text":"The semantic web frontier between classes and instances is not always clear, especially when the set of triples is representing a \"grammar\", so a set of structures that will be instanciated to create real data triples. One of the core ambiguity is represented by the rdf:type or a in Turtle. This relation is defining an instance of the class. The standard defines rdf:type a rdf:Property . which indicates that rdf:type is a \"relation\". However, things can quickly become complicated. Let's take the following sample slightly simplified extracted from the standard: rdf:Property a rdfs:Class ; rdfs:comment \"The class of RDF properties.\" ; rdfs:subClassOf rdfs:Resource . rdfs:Resource a rdfs:Class ; rdfs:label \"Resource\" ; rdfs:comment \"The class resource, everything.\" . rdf:Property and rdfs:Resource are rdfs:Class , but rdfs:Class a rdfs:Class ; rdfs:label \"Class\" ; rdfs:comment \"The class of classes.\" ; rdfs:subClassOf rdfs:Resource .","title":"Classes and instances"},{"location":"research/resources/","text":"Resources Back to index Technical debt The well-known curve of software technical debt. Maintenance costs The rule of 60/60 (Robert L. Glass): Between 40% and 80% of software costs are maintenance costs (average 60%); Around 60% of maintenance costs are enhancements. Source: Frequently forgotten fundamental facts about software engineering - alternate . This figure seems rather optimistic and the interval may more be 60-80% for maintenance costs. (Same source) Back to index (Last update: June 2020)","title":"Resources"},{"location":"research/resources/#resources","text":"Back to index","title":"Resources"},{"location":"research/resources/#technical-debt","text":"The well-known curve of software technical debt.","title":"Technical debt"},{"location":"research/resources/#maintenance-costs","text":"The rule of 60/60 (Robert L. Glass): Between 40% and 80% of software costs are maintenance costs (average 60%); Around 60% of maintenance costs are enhancements. Source: Frequently forgotten fundamental facts about software engineering - alternate . This figure seems rather optimistic and the interval may more be 60-80% for maintenance costs. (Same source) Back to index (Last update: June 2020)","title":"Maintenance costs"},{"location":"semantic/data-migration/","text":"Conference at the Madics 2019 CNRS Conference in Rennes The aerospace industry is managing aircraft products with PLM (product lifecycle management) software. Most of the time, the PLM software used are very customized and very old. Moreover in the day to day life, people are using a mix of various PLM systems and Excel spreadsheets. In order to change the business practices for the 3 core businesses of the aerospace companies (design office, manufacturing, support and services), the PLM backbones must evolve on two dimensions: first they must gather all data spread across various tools per business, and second they must integrate the three businesses together (digital continuity) in order to perform efficient concurrent engineering and gain on lead times. New end to end PLM backbones are available on the market but the migration to those products is slowed down by the lack of method and tools to properly migrate the data. The main constraint of PLM data migration is to be able to keep all the semantic links of past data into the new system, because the aircraft certification is tied to those links and data. At the intersection of all those constraints, the semantic web technologies (RDF/RDFS, Triplestores, SPARQL-based rules, etc.) can help Airbus to convert its core industrial data to migrate them into new generations of PLM systems. The presentation exposes a concrete industrial case and explain the status of the works and the tool chain involved in those works. Presentation (June 2020)","title":"Aerospace data migration"},{"location":"semantic/data-migration/#conference-at-the-madics-2019-cnrs-conference-in-rennes","text":"The aerospace industry is managing aircraft products with PLM (product lifecycle management) software. Most of the time, the PLM software used are very customized and very old. Moreover in the day to day life, people are using a mix of various PLM systems and Excel spreadsheets. In order to change the business practices for the 3 core businesses of the aerospace companies (design office, manufacturing, support and services), the PLM backbones must evolve on two dimensions: first they must gather all data spread across various tools per business, and second they must integrate the three businesses together (digital continuity) in order to perform efficient concurrent engineering and gain on lead times. New end to end PLM backbones are available on the market but the migration to those products is slowed down by the lack of method and tools to properly migrate the data. The main constraint of PLM data migration is to be able to keep all the semantic links of past data into the new system, because the aircraft certification is tied to those links and data. At the intersection of all those constraints, the semantic web technologies (RDF/RDFS, Triplestores, SPARQL-based rules, etc.) can help Airbus to convert its core industrial data to migrate them into new generations of PLM systems. The presentation exposes a concrete industrial case and explain the status of the works and the tool chain involved in those works. Presentation (June 2020)","title":"Conference at the Madics 2019 CNRS Conference in Rennes"}]}