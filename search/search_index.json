{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Olivier Rey's Papers Papers on PLM NEW - The four core functions showing you need a PLM NEW - Configuration management of industrial products in PDM/PLM PLM and graph data Papers on modeling Military Frameworks, Systems Engineering and Enterprise Architecture Archimate Recipes An Introduction to The Archimate Revolution A Simple Meta-Model for Portfolio Management Papers on data and interoperability The real nature of data GraphQL And Classic Web Services Considerations about Rest and Web Services Papers on IT processes The Various Stages of Digital Transformation The V2 Vortex The Five Levels of Conceptual Maturity for IT Teams Graph-oriented programming This section contains materials about graph-oriented programming, solving the technical debt issue and using graph transformations in software engineering. First article on graph-oriented programming Conference at the STAF/ICGT 2018 in Toulouse Project page: GraphApps project project (2013-2018) Research section: Graph transformations and semantic data migration Index Graph transformations Basic graph transformations Basic semantic graph transformations DSL for graph topology check Graph transformation applicability Grammar of graph transformation Graph-oriented programming language Semantic data migration project (2018-now) Using semantic web technologies for aerospace industrial data migration, Madics 2019 More UML to RDF considerations - Updated June 2020 Reflections on industry data Towards a graph-oriented programming language? Resources (Last update: August 2021)","title":"Index"},{"location":"#olivier-reys-papers","text":"","title":"Olivier Rey's Papers"},{"location":"#papers-on-plm","text":"NEW - The four core functions showing you need a PLM NEW - Configuration management of industrial products in PDM/PLM PLM and graph data","title":"Papers on PLM"},{"location":"#papers-on-modeling","text":"Military Frameworks, Systems Engineering and Enterprise Architecture Archimate Recipes An Introduction to The Archimate Revolution A Simple Meta-Model for Portfolio Management","title":"Papers on modeling"},{"location":"#papers-on-data-and-interoperability","text":"The real nature of data GraphQL And Classic Web Services Considerations about Rest and Web Services","title":"Papers on data and interoperability"},{"location":"#papers-on-it-processes","text":"The Various Stages of Digital Transformation The V2 Vortex The Five Levels of Conceptual Maturity for IT Teams","title":"Papers on IT processes"},{"location":"#graph-oriented-programming","text":"This section contains materials about graph-oriented programming, solving the technical debt issue and using graph transformations in software engineering. First article on graph-oriented programming Conference at the STAF/ICGT 2018 in Toulouse Project page: GraphApps project project (2013-2018)","title":"Graph-oriented programming"},{"location":"#research-section-graph-transformations-and-semantic-data-migration","text":"Index Graph transformations Basic graph transformations Basic semantic graph transformations DSL for graph topology check Graph transformation applicability Grammar of graph transformation Graph-oriented programming language Semantic data migration project (2018-now) Using semantic web technologies for aerospace industrial data migration, Madics 2019 More UML to RDF considerations - Updated June 2020 Reflections on industry data Towards a graph-oriented programming language? Resources (Last update: August 2021)","title":"Research section: Graph transformations and semantic data migration"},{"location":"about/LICENSE/","text":"This site is licensed under the terms and conditions of the GNU FDL V3 that can be found hereafter. GNU Free Documentation License Version 1.3, 3 November 2008 Copyright \u00a9 2000, 2001, 2002, 2007, 2008 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed. 0. PREAMBLE The purpose of this License is to make a manual, textbook, or other functional and useful document \"free\" in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or noncommercially. Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others. This License is a kind of \"copyleft\", which means that derivative works of the document must themselves be free in the same sense. It complements the GNU General Public License, which is a copyleft license designed for free software. We have designed this License in order to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does. But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book. We recommend this License principally for works whose purpose is instruction or reference. 1. APPLICABILITY AND DEFINITIONS This License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License. Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein. The \"Document\", below, refers to any such manual or work. Any member of the public is a licensee, and is addressed as \"you\". You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law. A \"Modified Version\" of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language. A \"Secondary Section\" is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document's overall subject (or to related matters) and contains nothing that could fall directly within that overall subject. (Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them. The \"Invariant Sections\" are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License. If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant. The Document may contain zero Invariant Sections. If the Document does not identify any Invariant Sections then there are none. The \"Cover Texts\" are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License. A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words. A \"Transparent\" copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters. A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent. An image format is not Transparent if used for any substantial amount of text. A copy that is not \"Transparent\" is called \"Opaque\". Examples of suitable formats for Transparent copies include plain ASCII without markup, Texinfo input format, LaTeX input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification. Examples of transparent image formats include PNG, XCF and JPG. Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only. The \"Title Page\" means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page. For works in formats which do not have any title page as such, \"Title Page\" means the text near the most prominent appearance of the work's title, preceding the beginning of the body of the text. The \"publisher\" means any person or entity that distributes copies of the Document to the public. A section \"Entitled XYZ\" means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language. (Here XYZ stands for a specific section name mentioned below, such as \"Acknowledgements\", \"Dedications\", \"Endorsements\", or \"History\".) To \"Preserve the Title\" of such a section when you modify the Document means that it remains a section \"Entitled XYZ\" according to this definition. The Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document. These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License. 2. VERBATIM COPYING You may copy and distribute the Document in any medium, either commercially or noncommercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License. You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute. However, you may accept compensation in exchange for copies. If you distribute a large enough number of copies you must also follow the conditions in section 3. You may also lend copies, under the same conditions stated above, and you may publicly display copies. 3. COPYING IN QUANTITY If you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document's license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover. Both covers must also clearly and legibly identify you as the publisher of these copies. The front cover must present the full title with all words of the title equally prominent and visible. You may add other material on the covers in addition. Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects. If the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages. If you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material. If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public. It is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document. 4. MODIFICATIONS You may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it. In addition, you must do these things in the Modified Version: A. Use in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission. B. List on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement. C. State on the Title page the name of the publisher of the Modified Version, as the publisher. D. Preserve all the copyright notices of the Document. E. Add an appropriate copyright notice for your modifications adjacent to the other copyright notices. F. Include, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below. G. Preserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document's license notice. H. Include an unaltered copy of this License. I. Preserve the section Entitled \"History\", Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled \"History\" in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence. J. Preserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the \"History\" section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission. K. For any section Entitled \"Acknowledgements\" or \"Dedications\", Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein. L. Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles. M. Delete any section Entitled \"Endorsements\". Such a section may not be included in the Modified Version. N. Do not retitle any existing section to be Entitled \"Endorsements\" or to conflict in title with any Invariant Section. O. Preserve any Warranty Disclaimers. If the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant. To do this, add their titles to the list of Invariant Sections in the Modified Version's license notice. These titles must be distinct from any other section titles. You may add a section Entitled \"Endorsements\", provided it contains nothing but endorsements of your Modified Version by various parties\u2014for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard. You may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version. Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity. If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one. The author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version. 5. COMBINING DOCUMENTS You may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers. The combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy. If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number. Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work. In the combination, you must combine any sections Entitled \"History\" in the various original documents, forming one section Entitled \"History\"; likewise combine any sections Entitled \"Acknowledgements\", and any sections Entitled \"Dedications\". You must delete all sections Entitled \"Endorsements\". 6. COLLECTIONS OF DOCUMENTS You may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects. You may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document. 7. AGGREGATION WITH INDEPENDENT WORKS A compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the copyright resulting from the compilation is not used to limit the legal rights of the compilation's users beyond what the individual works permit. When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document. If the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document's Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form. Otherwise they must appear on printed covers that bracket the whole aggregate. 8. TRANSLATION Translation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4. Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections. You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers. In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail. If a section in the Document is Entitled \"Acknowledgements\", \"Dedications\", or \"History\", the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title. 9. TERMINATION You may not copy, modify, sublicense, or distribute the Document except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense, or distribute it is void, and will automatically terminate your rights under this License. However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, receipt of a copy of some or all of the same material does not give you any rights to use it. 10. FUTURE REVISIONS OF THIS LICENSE The Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. See https://www.gnu.org/licenses/. Each version of the License is given a distinguishing version number. If the Document specifies that a particular numbered version of this License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation. If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation. If the Document specifies that a proxy can decide which future versions of this License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Document. 11. RELICENSING \"Massive Multiauthor Collaboration Site\" (or \"MMC Site\") means any World Wide Web server that publishes copyrightable works and also provides prominent facilities for anybody to edit those works. A public wiki that anybody can edit is an example of such a server. A \"Massive Multiauthor Collaboration\" (or \"MMC\") contained in the site means any set of copyrightable works thus published on the MMC site. \"CC-BY-SA\" means the Creative Commons Attribution-Share Alike 3.0 license published by Creative Commons Corporation, a not-for-profit corporation with a principal place of business in San Francisco, California, as well as future copyleft versions of that license published by that same organization. \"Incorporate\" means to publish or republish a Document, in whole or in part, as part of another Document. An MMC is \"eligible for relicensing\" if it is licensed under this License, and if all works that were first published under this License somewhere other than this MMC, and subsequently incorporated in whole or in part into the MMC, (1) had no cover texts or invariant sections, and (2) were thus incorporated prior to November 1, 2008. The operator of an MMC Site may republish an MMC contained in the site under CC-BY-SA on the same site at any time before August 1, 2009, provided the MMC is eligible for relicensing. ADDENDUM: How to use this License for your documents To use this License in a document you have written, include a copy of the License in the document and put the following copyright and license notices just after the title page:","title":"License"},{"location":"about/LICENSE/#gnu-free-documentation-license","text":"Version 1.3, 3 November 2008 Copyright \u00a9 2000, 2001, 2002, 2007, 2008 Free Software Foundation, Inc. https://fsf.org/ Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.","title":"GNU Free Documentation License"},{"location":"about/LICENSE/#0-preamble","text":"The purpose of this License is to make a manual, textbook, or other functional and useful document \"free\" in the sense of freedom: to assure everyone the effective freedom to copy and redistribute it, with or without modifying it, either commercially or noncommercially. Secondarily, this License preserves for the author and publisher a way to get credit for their work, while not being considered responsible for modifications made by others. This License is a kind of \"copyleft\", which means that derivative works of the document must themselves be free in the same sense. It complements the GNU General Public License, which is a copyleft license designed for free software. We have designed this License in order to use it for manuals for free software, because free software needs free documentation: a free program should come with manuals providing the same freedoms that the software does. But this License is not limited to software manuals; it can be used for any textual work, regardless of subject matter or whether it is published as a printed book. We recommend this License principally for works whose purpose is instruction or reference.","title":"0. PREAMBLE"},{"location":"about/LICENSE/#1-applicability-and-definitions","text":"This License applies to any manual or other work, in any medium, that contains a notice placed by the copyright holder saying it can be distributed under the terms of this License. Such a notice grants a world-wide, royalty-free license, unlimited in duration, to use that work under the conditions stated herein. The \"Document\", below, refers to any such manual or work. Any member of the public is a licensee, and is addressed as \"you\". You accept the license if you copy, modify or distribute the work in a way requiring permission under copyright law. A \"Modified Version\" of the Document means any work containing the Document or a portion of it, either copied verbatim, or with modifications and/or translated into another language. A \"Secondary Section\" is a named appendix or a front-matter section of the Document that deals exclusively with the relationship of the publishers or authors of the Document to the Document's overall subject (or to related matters) and contains nothing that could fall directly within that overall subject. (Thus, if the Document is in part a textbook of mathematics, a Secondary Section may not explain any mathematics.) The relationship could be a matter of historical connection with the subject or with related matters, or of legal, commercial, philosophical, ethical or political position regarding them. The \"Invariant Sections\" are certain Secondary Sections whose titles are designated, as being those of Invariant Sections, in the notice that says that the Document is released under this License. If a section does not fit the above definition of Secondary then it is not allowed to be designated as Invariant. The Document may contain zero Invariant Sections. If the Document does not identify any Invariant Sections then there are none. The \"Cover Texts\" are certain short passages of text that are listed, as Front-Cover Texts or Back-Cover Texts, in the notice that says that the Document is released under this License. A Front-Cover Text may be at most 5 words, and a Back-Cover Text may be at most 25 words. A \"Transparent\" copy of the Document means a machine-readable copy, represented in a format whose specification is available to the general public, that is suitable for revising the document straightforwardly with generic text editors or (for images composed of pixels) generic paint programs or (for drawings) some widely available drawing editor, and that is suitable for input to text formatters or for automatic translation to a variety of formats suitable for input to text formatters. A copy made in an otherwise Transparent file format whose markup, or absence of markup, has been arranged to thwart or discourage subsequent modification by readers is not Transparent. An image format is not Transparent if used for any substantial amount of text. A copy that is not \"Transparent\" is called \"Opaque\". Examples of suitable formats for Transparent copies include plain ASCII without markup, Texinfo input format, LaTeX input format, SGML or XML using a publicly available DTD, and standard-conforming simple HTML, PostScript or PDF designed for human modification. Examples of transparent image formats include PNG, XCF and JPG. Opaque formats include proprietary formats that can be read and edited only by proprietary word processors, SGML or XML for which the DTD and/or processing tools are not generally available, and the machine-generated HTML, PostScript or PDF produced by some word processors for output purposes only. The \"Title Page\" means, for a printed book, the title page itself, plus such following pages as are needed to hold, legibly, the material this License requires to appear in the title page. For works in formats which do not have any title page as such, \"Title Page\" means the text near the most prominent appearance of the work's title, preceding the beginning of the body of the text. The \"publisher\" means any person or entity that distributes copies of the Document to the public. A section \"Entitled XYZ\" means a named subunit of the Document whose title either is precisely XYZ or contains XYZ in parentheses following text that translates XYZ in another language. (Here XYZ stands for a specific section name mentioned below, such as \"Acknowledgements\", \"Dedications\", \"Endorsements\", or \"History\".) To \"Preserve the Title\" of such a section when you modify the Document means that it remains a section \"Entitled XYZ\" according to this definition. The Document may include Warranty Disclaimers next to the notice which states that this License applies to the Document. These Warranty Disclaimers are considered to be included by reference in this License, but only as regards disclaiming warranties: any other implication that these Warranty Disclaimers may have is void and has no effect on the meaning of this License.","title":"1. APPLICABILITY AND DEFINITIONS"},{"location":"about/LICENSE/#2-verbatim-copying","text":"You may copy and distribute the Document in any medium, either commercially or noncommercially, provided that this License, the copyright notices, and the license notice saying this License applies to the Document are reproduced in all copies, and that you add no other conditions whatsoever to those of this License. You may not use technical measures to obstruct or control the reading or further copying of the copies you make or distribute. However, you may accept compensation in exchange for copies. If you distribute a large enough number of copies you must also follow the conditions in section 3. You may also lend copies, under the same conditions stated above, and you may publicly display copies.","title":"2. VERBATIM COPYING"},{"location":"about/LICENSE/#3-copying-in-quantity","text":"If you publish printed copies (or copies in media that commonly have printed covers) of the Document, numbering more than 100, and the Document's license notice requires Cover Texts, you must enclose the copies in covers that carry, clearly and legibly, all these Cover Texts: Front-Cover Texts on the front cover, and Back-Cover Texts on the back cover. Both covers must also clearly and legibly identify you as the publisher of these copies. The front cover must present the full title with all words of the title equally prominent and visible. You may add other material on the covers in addition. Copying with changes limited to the covers, as long as they preserve the title of the Document and satisfy these conditions, can be treated as verbatim copying in other respects. If the required texts for either cover are too voluminous to fit legibly, you should put the first ones listed (as many as fit reasonably) on the actual cover, and continue the rest onto adjacent pages. If you publish or distribute Opaque copies of the Document numbering more than 100, you must either include a machine-readable Transparent copy along with each Opaque copy, or state in or with each Opaque copy a computer-network location from which the general network-using public has access to download using public-standard network protocols a complete Transparent copy of the Document, free of added material. If you use the latter option, you must take reasonably prudent steps, when you begin distribution of Opaque copies in quantity, to ensure that this Transparent copy will remain thus accessible at the stated location until at least one year after the last time you distribute an Opaque copy (directly or through your agents or retailers) of that edition to the public. It is requested, but not required, that you contact the authors of the Document well before redistributing any large number of copies, to give them a chance to provide you with an updated version of the Document.","title":"3. COPYING IN QUANTITY"},{"location":"about/LICENSE/#4-modifications","text":"You may copy and distribute a Modified Version of the Document under the conditions of sections 2 and 3 above, provided that you release the Modified Version under precisely this License, with the Modified Version filling the role of the Document, thus licensing distribution and modification of the Modified Version to whoever possesses a copy of it. In addition, you must do these things in the Modified Version: A. Use in the Title Page (and on the covers, if any) a title distinct from that of the Document, and from those of previous versions (which should, if there were any, be listed in the History section of the Document). You may use the same title as a previous version if the original publisher of that version gives permission. B. List on the Title Page, as authors, one or more persons or entities responsible for authorship of the modifications in the Modified Version, together with at least five of the principal authors of the Document (all of its principal authors, if it has fewer than five), unless they release you from this requirement. C. State on the Title page the name of the publisher of the Modified Version, as the publisher. D. Preserve all the copyright notices of the Document. E. Add an appropriate copyright notice for your modifications adjacent to the other copyright notices. F. Include, immediately after the copyright notices, a license notice giving the public permission to use the Modified Version under the terms of this License, in the form shown in the Addendum below. G. Preserve in that license notice the full lists of Invariant Sections and required Cover Texts given in the Document's license notice. H. Include an unaltered copy of this License. I. Preserve the section Entitled \"History\", Preserve its Title, and add to it an item stating at least the title, year, new authors, and publisher of the Modified Version as given on the Title Page. If there is no section Entitled \"History\" in the Document, create one stating the title, year, authors, and publisher of the Document as given on its Title Page, then add an item describing the Modified Version as stated in the previous sentence. J. Preserve the network location, if any, given in the Document for public access to a Transparent copy of the Document, and likewise the network locations given in the Document for previous versions it was based on. These may be placed in the \"History\" section. You may omit a network location for a work that was published at least four years before the Document itself, or if the original publisher of the version it refers to gives permission. K. For any section Entitled \"Acknowledgements\" or \"Dedications\", Preserve the Title of the section, and preserve in the section all the substance and tone of each of the contributor acknowledgements and/or dedications given therein. L. Preserve all the Invariant Sections of the Document, unaltered in their text and in their titles. Section numbers or the equivalent are not considered part of the section titles. M. Delete any section Entitled \"Endorsements\". Such a section may not be included in the Modified Version. N. Do not retitle any existing section to be Entitled \"Endorsements\" or to conflict in title with any Invariant Section. O. Preserve any Warranty Disclaimers. If the Modified Version includes new front-matter sections or appendices that qualify as Secondary Sections and contain no material copied from the Document, you may at your option designate some or all of these sections as invariant. To do this, add their titles to the list of Invariant Sections in the Modified Version's license notice. These titles must be distinct from any other section titles. You may add a section Entitled \"Endorsements\", provided it contains nothing but endorsements of your Modified Version by various parties\u2014for example, statements of peer review or that the text has been approved by an organization as the authoritative definition of a standard. You may add a passage of up to five words as a Front-Cover Text, and a passage of up to 25 words as a Back-Cover Text, to the end of the list of Cover Texts in the Modified Version. Only one passage of Front-Cover Text and one of Back-Cover Text may be added by (or through arrangements made by) any one entity. If the Document already includes a cover text for the same cover, previously added by you or by arrangement made by the same entity you are acting on behalf of, you may not add another; but you may replace the old one, on explicit permission from the previous publisher that added the old one. The author(s) and publisher(s) of the Document do not by this License give permission to use their names for publicity for or to assert or imply endorsement of any Modified Version.","title":"4. MODIFICATIONS"},{"location":"about/LICENSE/#5-combining-documents","text":"You may combine the Document with other documents released under this License, under the terms defined in section 4 above for modified versions, provided that you include in the combination all of the Invariant Sections of all of the original documents, unmodified, and list them all as Invariant Sections of your combined work in its license notice, and that you preserve all their Warranty Disclaimers. The combined work need only contain one copy of this License, and multiple identical Invariant Sections may be replaced with a single copy. If there are multiple Invariant Sections with the same name but different contents, make the title of each such section unique by adding at the end of it, in parentheses, the name of the original author or publisher of that section if known, or else a unique number. Make the same adjustment to the section titles in the list of Invariant Sections in the license notice of the combined work. In the combination, you must combine any sections Entitled \"History\" in the various original documents, forming one section Entitled \"History\"; likewise combine any sections Entitled \"Acknowledgements\", and any sections Entitled \"Dedications\". You must delete all sections Entitled \"Endorsements\".","title":"5. COMBINING DOCUMENTS"},{"location":"about/LICENSE/#6-collections-of-documents","text":"You may make a collection consisting of the Document and other documents released under this License, and replace the individual copies of this License in the various documents with a single copy that is included in the collection, provided that you follow the rules of this License for verbatim copying of each of the documents in all other respects. You may extract a single document from such a collection, and distribute it individually under this License, provided you insert a copy of this License into the extracted document, and follow this License in all other respects regarding verbatim copying of that document.","title":"6. COLLECTIONS OF DOCUMENTS"},{"location":"about/LICENSE/#7-aggregation-with-independent-works","text":"A compilation of the Document or its derivatives with other separate and independent documents or works, in or on a volume of a storage or distribution medium, is called an \"aggregate\" if the copyright resulting from the compilation is not used to limit the legal rights of the compilation's users beyond what the individual works permit. When the Document is included in an aggregate, this License does not apply to the other works in the aggregate which are not themselves derivative works of the Document. If the Cover Text requirement of section 3 is applicable to these copies of the Document, then if the Document is less than one half of the entire aggregate, the Document's Cover Texts may be placed on covers that bracket the Document within the aggregate, or the electronic equivalent of covers if the Document is in electronic form. Otherwise they must appear on printed covers that bracket the whole aggregate.","title":"7. AGGREGATION WITH INDEPENDENT WORKS"},{"location":"about/LICENSE/#8-translation","text":"Translation is considered a kind of modification, so you may distribute translations of the Document under the terms of section 4. Replacing Invariant Sections with translations requires special permission from their copyright holders, but you may include translations of some or all Invariant Sections in addition to the original versions of these Invariant Sections. You may include a translation of this License, and all the license notices in the Document, and any Warranty Disclaimers, provided that you also include the original English version of this License and the original versions of those notices and disclaimers. In case of a disagreement between the translation and the original version of this License or a notice or disclaimer, the original version will prevail. If a section in the Document is Entitled \"Acknowledgements\", \"Dedications\", or \"History\", the requirement (section 4) to Preserve its Title (section 1) will typically require changing the actual title.","title":"8. TRANSLATION"},{"location":"about/LICENSE/#9-termination","text":"You may not copy, modify, sublicense, or distribute the Document except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense, or distribute it is void, and will automatically terminate your rights under this License. However, if you cease all violation of this License, then your license from a particular copyright holder is reinstated (a) provisionally, unless and until the copyright holder explicitly and finally terminates your license, and (b) permanently, if the copyright holder fails to notify you of the violation by some reasonable means prior to 60 days after the cessation. Moreover, your license from a particular copyright holder is reinstated permanently if the copyright holder notifies you of the violation by some reasonable means, this is the first time you have received notice of violation of this License (for any work) from that copyright holder, and you cure the violation prior to 30 days after your receipt of the notice. Termination of your rights under this section does not terminate the licenses of parties who have received copies or rights from you under this License. If your rights have been terminated and not permanently reinstated, receipt of a copy of some or all of the same material does not give you any rights to use it.","title":"9. TERMINATION"},{"location":"about/LICENSE/#10-future-revisions-of-this-license","text":"The Free Software Foundation may publish new, revised versions of the GNU Free Documentation License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns. See https://www.gnu.org/licenses/. Each version of the License is given a distinguishing version number. If the Document specifies that a particular numbered version of this License \"or any later version\" applies to it, you have the option of following the terms and conditions either of that specified version or of any later version that has been published (not as a draft) by the Free Software Foundation. If the Document does not specify a version number of this License, you may choose any version ever published (not as a draft) by the Free Software Foundation. If the Document specifies that a proxy can decide which future versions of this License can be used, that proxy's public statement of acceptance of a version permanently authorizes you to choose that version for the Document.","title":"10. FUTURE REVISIONS OF THIS LICENSE"},{"location":"about/LICENSE/#11-relicensing","text":"\"Massive Multiauthor Collaboration Site\" (or \"MMC Site\") means any World Wide Web server that publishes copyrightable works and also provides prominent facilities for anybody to edit those works. A public wiki that anybody can edit is an example of such a server. A \"Massive Multiauthor Collaboration\" (or \"MMC\") contained in the site means any set of copyrightable works thus published on the MMC site. \"CC-BY-SA\" means the Creative Commons Attribution-Share Alike 3.0 license published by Creative Commons Corporation, a not-for-profit corporation with a principal place of business in San Francisco, California, as well as future copyleft versions of that license published by that same organization. \"Incorporate\" means to publish or republish a Document, in whole or in part, as part of another Document. An MMC is \"eligible for relicensing\" if it is licensed under this License, and if all works that were first published under this License somewhere other than this MMC, and subsequently incorporated in whole or in part into the MMC, (1) had no cover texts or invariant sections, and (2) were thus incorporated prior to November 1, 2008. The operator of an MMC Site may republish an MMC contained in the site under CC-BY-SA on the same site at any time before August 1, 2009, provided the MMC is eligible for relicensing. ADDENDUM: How to use this License for your documents To use this License in a document you have written, include a copy of the License in the document and put the following copyright and license notices just after the title page:","title":"11. RELICENSING"},{"location":"about/about/","text":"About The Author Olivier Rey LinkedIn profile rey [dot] olivier [at] gmail [dot] com Papers ResearchGate Areas of interest Software engineering, programming languages, programming paradigms, semantic web Modeling Archimate and enterprise architecture modeling frameworks MBSE/MBE Domain specific modeling DSL Mathematics (topology, graphs, category theory, algebraic geometry) Articles to write or migrate Deming and the system of profound knowledge French articles? Old stuff Migrate interesting stuff from old blog (and translate) VB and Delphi refactoring practices? Archive Topics: Modeling complex temporal events with graphs and topological graph transformations Like highway traffic The graph transformation changes the graph topology. What kind of mathematical object is it? Scale laws in graph (for instance adherence of domains in a multi-graph mode) What is the true nature of interdomain relationship? And how does it relate to the higher level relationship? Distance comparison between two graphs The fact one and the referential one Useful for AI projects, knowledge representation Isomorphic semantic representations: what graph transformations enable that? Modeling psychology frameworks (Freud, Jung, etc.) with graph patterns and graph distance measurement Rewiring rules and their limitations Cloning nodes and sub-graph: rules? (November 2019)","title":"Author"},{"location":"about/about/#about-the-author","text":"","title":"About The Author"},{"location":"about/about/#olivier-rey","text":"LinkedIn profile rey [dot] olivier [at] gmail [dot] com Papers ResearchGate","title":"Olivier Rey"},{"location":"about/about/#areas-of-interest","text":"Software engineering, programming languages, programming paradigms, semantic web Modeling Archimate and enterprise architecture modeling frameworks MBSE/MBE Domain specific modeling DSL Mathematics (topology, graphs, category theory, algebraic geometry)","title":"Areas of interest"},{"location":"about/about/#articles-to-write-or-migrate","text":"Deming and the system of profound knowledge French articles? Old stuff Migrate interesting stuff from old blog (and translate) VB and Delphi refactoring practices?","title":"Articles to write or migrate"},{"location":"about/about/#archive","text":"Topics: Modeling complex temporal events with graphs and topological graph transformations Like highway traffic The graph transformation changes the graph topology. What kind of mathematical object is it? Scale laws in graph (for instance adherence of domains in a multi-graph mode) What is the true nature of interdomain relationship? And how does it relate to the higher level relationship? Distance comparison between two graphs The fact one and the referential one Useful for AI projects, knowledge representation Isomorphic semantic representations: what graph transformations enable that? Modeling psychology frameworks (Freud, Jung, etc.) with graph patterns and graph distance measurement Rewiring rules and their limitations Cloning nodes and sub-graph: rules? (November 2019)","title":"Archive"},{"location":"articles/about-e2e-plm/","text":"About End-To-End PLM As time goes on, the product lifecycle management (PLM) systems propose more and more functionality, linked together, in platforms that tend to be always bigger with time. The increasing size of the PLM platforms, and their pretention to take in charge more and more processes, is answering to a dream of many users to have \"all data linked together in the same place\". If this dream can be true for small products, it seems to us as a dangerous chimera in the case of complex manufacturing products such as aircrafts. In this article, we will explain the core reasons why we don't believe that this model of the \"unique backbone\" is applicable everywhere. Indeed, when changing semantic domains, the data needs to be transformed. If the tertiary sector takes as granted the adapter pattern (on which we will come back) for a long time, some PLM vendors are selling a model where the adapter can be reduced to links between data . This article is explaining the reasons why, in complex cases, this simplistic approach may appear as very dangerous. PLM, nature of data and adaptation We explained in a recent article what we consider as being the basic functions of a PLM. We recommend the reading of this article before coming back to this one. In that article, we showed that, data being represented as a graph, it appears as quite easy to connect whatever object of a certain type to whatever other object of any other type with an instance of a link type. If we follow this statement, we quickly end up with the \"no-limit approach\" that could be be stated as follows: As log as data can be connected together, then we can manage them in the same software . However, contrary to what could appear, this model of linking data together has limits. Especially, in many cases, representing depedencies between business domains as links at the data level are a very bad representation of reality. In order to understand correctly the problem, we first recommended to have a look at the real nature of data article. In this article, we explain that data are created by business processes and that the way they are structured is dependant on the business domain. When data must flow from a business domain to another business domain, it must be \"adapted out\" the source business domain (transformed and reduced to a format that will be understandable by both the initiator and the receiver). Then the receiver will \"adapt the data in\" its own business universe. Then the receiver will be able to use the data. We often name the business domains \"semantic domains\", because when we change a data from business domain, it is generally not meaning exactly the same thing, and it is often not structured in the same way. For instance, if a \"part\" is created in the engineering semantic domain and sent to the support, this engineering part may be transformed into several \"support parts\" that will have support attributes and be localized at different use points. Concerning the engineering data attached to the part, most of them won't be useful for the support, even if they need to be secure in engineering system. For sure, the bridges between systems must ensure some traceability that enables to track back the same entity between systems. https://www.linkedin.com/feed/update/urn:li:activity:6804804965635239936/?commentUrn=urn%3Ali%3Acomment%3A(activity%3A6804804965635239936%2C6809574091432054784) Hi Axel, I know the OSLC initiative. I like the idea that the central repository (monolith) is not the solution and that the industry has to think in distributed systems. However, what does not convince me fully is that the OSLC approach is still at the data level. If this statement is true, if links can be done at the data level between various data models in various systems, then conceptually it pushes again for more monoliths (super PLMs that can manage several business domains). Conceptually, the semantic web is using just a small subset of the possible distributed IT architectures by establishing a global data model that is spread amongst systems, but is still representing the reality under the form of a global data model. The fact is the tertiary sector is not working on this paradigm: data are spread between systems but there are many places where data are transformed from one semantic domain to another one, with business rules, aka treatments, aka programs with functions. I try to introduce this concept here: https://orey.github.io/papers/articles/data-interop/ (following) Coming from outside of the industry, I am very puzzled about this systematic belief that data can be linked, as if a unique semantic representation was possible. In my article, I try to explain that modeling the reality can be done in many ways, and that the software industry (outside industry) proposed interchange standards that are enabling a program-based adaptation of data semantics between various business domains. And, what is very surprising to me also is that, when we look carefully to the existing systems, they are full of business rules and data transformation anyway. The first article on graph data in PLM is just an introduction which opens on another article on the structure of the bridges between systems. In a general way, those bridges, if they obey to the same rules as we can find in all the software industry - except industry - can embed some code to transform the semantics, so-called \"adapters\", that contain business rules. Maybe industry is the exception that confirms the rule and links between systems can be reduced to the very simple case of \"data links\", but I fear this statement is too reductive. (see message 3) (message 3) And it pushes for more monoliths and huge PLM platforms. I fear that this data-oriented approach will cause massive project failures in the future. For me, this is also a core structural problem of the semantic web. If you think that the reality can be described by linked data only, you just consider a modeling that is static and structural, but you omit the dynamic transformations that are also part of the representation of reality. When we see the troubles of data reconciliation in the semantic web, it is for me the illustration that this approach is not sufficient. Modeling the reality is a \"domain-specific\" activity and this is not because various domains can agree on an exchange protocol that their various semantics can be aggregated in a single model of linked data. I intend to bring more arguments of this fact in my next article. In all cases, it seems to me that this is a very interesting and crucial topic for the industry. L'autre id\u00e9e \u00e9videmment est que le PLM n'est plus g\u00e9n\u00e9rique mais compl\u00e8tement adapt\u00e9 \u00e0 un business domain - d'o\u00f9 l'id\u00e9e des ASD This section will try to explain, in simple words, how the PLM plays a central role in the transformation of the aerospace industry in the last decades. The legacy programs of the \\ag, such as the A320, A330, H225, etc., were designed originally without 3D models\\footnote{For the oldest programs, the drawings were made with paper and Chinese ink.}. Progressively, with the new workstations, appeared a generation of software enabling to do 3D design (also called CAD\\footnote{Computer-Aided Design.}). The 3D design was a major step in the product design because it enabled to see the product and to work on parts assemblies in a more ergonomic and productive way. In order to gather all the 3D models in a same place and to enable to load in a certain 3D environment\\footnote{This practice is called design in context''. The designer will load a certain amount of parts that will create its 3D working environment. In that environment, the designer will be able to design the new or modified parts according to the requirements.}, new management software'' appeared. The objective was to gather in the same place all documentation linked to parts or assemblies (the 3D models, the testing documents, material information, part number, etc.). Those software are generally called PDM\\footnote{Part Data Management.} With time, the PDM products proposed several features that turn them into PLM (product lifecycle management) systems (see Fig.~\\ref{fig:plm-functions}): \\begin{itemize} \\itemsep0em \\item A management of a tree representing'' the product\\footnote{For the recent programs, the tree is split conforming to the ATA chapters.} also called the product structure''; \\item A management of versions of parts and assemblies, linked to business objects called ''changes''\\footnote{The changes enable to keep track of the modifications of the A/C and enable to justify to the authority the evolutions of a specific instance of A/C compared to the certified baseline A/C.}; \\item A management of applicability that enables to filter out the product structure to create a list of all parts and assemblies applicable to a specific A/C serial number. \\end{itemize} \\imagepng{plm-functions}{The main features of a PLM}{0.7} The core features of the Fig.~\\ref{fig:plm-functions} are mostly centered on the product, \\emph{seen from the engineering office}. The Fig.~\\ref{fig:hl01} shows the high level processes covered by the ``traditional'' PLM scope. \\imagepng{hl01}{High level view of the covered processes}{0.2} With time, the PLM vendors added some modules to integrate their ``engineering PLM'' to other systems, in order to manage the lifecycle of the product, not only in the design office but also in the other divisions (manufacturing and support). %==============SECTION \\section{The integration of manufacturing engineering into the PLM} The manufacturing engineering activities are the activities that design the manufacturing process. In a world where the product is the result of the assembly of subcomponents (such as MCAs and CA\\footnote{Major Component Assembly and Component Assembly.}), the design of the manufacturing process is at the center of an OEM\\footnote{Original Equipment Manufacturer.} productivity. Specific products (such as Delmia) are proposing features to do process design. For sure, in order to design properly an assembly process, the software must use the exact list of parts to be assembled\\footnote{Called the manufacturing Bill Of Materials'', or mBOM''.}. The link to the PLM is quite obvious: if the PLM could provide a way to export the BOM to the manufacturing engineering tool, there would be a ``digital continuity'' between the design office and the industry\\footnote{Indeed, the process is a bit more complex than that because the engineering is creating the engineering BOM (or eBOM) that must be transformed in mBOM (the real BOM to be used). The mBOM is taking into account production constraints such as stocks.}. The \\ds\\ \\tdxp\\ proposes a deep integration of the engineering PLM and of the manufacturing engineering environment. Using the 3D of the design office enables to design the plant work stations, to optimize the assembly procedures, to design new tools, etc. This coupling of the two worlds is a major evolution in the landscape of PLM\\footnote{In the H160 PLM toolchain, \\ah\\ put in place an interface between CorePDM (a PTC Windchill-based PLM system) and \\ds\\ Delmia v5. This integration was complex and painful.}\\footnote{Even if the integration between PLM and Delmia is deep in the \\tdxp, the correct configuration of the bridges between the two worlds is not trivial.}. \\imagepng{hl02}{PLM scope with manufacturing engineering}{0.35} The Fig.~\\ref{fig:hl02} is showing the extension of PLM scope to manufacturing engineering\\footnote{The manufacturing configuration is currently master in SAP, because only the ERP has the real vision of the available parts. For sure, the manufacturing configuration is managed inside the PLM (Delmia) because assembly operations are attached to parts of the mBOM.}. %==============SECTION \\section{Integrating the manufacturing execution into the PLM?} The current process is as follows: the process that is designed in Delmia will go to SAP to be ``instantiated'' in the context of a specific instance of assembly. SAP has all the knowledge of the stocks, the workers and the real work stations (and their real availability). The core production process takes place in SAP. \\ds\\ is proposing a software to manage the manufacturing execution on the shop floor (Apriso). This tool has several intentions: \\begin{itemize} \\itemsep0em \\item Being able to provide to the blue collars a detailed documentation of the tasks to do (with potentially 3D models); \\item Being able to provide a higher level of details of the tasks managed by SAP; \\item Being able to capture measures in production through connected devices (like torque values with IoT keys). \\end{itemize} Even if \\ds\\ is working to integrate Apriso to the \\tdxp, this integration is not easy because, traditionally, Apriso is integrated with SAP (even if the detailed documentation can come from Delmia\\footnote{Which is the case for H160.}), so with the instance'' of process enabling to assemble a specific S/N and not with the template'' of process (managed in Delmia). \\imagepng{hl03}{Integrating the manufacturing executing to the PLM}{0.5} The Fig.~\\ref{fig:hl03} shows the extension of the PLM scope to manufacturing execution. %==============SECTION \\section{Integrating the ILS into the PLM} In CorePDM, \\ah\\ developed a specific module to manage the ILS into the H160 PLM. This integration is quite deep: it proposes an ILS tree with connections to the engineering product structure, and a mechanism to automatically identify the impact of an engineering change on the ILS tree. \\ds\\ is also going in that direction, with a prototype of ILS integration\\footnote{So-called S-View'' and S-BOM'' in the \\ds\\ language.} into the \\tdxp, led with Safran Aircraft Engines\\footnote{Logically, \\ds\\ is trying to leverage Delmia 3DX to adapt it to the ILS context.}. \\imagepng{hl04}{Integrating the ILS to the PLM}{0.65} The Fig.~\\ref{fig:hl04} shows the extension of the PLM scope to ILS. %==============SECTION \\section{Integrating the MBSE into the PLM} When a product is being built, it obeys to a certain set of requirements. In the system engineering area, a standard process was defined to go from the requirements to the physical view: this process is called RFLP'', standing for requirements, functional, logical, physical''. The objective of this process is to be able to create a digital link between the requirements and the design of the product. When this link is maintained over time, any modification on the product can be traced back to a certain requirement. This can be particularly useful for safety requirements, for instance, that should never be put at stake by a product modification. Those methods are traditionally used in electronic and electrical systems, but, with time, they are being used more and more for other A/C systems\\footnote{In \\ah, the dynamic systems organization of the engineering office is studying to apply this methodology to safety requirements.} \\imagepng{hl05}{Integrating the MBSE into the PLM}{1.0} If some \\ds\\ software already propose a partial implementation of the RFLP\\footnote{In Catia v6 for mechanical engineering, for instance.}, specific tools are often used to run this process. Those tools propose ways to graphically model the various views leading to the physical view (2D or 3D). That is why this process is called MBSE'', standing for Model-Based System Engineering'', or more generally nowadays MBE'' for Model-Based Engineering''. Software companies such as \\ds\\ try, progressively to integrate MBE tools into their suite of product\\footnote{NoMagic company was acquired in 2017 by \\ds\\ to integrate their main product Cameo into the \\tdxp.}. That extension is shown on Fig.~\\ref{fig:hl05}. %==============SECTION \\section{The PLM, an ever-growing software product?} Traditionally, interfaces between various software of the global ``product lifecycle'' process were difficult to set-up and to stabilize. \\aca\\ or \\ah\\ are not exceptions to this status. Due to the difficulty of interoperability between products\\footnote{Certain engineering data are not easy to transfer, for instance the 3D models. 3D files can be exchanged in STEP standardized formats but, in some cases, some data loss can occur in those exchanges.}, due also to a habit of undersizing the interface realization budget, and a habit of minimization of the complexity of the interfaces, the interfaces realization has often been a nightmare for business and IT teams. This fact created a kind of belief that the interface between systems problem is disappearing if all the software are integrated into the same platform''. This platform will aggregate and integrate with time more and more processes and softwares in a kind of ever-growing end-to-end'' platform. This is the \\ds\\ approach, a ``everything in the same platform'' approach\\footnote{For years, the same approach justified many custom developments into the SAP platform.}. %==============SECTION \\section{Coupling, urbanization and recurring costs} On an enterprise architecture standpoint, integrating several products into the same platform can sometimes be a good idea, especially if the interfaces between systems are complex and both ways. \\imagepng{urba01}{Different software for different users versus platform}{0.6} The fact is this approach generally brings many drawbacks (see Fig.~\\ref{fig:urba01} for a graphical comparison of the two models): \\begin{itemize} \\itemsep0em \\item Integrating softwares into the same platform does not reduce the functional complexity of exchanges between areas'' in the same platform (see the internal platform bridges'' in Fig.~\\ref{fig:urba01}); \\item The implementation of the platform is more complex than a double implementation of softwares, because the core model of the platform is more complex and more actors must be put around the table to find the right implementation options; \\item Making evolutions in a platform is generally more costly and more complex (touching one parameter in an area can have unexpected impacts in some other areas); \\item It creates couplings in terms of upgrade: everyone working on the same platform will have to cope with the new release of the platform \\emph{at the same time}, whether the timing is relevant or not for the business. \\end{itemize} We have to note that the everything in the same platform'' approach (so-called mainframe approach'' or ``monolith approach'' in the IT world) is \\emph{diametrically opposed} to the approach developed for decades in other businesses, especially in the services area (banking, insurance, travel, public services, etc.). In those businesses, the \\emph{urbanization} of the IT systems and the \\emph{decoupling} of IT systems are fundamental preoccupations that can be summarized by the following interrogations: \\begin{itemize} \\itemsep0em \\item In what software do we put what functionality for whom? \\item What business processes should be optimized? \\item Are there some areas of business still running with Excel only? What software could be deployed to integrate their processes to the other company processes? What kind of digital continuity is required? \\item What are the relevant interfaces between systems? How to minimize them? How to make them reusable to avoid ``point-to-point'' interfaces? What middleware must we use? What governance to put in place for distributed systems? \\item What systems should be split to increase business agility (a business will evolve independently from another)? \\item What systems should be aggregated to increase company performance? \\item What optimal systems map can lead to the best trade-off between agility and low IT recurring costs (RC)? \\end{itemize} Recurring costs is one of the most critical point. The services industry struggled to get out of the mainframes because they knew what constraints were bringing those approaches in terms of costs and couplings\\footnote{In terms of commercial dependencies also: when all programs run running on IBM mainframes, IBM could increase its price without any organization to have the capability of negotiating.}: the bigger the system, the more expensive to evolve. With integrated end to end platforms like \\tdxp\\footnote{Which might evolve to an integration of ERP, as \\ds\\ bought recently an ERP solution.}, the industrial world is taking \\emph{another road} than the road of the service businesses, another road than the worldwide IT community that seeks for decades to get rid of monoliths\\footnote{Since the 90s, the IT world is creating techniques to decouple the systems (distributed systems) and to professionalize their complex exchanges: middlewares, enterprise service bus (ESB), ETL, web services and APIs are part of this long term trend.}. Note that getting rid of monoliths is \\emph{not an IT objective}, but a way to ensure that the IT systems supporting the business will be able to evolve as fast as the competition. As \\tdx\\ is concerned, the question is: what functional scope should be in the \\tdxp\\ and what should remain outside the platform? We provide first elements in the following section. %==============SECTION \\section{Enterprise architecture recommendations} \\subsection{The core strength of \\tdx} After manipulating the \\tdx\\ product, and after reading many documentations from \\ds, being about the current product or the \\tdx\\ roadmaps, we consider that the \\tdxp\\ is very innovative and efficient in the digital management of the product line (with variants and options) in connection to the manufacturing engineering (scope already covered by Enovia 3DX and Delmia 3DX). A real integration effort has been realized by \\ds\\ and seems particularly adapted to the \\ah\\ industrial strategy of \\emph{site specialization} and MCA/CA-based industry: The product enables to define MCAs with variants and options and one or several industrial models attached. The \\tdx\\ seems particularly adapted to the H160 program, and to the H175 program, programs that are implementing a distributed industrial model. %TODO Ajouter pour le programme 175 la mention aux partenaires cho=inois with a special study to be led for the partner management (Chinese and French partners). \\subsection{Investing on what works, investigating the rest} \\label{corescope} The EA recommendation is to create a formal distinction between what seems really usable and what is still under construction in the \\tdx\\ product. Currently, the scope that seems well taken in charge by the \\tdx\\ suite is the CAD (Catia 3DX), the product line, variants and options, the \\cm\\ and the change management (with Enovia 3DX) and the manufacturing engineering (with Delmia 3DX). For \\ah, in a context where those products should be use as ``Out-Of-The-Box'' as possible, this scope seems the right scope to invest into. Out of this core scope, \\ah\\ should, for sure, participate to the investigations (so-called ``incubators'') that are put in place at the \\ag\\ level, in the DDMS program. \\styledreco{core} That means: Catia 3DX, Enovia 3DX (variant management, configuration management, change management) and Delmia 3DX\\footnote{The support functions of \\tdx\\ (that could one day enable to decommission ILS H175) will not be ready before 2021 best case scenario.}. The three business domains of the industry An industrial company is generally architectured on 3 business domains: Engineering Office: Where the products are designed, and certified in some industries; Industry: Where the products are built; Support: Where the products are supported. Those three domains, while working on the same data, have very distinct constraints and their optimal interfacing is a the center of many problems and of many marketing speeches.","title":"About End-To-End PLM"},{"location":"articles/about-e2e-plm/#about-end-to-end-plm","text":"As time goes on, the product lifecycle management (PLM) systems propose more and more functionality, linked together, in platforms that tend to be always bigger with time. The increasing size of the PLM platforms, and their pretention to take in charge more and more processes, is answering to a dream of many users to have \"all data linked together in the same place\". If this dream can be true for small products, it seems to us as a dangerous chimera in the case of complex manufacturing products such as aircrafts. In this article, we will explain the core reasons why we don't believe that this model of the \"unique backbone\" is applicable everywhere. Indeed, when changing semantic domains, the data needs to be transformed. If the tertiary sector takes as granted the adapter pattern (on which we will come back) for a long time, some PLM vendors are selling a model where the adapter can be reduced to links between data . This article is explaining the reasons why, in complex cases, this simplistic approach may appear as very dangerous.","title":"About End-To-End PLM"},{"location":"articles/about-e2e-plm/#plm-nature-of-data-and-adaptation","text":"We explained in a recent article what we consider as being the basic functions of a PLM. We recommend the reading of this article before coming back to this one. In that article, we showed that, data being represented as a graph, it appears as quite easy to connect whatever object of a certain type to whatever other object of any other type with an instance of a link type. If we follow this statement, we quickly end up with the \"no-limit approach\" that could be be stated as follows: As log as data can be connected together, then we can manage them in the same software . However, contrary to what could appear, this model of linking data together has limits. Especially, in many cases, representing depedencies between business domains as links at the data level are a very bad representation of reality. In order to understand correctly the problem, we first recommended to have a look at the real nature of data article. In this article, we explain that data are created by business processes and that the way they are structured is dependant on the business domain. When data must flow from a business domain to another business domain, it must be \"adapted out\" the source business domain (transformed and reduced to a format that will be understandable by both the initiator and the receiver). Then the receiver will \"adapt the data in\" its own business universe. Then the receiver will be able to use the data. We often name the business domains \"semantic domains\", because when we change a data from business domain, it is generally not meaning exactly the same thing, and it is often not structured in the same way. For instance, if a \"part\" is created in the engineering semantic domain and sent to the support, this engineering part may be transformed into several \"support parts\" that will have support attributes and be localized at different use points. Concerning the engineering data attached to the part, most of them won't be useful for the support, even if they need to be secure in engineering system. For sure, the bridges between systems must ensure some traceability that enables to track back the same entity between systems. https://www.linkedin.com/feed/update/urn:li:activity:6804804965635239936/?commentUrn=urn%3Ali%3Acomment%3A(activity%3A6804804965635239936%2C6809574091432054784) Hi Axel, I know the OSLC initiative. I like the idea that the central repository (monolith) is not the solution and that the industry has to think in distributed systems. However, what does not convince me fully is that the OSLC approach is still at the data level. If this statement is true, if links can be done at the data level between various data models in various systems, then conceptually it pushes again for more monoliths (super PLMs that can manage several business domains). Conceptually, the semantic web is using just a small subset of the possible distributed IT architectures by establishing a global data model that is spread amongst systems, but is still representing the reality under the form of a global data model. The fact is the tertiary sector is not working on this paradigm: data are spread between systems but there are many places where data are transformed from one semantic domain to another one, with business rules, aka treatments, aka programs with functions. I try to introduce this concept here: https://orey.github.io/papers/articles/data-interop/ (following) Coming from outside of the industry, I am very puzzled about this systematic belief that data can be linked, as if a unique semantic representation was possible. In my article, I try to explain that modeling the reality can be done in many ways, and that the software industry (outside industry) proposed interchange standards that are enabling a program-based adaptation of data semantics between various business domains. And, what is very surprising to me also is that, when we look carefully to the existing systems, they are full of business rules and data transformation anyway. The first article on graph data in PLM is just an introduction which opens on another article on the structure of the bridges between systems. In a general way, those bridges, if they obey to the same rules as we can find in all the software industry - except industry - can embed some code to transform the semantics, so-called \"adapters\", that contain business rules. Maybe industry is the exception that confirms the rule and links between systems can be reduced to the very simple case of \"data links\", but I fear this statement is too reductive. (see message 3) (message 3) And it pushes for more monoliths and huge PLM platforms. I fear that this data-oriented approach will cause massive project failures in the future. For me, this is also a core structural problem of the semantic web. If you think that the reality can be described by linked data only, you just consider a modeling that is static and structural, but you omit the dynamic transformations that are also part of the representation of reality. When we see the troubles of data reconciliation in the semantic web, it is for me the illustration that this approach is not sufficient. Modeling the reality is a \"domain-specific\" activity and this is not because various domains can agree on an exchange protocol that their various semantics can be aggregated in a single model of linked data. I intend to bring more arguments of this fact in my next article. In all cases, it seems to me that this is a very interesting and crucial topic for the industry. L'autre id\u00e9e \u00e9videmment est que le PLM n'est plus g\u00e9n\u00e9rique mais compl\u00e8tement adapt\u00e9 \u00e0 un business domain - d'o\u00f9 l'id\u00e9e des ASD This section will try to explain, in simple words, how the PLM plays a central role in the transformation of the aerospace industry in the last decades. The legacy programs of the \\ag, such as the A320, A330, H225, etc., were designed originally without 3D models\\footnote{For the oldest programs, the drawings were made with paper and Chinese ink.}. Progressively, with the new workstations, appeared a generation of software enabling to do 3D design (also called CAD\\footnote{Computer-Aided Design.}). The 3D design was a major step in the product design because it enabled to see the product and to work on parts assemblies in a more ergonomic and productive way. In order to gather all the 3D models in a same place and to enable to load in a certain 3D environment\\footnote{This practice is called design in context''. The designer will load a certain amount of parts that will create its 3D working environment. In that environment, the designer will be able to design the new or modified parts according to the requirements.}, new management software'' appeared. The objective was to gather in the same place all documentation linked to parts or assemblies (the 3D models, the testing documents, material information, part number, etc.). Those software are generally called PDM\\footnote{Part Data Management.} With time, the PDM products proposed several features that turn them into PLM (product lifecycle management) systems (see Fig.~\\ref{fig:plm-functions}): \\begin{itemize} \\itemsep0em \\item A management of a tree representing'' the product\\footnote{For the recent programs, the tree is split conforming to the ATA chapters.} also called the product structure''; \\item A management of versions of parts and assemblies, linked to business objects called ''changes''\\footnote{The changes enable to keep track of the modifications of the A/C and enable to justify to the authority the evolutions of a specific instance of A/C compared to the certified baseline A/C.}; \\item A management of applicability that enables to filter out the product structure to create a list of all parts and assemblies applicable to a specific A/C serial number. \\end{itemize} \\imagepng{plm-functions}{The main features of a PLM}{0.7} The core features of the Fig.~\\ref{fig:plm-functions} are mostly centered on the product, \\emph{seen from the engineering office}. The Fig.~\\ref{fig:hl01} shows the high level processes covered by the ``traditional'' PLM scope. \\imagepng{hl01}{High level view of the covered processes}{0.2} With time, the PLM vendors added some modules to integrate their ``engineering PLM'' to other systems, in order to manage the lifecycle of the product, not only in the design office but also in the other divisions (manufacturing and support). %==============SECTION \\section{The integration of manufacturing engineering into the PLM} The manufacturing engineering activities are the activities that design the manufacturing process. In a world where the product is the result of the assembly of subcomponents (such as MCAs and CA\\footnote{Major Component Assembly and Component Assembly.}), the design of the manufacturing process is at the center of an OEM\\footnote{Original Equipment Manufacturer.} productivity. Specific products (such as Delmia) are proposing features to do process design. For sure, in order to design properly an assembly process, the software must use the exact list of parts to be assembled\\footnote{Called the manufacturing Bill Of Materials'', or mBOM''.}. The link to the PLM is quite obvious: if the PLM could provide a way to export the BOM to the manufacturing engineering tool, there would be a ``digital continuity'' between the design office and the industry\\footnote{Indeed, the process is a bit more complex than that because the engineering is creating the engineering BOM (or eBOM) that must be transformed in mBOM (the real BOM to be used). The mBOM is taking into account production constraints such as stocks.}. The \\ds\\ \\tdxp\\ proposes a deep integration of the engineering PLM and of the manufacturing engineering environment. Using the 3D of the design office enables to design the plant work stations, to optimize the assembly procedures, to design new tools, etc. This coupling of the two worlds is a major evolution in the landscape of PLM\\footnote{In the H160 PLM toolchain, \\ah\\ put in place an interface between CorePDM (a PTC Windchill-based PLM system) and \\ds\\ Delmia v5. This integration was complex and painful.}\\footnote{Even if the integration between PLM and Delmia is deep in the \\tdxp, the correct configuration of the bridges between the two worlds is not trivial.}. \\imagepng{hl02}{PLM scope with manufacturing engineering}{0.35} The Fig.~\\ref{fig:hl02} is showing the extension of PLM scope to manufacturing engineering\\footnote{The manufacturing configuration is currently master in SAP, because only the ERP has the real vision of the available parts. For sure, the manufacturing configuration is managed inside the PLM (Delmia) because assembly operations are attached to parts of the mBOM.}. %==============SECTION \\section{Integrating the manufacturing execution into the PLM?} The current process is as follows: the process that is designed in Delmia will go to SAP to be ``instantiated'' in the context of a specific instance of assembly. SAP has all the knowledge of the stocks, the workers and the real work stations (and their real availability). The core production process takes place in SAP. \\ds\\ is proposing a software to manage the manufacturing execution on the shop floor (Apriso). This tool has several intentions: \\begin{itemize} \\itemsep0em \\item Being able to provide to the blue collars a detailed documentation of the tasks to do (with potentially 3D models); \\item Being able to provide a higher level of details of the tasks managed by SAP; \\item Being able to capture measures in production through connected devices (like torque values with IoT keys). \\end{itemize} Even if \\ds\\ is working to integrate Apriso to the \\tdxp, this integration is not easy because, traditionally, Apriso is integrated with SAP (even if the detailed documentation can come from Delmia\\footnote{Which is the case for H160.}), so with the instance'' of process enabling to assemble a specific S/N and not with the template'' of process (managed in Delmia). \\imagepng{hl03}{Integrating the manufacturing executing to the PLM}{0.5} The Fig.~\\ref{fig:hl03} shows the extension of the PLM scope to manufacturing execution. %==============SECTION \\section{Integrating the ILS into the PLM} In CorePDM, \\ah\\ developed a specific module to manage the ILS into the H160 PLM. This integration is quite deep: it proposes an ILS tree with connections to the engineering product structure, and a mechanism to automatically identify the impact of an engineering change on the ILS tree. \\ds\\ is also going in that direction, with a prototype of ILS integration\\footnote{So-called S-View'' and S-BOM'' in the \\ds\\ language.} into the \\tdxp, led with Safran Aircraft Engines\\footnote{Logically, \\ds\\ is trying to leverage Delmia 3DX to adapt it to the ILS context.}. \\imagepng{hl04}{Integrating the ILS to the PLM}{0.65} The Fig.~\\ref{fig:hl04} shows the extension of the PLM scope to ILS. %==============SECTION \\section{Integrating the MBSE into the PLM} When a product is being built, it obeys to a certain set of requirements. In the system engineering area, a standard process was defined to go from the requirements to the physical view: this process is called RFLP'', standing for requirements, functional, logical, physical''. The objective of this process is to be able to create a digital link between the requirements and the design of the product. When this link is maintained over time, any modification on the product can be traced back to a certain requirement. This can be particularly useful for safety requirements, for instance, that should never be put at stake by a product modification. Those methods are traditionally used in electronic and electrical systems, but, with time, they are being used more and more for other A/C systems\\footnote{In \\ah, the dynamic systems organization of the engineering office is studying to apply this methodology to safety requirements.} \\imagepng{hl05}{Integrating the MBSE into the PLM}{1.0} If some \\ds\\ software already propose a partial implementation of the RFLP\\footnote{In Catia v6 for mechanical engineering, for instance.}, specific tools are often used to run this process. Those tools propose ways to graphically model the various views leading to the physical view (2D or 3D). That is why this process is called MBSE'', standing for Model-Based System Engineering'', or more generally nowadays MBE'' for Model-Based Engineering''. Software companies such as \\ds\\ try, progressively to integrate MBE tools into their suite of product\\footnote{NoMagic company was acquired in 2017 by \\ds\\ to integrate their main product Cameo into the \\tdxp.}. That extension is shown on Fig.~\\ref{fig:hl05}. %==============SECTION \\section{The PLM, an ever-growing software product?} Traditionally, interfaces between various software of the global ``product lifecycle'' process were difficult to set-up and to stabilize. \\aca\\ or \\ah\\ are not exceptions to this status. Due to the difficulty of interoperability between products\\footnote{Certain engineering data are not easy to transfer, for instance the 3D models. 3D files can be exchanged in STEP standardized formats but, in some cases, some data loss can occur in those exchanges.}, due also to a habit of undersizing the interface realization budget, and a habit of minimization of the complexity of the interfaces, the interfaces realization has often been a nightmare for business and IT teams. This fact created a kind of belief that the interface between systems problem is disappearing if all the software are integrated into the same platform''. This platform will aggregate and integrate with time more and more processes and softwares in a kind of ever-growing end-to-end'' platform. This is the \\ds\\ approach, a ``everything in the same platform'' approach\\footnote{For years, the same approach justified many custom developments into the SAP platform.}. %==============SECTION \\section{Coupling, urbanization and recurring costs} On an enterprise architecture standpoint, integrating several products into the same platform can sometimes be a good idea, especially if the interfaces between systems are complex and both ways. \\imagepng{urba01}{Different software for different users versus platform}{0.6} The fact is this approach generally brings many drawbacks (see Fig.~\\ref{fig:urba01} for a graphical comparison of the two models): \\begin{itemize} \\itemsep0em \\item Integrating softwares into the same platform does not reduce the functional complexity of exchanges between areas'' in the same platform (see the internal platform bridges'' in Fig.~\\ref{fig:urba01}); \\item The implementation of the platform is more complex than a double implementation of softwares, because the core model of the platform is more complex and more actors must be put around the table to find the right implementation options; \\item Making evolutions in a platform is generally more costly and more complex (touching one parameter in an area can have unexpected impacts in some other areas); \\item It creates couplings in terms of upgrade: everyone working on the same platform will have to cope with the new release of the platform \\emph{at the same time}, whether the timing is relevant or not for the business. \\end{itemize} We have to note that the everything in the same platform'' approach (so-called mainframe approach'' or ``monolith approach'' in the IT world) is \\emph{diametrically opposed} to the approach developed for decades in other businesses, especially in the services area (banking, insurance, travel, public services, etc.). In those businesses, the \\emph{urbanization} of the IT systems and the \\emph{decoupling} of IT systems are fundamental preoccupations that can be summarized by the following interrogations: \\begin{itemize} \\itemsep0em \\item In what software do we put what functionality for whom? \\item What business processes should be optimized? \\item Are there some areas of business still running with Excel only? What software could be deployed to integrate their processes to the other company processes? What kind of digital continuity is required? \\item What are the relevant interfaces between systems? How to minimize them? How to make them reusable to avoid ``point-to-point'' interfaces? What middleware must we use? What governance to put in place for distributed systems? \\item What systems should be split to increase business agility (a business will evolve independently from another)? \\item What systems should be aggregated to increase company performance? \\item What optimal systems map can lead to the best trade-off between agility and low IT recurring costs (RC)? \\end{itemize} Recurring costs is one of the most critical point. The services industry struggled to get out of the mainframes because they knew what constraints were bringing those approaches in terms of costs and couplings\\footnote{In terms of commercial dependencies also: when all programs run running on IBM mainframes, IBM could increase its price without any organization to have the capability of negotiating.}: the bigger the system, the more expensive to evolve. With integrated end to end platforms like \\tdxp\\footnote{Which might evolve to an integration of ERP, as \\ds\\ bought recently an ERP solution.}, the industrial world is taking \\emph{another road} than the road of the service businesses, another road than the worldwide IT community that seeks for decades to get rid of monoliths\\footnote{Since the 90s, the IT world is creating techniques to decouple the systems (distributed systems) and to professionalize their complex exchanges: middlewares, enterprise service bus (ESB), ETL, web services and APIs are part of this long term trend.}. Note that getting rid of monoliths is \\emph{not an IT objective}, but a way to ensure that the IT systems supporting the business will be able to evolve as fast as the competition. As \\tdx\\ is concerned, the question is: what functional scope should be in the \\tdxp\\ and what should remain outside the platform? We provide first elements in the following section. %==============SECTION \\section{Enterprise architecture recommendations} \\subsection{The core strength of \\tdx} After manipulating the \\tdx\\ product, and after reading many documentations from \\ds, being about the current product or the \\tdx\\ roadmaps, we consider that the \\tdxp\\ is very innovative and efficient in the digital management of the product line (with variants and options) in connection to the manufacturing engineering (scope already covered by Enovia 3DX and Delmia 3DX). A real integration effort has been realized by \\ds\\ and seems particularly adapted to the \\ah\\ industrial strategy of \\emph{site specialization} and MCA/CA-based industry: The product enables to define MCAs with variants and options and one or several industrial models attached. The \\tdx\\ seems particularly adapted to the H160 program, and to the H175 program, programs that are implementing a distributed industrial model. %TODO Ajouter pour le programme 175 la mention aux partenaires cho=inois with a special study to be led for the partner management (Chinese and French partners). \\subsection{Investing on what works, investigating the rest} \\label{corescope} The EA recommendation is to create a formal distinction between what seems really usable and what is still under construction in the \\tdx\\ product. Currently, the scope that seems well taken in charge by the \\tdx\\ suite is the CAD (Catia 3DX), the product line, variants and options, the \\cm\\ and the change management (with Enovia 3DX) and the manufacturing engineering (with Delmia 3DX). For \\ah, in a context where those products should be use as ``Out-Of-The-Box'' as possible, this scope seems the right scope to invest into. Out of this core scope, \\ah\\ should, for sure, participate to the investigations (so-called ``incubators'') that are put in place at the \\ag\\ level, in the DDMS program. \\styledreco{core} That means: Catia 3DX, Enovia 3DX (variant management, configuration management, change management) and Delmia 3DX\\footnote{The support functions of \\tdx\\ (that could one day enable to decommission ILS H175) will not be ready before 2021 best case scenario.}.","title":"PLM, nature of data and adaptation"},{"location":"articles/about-e2e-plm/#the-three-business-domains-of-the-industry","text":"An industrial company is generally architectured on 3 business domains: Engineering Office: Where the products are designed, and certified in some industries; Industry: Where the products are built; Support: Where the products are supported. Those three domains, while working on the same data, have very distinct constraints and their optimal interfacing is a the center of many problems and of many marketing speeches.","title":"The three business domains of the industry"},{"location":"articles/about-plm/","text":"PLM and graph data Photo by Lotus Head from FreeImages This article is the first of a series on PLM. It aims to describe the main core functions of a PLM by taking the perspective of graph data. As the industry is beginning to look towards IT a bit more seriously that what was done in the last decades, it may be important for IT people inside and outside the industry to understand the basics of a PLM. This can lead to points of comparison with other kinds of IT systems that are met frequently outside the industry (in the so-called \"tertiary\" sector). What is a PLM system? PLM means Product Lifecycle Management. This acronym generally describes systems that are used to manage the \"lifecycle\" of a manufacturing product (we will come back on the lifecycle notion). Indeed, a complex manufacturing product is composed of several parts that will evolve during its production life. The PLM system is supposed to enable the management of product data all along its lifetime and for several stakeholders concerned by it (engineering for the product design, industry for the product manufacturing and support for the product support). A graph of business objects We will propose the following IT definition of the PLM: A PLM is an IT system that is essentially manipulating a graph of \"business objects\" . We can define a business class as a conceptual entity with: A type (for instance \"Part\"); A set of characteristics (also called attributes, fields or properties): Those attributes can be typed data (strings, numerical values, currencies, date, etc.) but also documents such as Office documents, 2D or 3D drawings, raw data resulting from simulation testing, etc.); A set of lists of authorized links towards other classes. Modern PLMs propose standard classes (we will come back on that point), but it is also possible to parameterize custom classes. Once a class is defined, the PLM system enables to create instances of the class, what are called \"objects\". Each business object will be of a certain class type and will have different values for its attributes (see Figure 1). Figure 1: Structural view of a business class The basic functions of the PLM will be to enable the management of one product graph by a more or less large team of people from different organizations. The lifecycle of a business object The notion of lifecycle is often not clear for IT people that come from other areas than industry. The reason is the tertiary sector is not really using this term and prefers to speak about \"states\" or \"state machines\". In the manufacturing industry, it is important to understand the lifecycle of the data that are representing the product. We can say that every data, for instance data associated to a part, will have schematically 2 different states: It can be \"in work\": people are working on a part but as the work is not finished, the data are not available for the rest of the community; It can be \"released\": the data is considered as stable and is published to the rest of the community. Indeed, many versions of the same business objects can be stored in an \"in work\" status before being released. Also, if a part evolves, several versions will be released with time (which opens the topic of \"applicability\" that we will see later on). In other words, industrial data is managed as IT people manage software source code, and not really as IT people manage data in the context of tertiary sector applications. In the tertiary sector, the \"management data\" are generally stored with other purposes: It is required to be able to access to the data that are valid today; It is required to be able to access to important previous versions of data for auditing purposes (for instance before a contract modification); But it is not generally required to secure every change on data. Note that the current way of managing data in the IT world is generating technical debt and graph-oriented techniques enable to solve that issue (see here ). Coming back to the industry, industrial data being inside a graph, every change on a \"node\" of this graph (a business object) may have impacts on the linked objects. Let us consider Figure 2. Figure 2: Basic evolution of a graph of objects Originally, all objects of the graph are considered as being in version 1. The instance A1 of A changes to version 2. A1 V2 can have different values in its attributes but the change implies questions about the links A1 maintains with B1 , B2 and C1 . Are they still valid? Maybe, on an industrial standpoint, some tests must be done (or digitally, or physically) to assess this point. Let's now consider Figure 3 now where B1 is evolving. Figure 3: Evolution of a linked object Must A1 evolve too? If B1 V1 was an error, maybe A1 V1 should be transformed in A1 V2 pointing to B1 V2, and A1 V1 should be made not usable by anybody... The notion of change In Figure 3, we can see that it would probably make sense to make a new version of A1 in order to take into account the change in B1 . Let us name Change1 the change that transforms B1 V1 into B1 V2. Change1 is an instance of the the Change class, which is a business object. When defining it in the first step, Change1 will be in version 1. But if, due to Change1 , we have to change A1 V1 into A1 V2, then this change should also be associated to Change1 . The Figure 4 is showing the change of B1 and then the change of A1 . Figure 4: The change business object The Change1 change is referencing in a first step only the previous and present version of B1 and in a second step the previous and current version of A1 . Note that we could have created 2 versions of the Change1 , one for the original change of B1 and the second one for the change of B1 and the impacted change on A1 . This kind of choice will depend on the industrial change policy. In the industry, the change object is crucial and often embeds many data including documents. There are many reasons for that, to begin with the costs of a change. Indeed, any change on a manufactured product may imply costly impacts on the production line and on the support of the product. In some industries like aerospace, the product may be \"certified\" (airworthiness for instance). Any change to the product will imply a certification impact assessment that could result in costly re-certification. Another reason to manage changes is to be able to find back in the past what reason caused a change that resulted in product performance regression or problem. Generally, the change object is at the core of a special process, often named \"change board\" or \"change committee\", where many stakeholders gather in order to determine if the change is worth doing, and if yes, what is the best solution (or the less worst). About the aggregation link In the previous figures (from 1 to 4), the links between business objects were standard arrows, such as the one on the left of the Figure 5 below. Figure 5: The semantics of links On the right of Figure 5, IT people will have recognized a link with a white diamond shape. For UML users, this link means \"aggregation\". Aggregation can be considered as a special case of relationship between 2 classes: It just bears more meaning than the simple arrow, the meaning that if A1 is aggregating B1 , that means that A1 is a \"bigger\" structure than B1 and A1 is some kind of \"container\" object. The aggregation link asks an important question in the PLM system: What is the elementary brick that contains only data and no other sub-brick? This should be a convention of the particular industry. The ambiguous notion of part Generally, a manufacturing product can be seen as made of \"parts\". There are conceptually 2 kinds of parts: The elementary parts: The ones that cannot be split into smaller parts; The assemblies: The parts that are containing other parts. This notion is very important because: Defining properly the level of elementary parts is defining the minimal scale at which a certain business will manage its data lifecycle; Different businesses are often seeing the product at different levels (in terms of \"elementary\" parts), for instance support and engineering in aerospace. The industry often refers as \"part\" something that can be or an elementary part or an assembly (because both entities usually have part numbers). This is often confusing and leads to misunderstanding with IT people. In some cases, it could be clearer to talk about a \"leaf of the graph\" or just a \"leaf\" as being the agreed lowest level of data managed in lifecycle, and about \"containers\" if the object managed is a container for other objects. For instance, an assembly being a set of parts, the parts in that context could be \"leafs\", and the assembly would be a container. For sure, a container has fundamentally the exact same structure as the leaf, the structure shown in Figure 1: It has properties with real data attached to its instance; It has links to other business objects. The only difference between a leaf and a container is that, in the container case, there is at least one link that can be considered semantically as an \"aggregation\" (see Figure 5). A leaf should have no aggregation link. Product structure, aggregation and semantic ambiguity As soon as we define a particular kind of links (aggregation), we can define \"trees\" which are partial representation of the global product graph just looking at the aggregation links between containers and leafs. This is often called: \"product structure\". Even if this view is still very used in the industry, we must never forget that it is only a very partial (and potentially misleading) view of the product (moreover, this view can be insufficient for several stakeholders, such as the support engineering in aerospace). Indeed, semantically typing the links is opening the door to a lot of semantic ambiguities. If it seems obvious that a assembly can be represented as an object that \"contains\" other objects that would be \"parts\", the fact that a group of components are stored in the same functional area (e.g. an area named \"all assemblies related to air conditioning\") introduced a double meaning in the aggregation link: The original meaning was: A physical component is physically the result of the assembly of smaller sub-components; The second meaning of aggregation would be an administrative way of gathering things (like a set). The result is that, in the same \"product structure\", you can have quickly two or more different semantic interpretations of the same \"aggregation\" link. Figure 6: The danger of the aggregation link In the Figure 6 above, it is easy to see that the aggregation link tagged 1 is representing the physical aggregation, and the aggregation link tagged 2 is representing the administrative classification. In reality, things are often much worse. Indeed, we usually can find many different concepts in the \"product structure\": The organization of the company (each organization will have a sub-tree with its own components); The variants of the product; The various versions of the same component (named \"variable component\" and aggregated under a \"non-variable component\"); Etc. The influence of ERP systems on modern PLM systems Modern PLM systems try to avoid this confusion brought by the \"tree vision\" of the product, by proposing other business classes and objects that enable the modeling of the industrial concepts. For instance, the organization of the company should have an impact on access rights but not on the structure of the graph of objects. The variants of the products can also be managed with new concepts that enable to manage smoothly options compatibility and incompatibility. Versions of the same business object can be managed without all versions to appear in the \"product structure\". The modern PLM systems propose more and more business classes that everybody need in the industry. Even if, structurally, those objects are still conforming to the Figure 1 definition, they represent in a much more accurate way the semantics of the industry, being engineering, manufacturing or support. For sure, those objects also come with their links to other domain specific business objects. We could say that the PLM systems evolved as the ERP systems did in the last decades: towards a better description of the business semantics, while keeping the core graph, lifecycle and change engine available. This, for sure, is a major step ahead most companies still don't benefit from, unfortunately. About applicability Before closing this first brief introduction to PLM systems concepts, we must stop a while on the applicability context. Let's consider Figure 4 right bottom blue area. We have many versions of A1 and B1 that are existing. What version is really \"applicable\" to a certain product that was produced in the assembly line Saturday May 1st 2021? In a graph, the simplest way to do it would be to have a business object representing the various instances of the product and to create a link between the instance number 12 of the product and the proper version of A1 and B1 it is using. This is what is shown in Figure 7 (with an intermediate object that represents the product version). Figure 7: Product evolution in product graph Product instance 12 and 13 are instances of the V1 of the product. After the change, 14 and 15 are instances of the V2 of the product. The idea here is to use the graph in order not to have to \"calculate anything\". As the product is made of released business objects, the product is a set of links to business objects (that on their side can also link to other objects). Here, we see the importance of having homogeneous semantic links, especially if the head of the tree (the release of the product) is supposed to be the head of the full product tree for this particular version. For sure, if the graph is full of semantic ambiguity (like the ones we mentioned), we may end up in the necessity of adding complex management rules to get the real tree of applicable objects for a specific serial number (this is often called \"configuration calculation\"). Criticizing this model would need a dedicated article. All that we need to say is that modern PLM systems are more and more graph-aware and are managing the product configuration in manners that are used successfully for decades in the domain of complex huge softwares. To be simple: using the real power of graphs with adequate domain specific business objects leads to a very powerful industry management approach. The core features of the PLM systems If we attempt a synthesis of this brief introduction, we could say that the PLM system very core features appears to be: The capability of managing a graph of business objects; Business objects are instances of classes having fields, documents and links to each other; The functionality of managing the lifecycle of business objects: Version management, Change management. The functionality of proposing domain-specific business objects and domain-specific relationships between objects, each set of business objects/links enabling specific features (quite often sold as \"modules\" of the PLM system). Essentially, the real core of a PLM system is graph data . That implies that more advanced features linked to configuration and applicability can be made simple thanks to correct graph data usage. The complexity of graph data modeling Representing data as graphs is very powerful and used in many applications nowadays. It enables to dissociate (under certain conditions) the data from the the links data have together. However, implementing management rules in graphs is not easy (see the graph transformation page). Getting inspired by PLMs outside of industry If the PLM systems are not easy to understand by people coming from the tertiary sector (like banks, insurances or public sectors), in a world obsessed with data (see The real nature of data ), the introduction of graphs in management data is definitely a good idea (see here and there in this site). More and more, we see attempts of PLM use in the tertiary sector in business domains that are not obviously similar to industry use cases. But we have to remember that tertiary sector applications are managing a lot of business rules while industry are managing much less (that will be the object of a future article about \"end to end PLM\"). The challenge of tertiary sector applications is to benefit from the graph data approach while being able to implement and maintain complex business rules in a universe of graph data. This is not trivial and the purpose of most of the pages of this site. See also Other articles on the PLM series: Article 2: Configuration management of industrial products in PDM/PLM Article 3: The four core functions showing you need a PLM Other connected articles: The real nature of data ( May 2021, updated July 2021 )","title":"PLM and graph data"},{"location":"articles/about-plm/#plm-and-graph-data","text":"Photo by Lotus Head from FreeImages This article is the first of a series on PLM. It aims to describe the main core functions of a PLM by taking the perspective of graph data. As the industry is beginning to look towards IT a bit more seriously that what was done in the last decades, it may be important for IT people inside and outside the industry to understand the basics of a PLM. This can lead to points of comparison with other kinds of IT systems that are met frequently outside the industry (in the so-called \"tertiary\" sector).","title":"PLM and graph data"},{"location":"articles/about-plm/#what-is-a-plm-system","text":"PLM means Product Lifecycle Management. This acronym generally describes systems that are used to manage the \"lifecycle\" of a manufacturing product (we will come back on the lifecycle notion). Indeed, a complex manufacturing product is composed of several parts that will evolve during its production life. The PLM system is supposed to enable the management of product data all along its lifetime and for several stakeholders concerned by it (engineering for the product design, industry for the product manufacturing and support for the product support).","title":"What is a PLM system?"},{"location":"articles/about-plm/#a-graph-of-business-objects","text":"We will propose the following IT definition of the PLM: A PLM is an IT system that is essentially manipulating a graph of \"business objects\" . We can define a business class as a conceptual entity with: A type (for instance \"Part\"); A set of characteristics (also called attributes, fields or properties): Those attributes can be typed data (strings, numerical values, currencies, date, etc.) but also documents such as Office documents, 2D or 3D drawings, raw data resulting from simulation testing, etc.); A set of lists of authorized links towards other classes. Modern PLMs propose standard classes (we will come back on that point), but it is also possible to parameterize custom classes. Once a class is defined, the PLM system enables to create instances of the class, what are called \"objects\". Each business object will be of a certain class type and will have different values for its attributes (see Figure 1). Figure 1: Structural view of a business class The basic functions of the PLM will be to enable the management of one product graph by a more or less large team of people from different organizations.","title":"A graph of business objects"},{"location":"articles/about-plm/#the-lifecycle-of-a-business-object","text":"The notion of lifecycle is often not clear for IT people that come from other areas than industry. The reason is the tertiary sector is not really using this term and prefers to speak about \"states\" or \"state machines\". In the manufacturing industry, it is important to understand the lifecycle of the data that are representing the product. We can say that every data, for instance data associated to a part, will have schematically 2 different states: It can be \"in work\": people are working on a part but as the work is not finished, the data are not available for the rest of the community; It can be \"released\": the data is considered as stable and is published to the rest of the community. Indeed, many versions of the same business objects can be stored in an \"in work\" status before being released. Also, if a part evolves, several versions will be released with time (which opens the topic of \"applicability\" that we will see later on). In other words, industrial data is managed as IT people manage software source code, and not really as IT people manage data in the context of tertiary sector applications. In the tertiary sector, the \"management data\" are generally stored with other purposes: It is required to be able to access to the data that are valid today; It is required to be able to access to important previous versions of data for auditing purposes (for instance before a contract modification); But it is not generally required to secure every change on data. Note that the current way of managing data in the IT world is generating technical debt and graph-oriented techniques enable to solve that issue (see here ). Coming back to the industry, industrial data being inside a graph, every change on a \"node\" of this graph (a business object) may have impacts on the linked objects. Let us consider Figure 2. Figure 2: Basic evolution of a graph of objects Originally, all objects of the graph are considered as being in version 1. The instance A1 of A changes to version 2. A1 V2 can have different values in its attributes but the change implies questions about the links A1 maintains with B1 , B2 and C1 . Are they still valid? Maybe, on an industrial standpoint, some tests must be done (or digitally, or physically) to assess this point. Let's now consider Figure 3 now where B1 is evolving. Figure 3: Evolution of a linked object Must A1 evolve too? If B1 V1 was an error, maybe A1 V1 should be transformed in A1 V2 pointing to B1 V2, and A1 V1 should be made not usable by anybody...","title":"The lifecycle of a business object"},{"location":"articles/about-plm/#the-notion-of-change","text":"In Figure 3, we can see that it would probably make sense to make a new version of A1 in order to take into account the change in B1 . Let us name Change1 the change that transforms B1 V1 into B1 V2. Change1 is an instance of the the Change class, which is a business object. When defining it in the first step, Change1 will be in version 1. But if, due to Change1 , we have to change A1 V1 into A1 V2, then this change should also be associated to Change1 . The Figure 4 is showing the change of B1 and then the change of A1 . Figure 4: The change business object The Change1 change is referencing in a first step only the previous and present version of B1 and in a second step the previous and current version of A1 . Note that we could have created 2 versions of the Change1 , one for the original change of B1 and the second one for the change of B1 and the impacted change on A1 . This kind of choice will depend on the industrial change policy. In the industry, the change object is crucial and often embeds many data including documents. There are many reasons for that, to begin with the costs of a change. Indeed, any change on a manufactured product may imply costly impacts on the production line and on the support of the product. In some industries like aerospace, the product may be \"certified\" (airworthiness for instance). Any change to the product will imply a certification impact assessment that could result in costly re-certification. Another reason to manage changes is to be able to find back in the past what reason caused a change that resulted in product performance regression or problem. Generally, the change object is at the core of a special process, often named \"change board\" or \"change committee\", where many stakeholders gather in order to determine if the change is worth doing, and if yes, what is the best solution (or the less worst).","title":"The notion of change"},{"location":"articles/about-plm/#about-the-aggregation-link","text":"In the previous figures (from 1 to 4), the links between business objects were standard arrows, such as the one on the left of the Figure 5 below. Figure 5: The semantics of links On the right of Figure 5, IT people will have recognized a link with a white diamond shape. For UML users, this link means \"aggregation\". Aggregation can be considered as a special case of relationship between 2 classes: It just bears more meaning than the simple arrow, the meaning that if A1 is aggregating B1 , that means that A1 is a \"bigger\" structure than B1 and A1 is some kind of \"container\" object. The aggregation link asks an important question in the PLM system: What is the elementary brick that contains only data and no other sub-brick? This should be a convention of the particular industry.","title":"About the aggregation link"},{"location":"articles/about-plm/#the-ambiguous-notion-of-part","text":"Generally, a manufacturing product can be seen as made of \"parts\". There are conceptually 2 kinds of parts: The elementary parts: The ones that cannot be split into smaller parts; The assemblies: The parts that are containing other parts. This notion is very important because: Defining properly the level of elementary parts is defining the minimal scale at which a certain business will manage its data lifecycle; Different businesses are often seeing the product at different levels (in terms of \"elementary\" parts), for instance support and engineering in aerospace. The industry often refers as \"part\" something that can be or an elementary part or an assembly (because both entities usually have part numbers). This is often confusing and leads to misunderstanding with IT people. In some cases, it could be clearer to talk about a \"leaf of the graph\" or just a \"leaf\" as being the agreed lowest level of data managed in lifecycle, and about \"containers\" if the object managed is a container for other objects. For instance, an assembly being a set of parts, the parts in that context could be \"leafs\", and the assembly would be a container. For sure, a container has fundamentally the exact same structure as the leaf, the structure shown in Figure 1: It has properties with real data attached to its instance; It has links to other business objects. The only difference between a leaf and a container is that, in the container case, there is at least one link that can be considered semantically as an \"aggregation\" (see Figure 5). A leaf should have no aggregation link.","title":"The ambiguous notion of part"},{"location":"articles/about-plm/#product-structure-aggregation-and-semantic-ambiguity","text":"As soon as we define a particular kind of links (aggregation), we can define \"trees\" which are partial representation of the global product graph just looking at the aggregation links between containers and leafs. This is often called: \"product structure\". Even if this view is still very used in the industry, we must never forget that it is only a very partial (and potentially misleading) view of the product (moreover, this view can be insufficient for several stakeholders, such as the support engineering in aerospace). Indeed, semantically typing the links is opening the door to a lot of semantic ambiguities. If it seems obvious that a assembly can be represented as an object that \"contains\" other objects that would be \"parts\", the fact that a group of components are stored in the same functional area (e.g. an area named \"all assemblies related to air conditioning\") introduced a double meaning in the aggregation link: The original meaning was: A physical component is physically the result of the assembly of smaller sub-components; The second meaning of aggregation would be an administrative way of gathering things (like a set). The result is that, in the same \"product structure\", you can have quickly two or more different semantic interpretations of the same \"aggregation\" link. Figure 6: The danger of the aggregation link In the Figure 6 above, it is easy to see that the aggregation link tagged 1 is representing the physical aggregation, and the aggregation link tagged 2 is representing the administrative classification. In reality, things are often much worse. Indeed, we usually can find many different concepts in the \"product structure\": The organization of the company (each organization will have a sub-tree with its own components); The variants of the product; The various versions of the same component (named \"variable component\" and aggregated under a \"non-variable component\"); Etc.","title":"Product structure, aggregation and semantic ambiguity"},{"location":"articles/about-plm/#the-influence-of-erp-systems-on-modern-plm-systems","text":"Modern PLM systems try to avoid this confusion brought by the \"tree vision\" of the product, by proposing other business classes and objects that enable the modeling of the industrial concepts. For instance, the organization of the company should have an impact on access rights but not on the structure of the graph of objects. The variants of the products can also be managed with new concepts that enable to manage smoothly options compatibility and incompatibility. Versions of the same business object can be managed without all versions to appear in the \"product structure\". The modern PLM systems propose more and more business classes that everybody need in the industry. Even if, structurally, those objects are still conforming to the Figure 1 definition, they represent in a much more accurate way the semantics of the industry, being engineering, manufacturing or support. For sure, those objects also come with their links to other domain specific business objects. We could say that the PLM systems evolved as the ERP systems did in the last decades: towards a better description of the business semantics, while keeping the core graph, lifecycle and change engine available. This, for sure, is a major step ahead most companies still don't benefit from, unfortunately.","title":"The influence of ERP systems on modern PLM systems"},{"location":"articles/about-plm/#about-applicability","text":"Before closing this first brief introduction to PLM systems concepts, we must stop a while on the applicability context. Let's consider Figure 4 right bottom blue area. We have many versions of A1 and B1 that are existing. What version is really \"applicable\" to a certain product that was produced in the assembly line Saturday May 1st 2021? In a graph, the simplest way to do it would be to have a business object representing the various instances of the product and to create a link between the instance number 12 of the product and the proper version of A1 and B1 it is using. This is what is shown in Figure 7 (with an intermediate object that represents the product version). Figure 7: Product evolution in product graph Product instance 12 and 13 are instances of the V1 of the product. After the change, 14 and 15 are instances of the V2 of the product. The idea here is to use the graph in order not to have to \"calculate anything\". As the product is made of released business objects, the product is a set of links to business objects (that on their side can also link to other objects). Here, we see the importance of having homogeneous semantic links, especially if the head of the tree (the release of the product) is supposed to be the head of the full product tree for this particular version. For sure, if the graph is full of semantic ambiguity (like the ones we mentioned), we may end up in the necessity of adding complex management rules to get the real tree of applicable objects for a specific serial number (this is often called \"configuration calculation\"). Criticizing this model would need a dedicated article. All that we need to say is that modern PLM systems are more and more graph-aware and are managing the product configuration in manners that are used successfully for decades in the domain of complex huge softwares. To be simple: using the real power of graphs with adequate domain specific business objects leads to a very powerful industry management approach.","title":"About applicability"},{"location":"articles/about-plm/#the-core-features-of-the-plm-systems","text":"If we attempt a synthesis of this brief introduction, we could say that the PLM system very core features appears to be: The capability of managing a graph of business objects; Business objects are instances of classes having fields, documents and links to each other; The functionality of managing the lifecycle of business objects: Version management, Change management. The functionality of proposing domain-specific business objects and domain-specific relationships between objects, each set of business objects/links enabling specific features (quite often sold as \"modules\" of the PLM system). Essentially, the real core of a PLM system is graph data . That implies that more advanced features linked to configuration and applicability can be made simple thanks to correct graph data usage.","title":"The core features of the PLM systems"},{"location":"articles/about-plm/#the-complexity-of-graph-data-modeling","text":"Representing data as graphs is very powerful and used in many applications nowadays. It enables to dissociate (under certain conditions) the data from the the links data have together. However, implementing management rules in graphs is not easy (see the graph transformation page).","title":"The complexity of graph data modeling"},{"location":"articles/about-plm/#getting-inspired-by-plms-outside-of-industry","text":"If the PLM systems are not easy to understand by people coming from the tertiary sector (like banks, insurances or public sectors), in a world obsessed with data (see The real nature of data ), the introduction of graphs in management data is definitely a good idea (see here and there in this site). More and more, we see attempts of PLM use in the tertiary sector in business domains that are not obviously similar to industry use cases. But we have to remember that tertiary sector applications are managing a lot of business rules while industry are managing much less (that will be the object of a future article about \"end to end PLM\"). The challenge of tertiary sector applications is to benefit from the graph data approach while being able to implement and maintain complex business rules in a universe of graph data. This is not trivial and the purpose of most of the pages of this site.","title":"Getting inspired by PLMs outside of industry"},{"location":"articles/about-plm/#see-also","text":"Other articles on the PLM series: Article 2: Configuration management of industrial products in PDM/PLM Article 3: The four core functions showing you need a PLM Other connected articles: The real nature of data ( May 2021, updated July 2021 )","title":"See also"},{"location":"articles/about-rest/","text":"Considerations About Rest And Web Services It's been a very long time since I've been explaining this to a lot of people and maybe today I should try to put the full explanation on paper. A Bit Of History When the REST concept was published, sometimes around 2001, I was in a middleware team in a big software company. In that team, we were accustomed to use business services for more than a decade. Indeed, the term \"web service\" or \"SOA\" was not existing at the time. But, in many business domains such as airline commercial business, banking, insurance, etc., many people knew what was RPC and were using it extensively to communicate between systems. RPC means Remote Procedure Call. RPC concept was introduced to me with the DCE ( Distributed Computing Environment ). DCE was a very powerful standard that were never completely used, as far as I know, but it explained in great details the basis of interoperability between systems. For sure, the standard was only making a synthesis of ideas that were much older. What Is RPC? Interoperability Contract Fundamentally RPC is, like it is said in its name, a remote procedure call. To understand the concept, let's imagine 2 programs that want to communicate, first program being A and second being B . B will publish an API in order to be called. In most procedural programming languages (like C), if the two programs are located on the same machine, A can call the API of B (see the top of Figure 1). Figure 1: Local Procedure Call and Remote Procedure Call The idea of interoperability in RPC is that, if B is located in a remote machine (or a remote process), B should not change when invoked by A . On the other side, A should not change in its invocation of B interface. So A will call a B interface locally to its system, this interface hiding a client stub that will wrap/serialize data in a certain format to be sent on the wire; on the machine hosting B , there will be a server stub unwraping/unserializing data to call locally the B interface. For sure, in order to work in a complex network, the message sent from A will have to fond is route to the machine hosting B . We have here all the elements of the client-server architecture. The Notion Of \"Verb\" Well, RPC is a bit more than that. Because, when calling a remote procedure (or function), semantically, we call a remote system asking it to do something (the B API) with the input data that we provide (the request). We expect data back (the response). This is/was called a \"verb\". A says to B : \"perform B-interface contract with my input data and give me back the contract expected output data\". Trends passed on many parameters: The protocols used changed, The addressing schemes changed, The format of the data changed (from many proprietary formats or Edifact, to XML to JSON). But the fact is, in most businesses proposing a certain degree of business complexity, RPC is still there. Most often, verbs are expressing an \"action\" semantic and requests are proposing data trees, so as responses. No Assumptions on the Technology Used We must notice that RPC does not make any assumption on the technology used by the server (the one that implements the B interface). The contract is limited to a verb, and two trees of data, one for the request and one for the response. We could note the contract as being: A uses response = B-interface(request) . The Corba Failed Attempt Principle In the 90s, the objet-oriented programming (OOP) being trendy, the intention behind Corba was born. The idea was to generalize the object notions to a distributed world. The logic consequence was to imagine a client-server protocol for objects to communicate between one another. The principle is simple: an object A calls locally a method m(...) on an object B . If we imagine the B instance as being remote, the idea is the same than RPC: The method should have a client and server stub. Figure 1: Local Method Invocation and Remote Method Invocation The fact is, despite the fact that it looks like RPC, this model is radically different from it: It supposes that the remote system is object oriented; It supposes that the remote system is stateful (the B instance must exist for its method to be called); It supposes an addressing system to find the reference of the instance of B; The contract of the method is not self sufficient, indeed, conceptually A asks for: response = state(B).m(request) which introduce some uncertainty on the call (because it depends on B state); The contract is supposing the state of B should be managed differently from the request parameters, and so, it puts a different semantic weight on B that should be a \"first-class entity\" whereas the request parameters are considered as \"second class entities\". This way of thinking distributed systems is leading to consider that the network offers \"objects that can provide services on themselves\", instead of providing simple services (i.e. distributed functions). In 2017's language, we could say that CORBA is proposing a \"resource-oriented architecture\". The Drawbacks of the ORB Approach An ORB (Object Request Broker) is a middleware that enables to manage those distributed objects, their remote invocation and the required addressing schema. The main drawback of the ORB approach is that a service can be, semantically, much more complex than calling one method of one object. Indeed, if we call a method on an object, we can imagine that the object will process this method on its state, or that it will process it considering its aggregation (\"facade\" pattern). For services, we do not make any assumptions of the kind. We just call a function that is supposed to do something with the inbound parameters that we provide (inbound parameters that are generally some kind of tree of objects having or not links together). Certainly, we can \"twist\" Corba in order to make it look like RPC: we can use objects that do not contain state and that have \"methods\" that are indeed just \"procedures\". But the addressing repository will have to manage object instance addresses instead of knowing the service location. An Idea That Keeps Coming Back This idea keeps coming back. Rest architecture can be seen as a reformulation of Corba principles: Resources are benefiting from an addressing scheme (URI); Resources have a name (class name), they are identified through their instance and they publish methods; Invoking a service is indeed a remote method invocation. Moreover, a strange design option is taken in the Rest specification: the presentation layer is binded on the underlying network protocol (http) in a very hard way. Indeed, the only verbs that seem to be invokable are CRUD verbs (CRUD standing for Create Retrieve Update Delete). We can also see this ORB principle applied, in a weaker way, in WSDL. When SOAP web services were really in the spirit of RPC, WSDL standard groups transactions in a way that is sometimes near to the idea of a group of methods operating on a remote object. We can note also in WSDL, a very strange hard biding on the underlying protocol (the WSDL schema integrates information from several OSI layers which is bad). Rest was those last years recently put in front of the scene due to IT marketing and the creation of Swagger.io The Core Problem of Resource Orientation Resource orientation, like in an ORB, is semantically a very hard restriction of RPC. RPC is a way of calling a service that will transform a graph of data in another graph of data. In the ORB approach, we call a method on a object, which is very restrictive. This can work if the semantics of the business is simple. If my business objects are modeled properly by one class, then maybe an ORB can work. If the business objects I have to model need several classes interacting together in a complex manner, then using an ORB will be a real pain. When a service can transform my graph of objects into another graph of objects, an ORB will force me to \"adapt\" my semantics to a pure technical object-oriented method call. For sure, this is true for Rest, as it was true for Corba. For pure Rest, it is even worse: being able to consider that only the http verbs (network layer) should be used to express the semantics of the presentation layer, the semantics of the \"functional verbs\" seems even more restrictive than Corba. For social networks like Twitter or Facebook, it seems to work. I can define a Rest API providing all the services offered by the platform. For a business application, we generally cannot use Rest because the constraints (RMI with CRUD verbs) are too strong. Again, like in Corba, we can cheat: we can use \"almost Rest\" and have a request body with JSON. That turns the method invocation into an almost-service. We can also include in the request a service name, but that is pretending to implement Rest and doing JSON-RPC-like behind the scene. Conclusion Rest is very practical for the applications which semantics is simple and can be adapted to two constraints: A resource oriented API (pushing for services to be RMI); A verb semantic limited to a variation of CRUD. For other applications, like business applications [1], we believe things never changed for decades. Before SOA, RPC services were existing. They were the same stuff than JSON-RPC like transactions today. Service orientation is a much more general way of implementing service distribution than ORB concepts. In particular, service orientation does not presuppose that the remote server sees the data as the caller does. The service signature is an agreed contract for both the client and the server to communicate together, but each of them can restructure data as they want, in an object oriented way or not. So, my advice is not to force yourself to implement a Rest API to your application because it is trendy, but to do it only if your business semantics enables it. See Also About GraphQL Notes [1] - In some businesses, like the airline one (standardized by IATA ), services have big requests and big responses for decades because the business requires it. ( December 2017 )","title":"Considerations About Rest And Web Services"},{"location":"articles/about-rest/#considerations-about-rest-and-web-services","text":"It's been a very long time since I've been explaining this to a lot of people and maybe today I should try to put the full explanation on paper.","title":"Considerations About Rest And Web Services"},{"location":"articles/about-rest/#a-bit-of-history","text":"When the REST concept was published, sometimes around 2001, I was in a middleware team in a big software company. In that team, we were accustomed to use business services for more than a decade. Indeed, the term \"web service\" or \"SOA\" was not existing at the time. But, in many business domains such as airline commercial business, banking, insurance, etc., many people knew what was RPC and were using it extensively to communicate between systems. RPC means Remote Procedure Call. RPC concept was introduced to me with the DCE ( Distributed Computing Environment ). DCE was a very powerful standard that were never completely used, as far as I know, but it explained in great details the basis of interoperability between systems. For sure, the standard was only making a synthesis of ideas that were much older.","title":"A Bit Of History"},{"location":"articles/about-rest/#what-is-rpc","text":"","title":"What Is RPC?"},{"location":"articles/about-rest/#interoperability-contract","text":"Fundamentally RPC is, like it is said in its name, a remote procedure call. To understand the concept, let's imagine 2 programs that want to communicate, first program being A and second being B . B will publish an API in order to be called. In most procedural programming languages (like C), if the two programs are located on the same machine, A can call the API of B (see the top of Figure 1). Figure 1: Local Procedure Call and Remote Procedure Call The idea of interoperability in RPC is that, if B is located in a remote machine (or a remote process), B should not change when invoked by A . On the other side, A should not change in its invocation of B interface. So A will call a B interface locally to its system, this interface hiding a client stub that will wrap/serialize data in a certain format to be sent on the wire; on the machine hosting B , there will be a server stub unwraping/unserializing data to call locally the B interface. For sure, in order to work in a complex network, the message sent from A will have to fond is route to the machine hosting B . We have here all the elements of the client-server architecture.","title":"Interoperability Contract"},{"location":"articles/about-rest/#the-notion-of-verb","text":"Well, RPC is a bit more than that. Because, when calling a remote procedure (or function), semantically, we call a remote system asking it to do something (the B API) with the input data that we provide (the request). We expect data back (the response). This is/was called a \"verb\". A says to B : \"perform B-interface contract with my input data and give me back the contract expected output data\". Trends passed on many parameters: The protocols used changed, The addressing schemes changed, The format of the data changed (from many proprietary formats or Edifact, to XML to JSON). But the fact is, in most businesses proposing a certain degree of business complexity, RPC is still there. Most often, verbs are expressing an \"action\" semantic and requests are proposing data trees, so as responses.","title":"The Notion Of \"Verb\""},{"location":"articles/about-rest/#no-assumptions-on-the-technology-used","text":"We must notice that RPC does not make any assumption on the technology used by the server (the one that implements the B interface). The contract is limited to a verb, and two trees of data, one for the request and one for the response. We could note the contract as being: A uses response = B-interface(request) .","title":"No Assumptions on the Technology Used"},{"location":"articles/about-rest/#the-corba-failed-attempt","text":"","title":"The Corba Failed Attempt"},{"location":"articles/about-rest/#principle","text":"In the 90s, the objet-oriented programming (OOP) being trendy, the intention behind Corba was born. The idea was to generalize the object notions to a distributed world. The logic consequence was to imagine a client-server protocol for objects to communicate between one another. The principle is simple: an object A calls locally a method m(...) on an object B . If we imagine the B instance as being remote, the idea is the same than RPC: The method should have a client and server stub. Figure 1: Local Method Invocation and Remote Method Invocation The fact is, despite the fact that it looks like RPC, this model is radically different from it: It supposes that the remote system is object oriented; It supposes that the remote system is stateful (the B instance must exist for its method to be called); It supposes an addressing system to find the reference of the instance of B; The contract of the method is not self sufficient, indeed, conceptually A asks for: response = state(B).m(request) which introduce some uncertainty on the call (because it depends on B state); The contract is supposing the state of B should be managed differently from the request parameters, and so, it puts a different semantic weight on B that should be a \"first-class entity\" whereas the request parameters are considered as \"second class entities\". This way of thinking distributed systems is leading to consider that the network offers \"objects that can provide services on themselves\", instead of providing simple services (i.e. distributed functions). In 2017's language, we could say that CORBA is proposing a \"resource-oriented architecture\".","title":"Principle"},{"location":"articles/about-rest/#the-drawbacks-of-the-orb-approach","text":"An ORB (Object Request Broker) is a middleware that enables to manage those distributed objects, their remote invocation and the required addressing schema. The main drawback of the ORB approach is that a service can be, semantically, much more complex than calling one method of one object. Indeed, if we call a method on an object, we can imagine that the object will process this method on its state, or that it will process it considering its aggregation (\"facade\" pattern). For services, we do not make any assumptions of the kind. We just call a function that is supposed to do something with the inbound parameters that we provide (inbound parameters that are generally some kind of tree of objects having or not links together). Certainly, we can \"twist\" Corba in order to make it look like RPC: we can use objects that do not contain state and that have \"methods\" that are indeed just \"procedures\". But the addressing repository will have to manage object instance addresses instead of knowing the service location.","title":"The Drawbacks of the ORB Approach"},{"location":"articles/about-rest/#an-idea-that-keeps-coming-back","text":"This idea keeps coming back. Rest architecture can be seen as a reformulation of Corba principles: Resources are benefiting from an addressing scheme (URI); Resources have a name (class name), they are identified through their instance and they publish methods; Invoking a service is indeed a remote method invocation. Moreover, a strange design option is taken in the Rest specification: the presentation layer is binded on the underlying network protocol (http) in a very hard way. Indeed, the only verbs that seem to be invokable are CRUD verbs (CRUD standing for Create Retrieve Update Delete). We can also see this ORB principle applied, in a weaker way, in WSDL. When SOAP web services were really in the spirit of RPC, WSDL standard groups transactions in a way that is sometimes near to the idea of a group of methods operating on a remote object. We can note also in WSDL, a very strange hard biding on the underlying protocol (the WSDL schema integrates information from several OSI layers which is bad). Rest was those last years recently put in front of the scene due to IT marketing and the creation of Swagger.io","title":"An Idea That Keeps Coming Back"},{"location":"articles/about-rest/#the-core-problem-of-resource-orientation","text":"Resource orientation, like in an ORB, is semantically a very hard restriction of RPC. RPC is a way of calling a service that will transform a graph of data in another graph of data. In the ORB approach, we call a method on a object, which is very restrictive. This can work if the semantics of the business is simple. If my business objects are modeled properly by one class, then maybe an ORB can work. If the business objects I have to model need several classes interacting together in a complex manner, then using an ORB will be a real pain. When a service can transform my graph of objects into another graph of objects, an ORB will force me to \"adapt\" my semantics to a pure technical object-oriented method call. For sure, this is true for Rest, as it was true for Corba. For pure Rest, it is even worse: being able to consider that only the http verbs (network layer) should be used to express the semantics of the presentation layer, the semantics of the \"functional verbs\" seems even more restrictive than Corba. For social networks like Twitter or Facebook, it seems to work. I can define a Rest API providing all the services offered by the platform. For a business application, we generally cannot use Rest because the constraints (RMI with CRUD verbs) are too strong. Again, like in Corba, we can cheat: we can use \"almost Rest\" and have a request body with JSON. That turns the method invocation into an almost-service. We can also include in the request a service name, but that is pretending to implement Rest and doing JSON-RPC-like behind the scene.","title":"The Core Problem of Resource Orientation"},{"location":"articles/about-rest/#conclusion","text":"Rest is very practical for the applications which semantics is simple and can be adapted to two constraints: A resource oriented API (pushing for services to be RMI); A verb semantic limited to a variation of CRUD. For other applications, like business applications [1], we believe things never changed for decades. Before SOA, RPC services were existing. They were the same stuff than JSON-RPC like transactions today. Service orientation is a much more general way of implementing service distribution than ORB concepts. In particular, service orientation does not presuppose that the remote server sees the data as the caller does. The service signature is an agreed contract for both the client and the server to communicate together, but each of them can restructure data as they want, in an object oriented way or not. So, my advice is not to force yourself to implement a Rest API to your application because it is trendy, but to do it only if your business semantics enables it.","title":"Conclusion"},{"location":"articles/about-rest/#see-also","text":"About GraphQL","title":"See Also"},{"location":"articles/about-rest/#notes","text":"[1] - In some businesses, like the airline one (standardized by IATA ), services have big requests and big responses for decades because the business requires it. ( December 2017 )","title":"Notes"},{"location":"articles/archimate-intro/","text":"Introduction to the Archimate Revolution In the last decade, a quiet revolution took place in the Enterprise Architecture (EA) domain. This revolution is called Archimate . This article is the first of a series of articles focusing on some specific aspects of Archimate and practices around the standard. All Archimate diagrams of this site are done using Archi . This tool is free and great. If you use it, consider making a donation. A Semantic Modeling Language Brief Introduction To The Archimate Meta-Model Archimate is a modeling language that enables to describe and study several aspects of the enterprise: Its strategy and motivations, Its projects, And the 4 core layers of enterprise description: The business layer, The software layer, The technology layer (infrastructure), The physical layer. All those aspects propose: Typed artifacts, Typed relationships between artifacts. Note that several relationships types can interconnect many different types or can be used in many contexts. The detailed description of all artifacts is not in the scope of this article and we can advise the reader to refer to the book Enterprise Architecture At Work from Mark Lankshorst . Archimate also defines interlayer relationships which enables to \"connect\" the various layers together. This is an absolutely fundamental feature of the language. A Graph Model Indeed, an Archimate model is actually a graph . For those who are familiar with the Semantic Web ( RDF, RDFS , and OWL ), any Archimate model is a semantic graph. The graph model is the result of the union of all diagrams implemented with the same set of artifacts. For each artifact, the union of all its incoming and outgoing relationships creates the neighborhood of the artifact in the graph model. When you use Archimate, you represent things in views, using the various artifacts that are available to you. Each element you draw on a certain view (there are many types of views in the standard) will have a certain type, like \"Business Process\" or \"Application Function\". Thus, the resulting model will be a set of views, each of them presenting many interconnected artifacts that all are instances of types that have a specific meaning . Viewpoint And Meta-Model The creation of a semantic language generally implies the creation a meta-model. Many tools are existing to create meta-models (for instance Eclipse EMF with Sirius , or MetaEdit+ depending if you want to pay or not). In the history of enterprise architecture, many approaches defined the notion of \"view point\": in order to act on the enterprise as a whole (or as a system ), the first step is to describe the enterprise. In order to describe it, due to the complexity of the task, the architect must use several viewpoints. The union of those viewpoints is defining the model of the enterprise. The Zachman framework was one of the first publicly available enterprise architecture \"framework\", enabling to address the many viewpoints describing the enterprise. The problem with this framework, but also with many other enterprise architecture frameworks, is that they propose many viewpoints, generally each of them proposing a complete separate meta-model for each viewpoint (with artifact types and relationship types), but no consistent view of those various viewpoints. This is a major issue for the framework because: Separate meta-models for separate viewpoints will, most often, enable to create a globally ambiguous model; Separate meta-models can create various ways of representing the same reality; There can be semantic overlap between various viewpoints and their respective meta-model; What should be a single viewpoint can artificially be split into different view points with different meta-models (we'll see later in this article the dramatic consequences of the Long\u00e9p\u00e9 Enterprise Architecture model in France). If the consistency is not \"built in\" the framework (i.e. in its meta-model), if it is not part of the primary requirements, then the framework will be very difficult to use. Indeed, interconnecting various meta-models to be able to create something consistent is not an easy task. Indeed, Archimate seems to be the best illustration of it, and I fear I don't know many other samples [1]. Samples of Non Consistent Modeling Approaches In IT, we have forgotten about the crucial importance of having a consistent way to describe the \"reality\", or let's say the concepts that we are manipulating frequently. Sample #1: UML UML can be the first example of the non consistency of a meta-model. UML is not consistent because it proposes many meta-models that are not semantically connected together as a whole. Suppose you made class diagrams and sequence diagrams for a set of classes. You can add a state diagram for a particular process of a specific class A of your model. In UML, there is no way to know if this state diagram is consistent or not with the rest of the diagrams that include A . This is due to the fact that UML proposes a set of various kinds of views that are not linked together . Each view type has its own meta-model. Some of the meta-models are reusing the same artifacts, which enable modeling tools to connect the artifacts to the views they are used into. But many diagram types are just disconnected meta-models. This problem can perhaps explain why modeling in UML was progressively abandoned by many projects; because it was not really suited to express in a consistent way what the code should be. If, during design, doing UML model cannot prevent you from making design mistakes, the use of the modeling language is much less interesting [2]. Sample #2: Long\u00e9p\u00e9 Another dramatic inconsistent model is the Long\u00e9p\u00e9 French model which defined, instead of Archimate \"software layer\", two different layers: One \"functional architecture layer\", that could be interpreted as the Application Function part of the Archimate meta-model, One \"application architecture layer\", that could be interpreted as the Application Component part of the Archimate meta-model. As in Archimate, there is, for Long\u00e9p\u00e9, a business layer and an infrastructure layer, which scopes are almost the same than Archimate's. In the Long\u00e9p\u00e9 model, every layer has a model, and every model can be \"derived\" from the model of the superior layer. This means that the application layer can be \"derived from\" the functional layer. This assertion is obscure, misleading and semantically erroneous. In Archimate, the application function is \"assigned to\" a application component. That enables to manage the good and the bad assignment, what is called the \"urbanisation\" in French, notion of good or bad function positioning in the global IT systems: If some functions are well positioned in the IT systems, that means that they communicate easily and can evolve in a natural way without questioning the full architecture; If some functions are badly positioned in the IT systems (so assigned to the wrong components), we will generate useless dependencies between applications, useless web services, difficulties in terms of evolution, costs and complexity. The problem of \"function positioning\" is not in the scope of this article, but it is at the heart of the Long\u00e9p\u00e9's reflection, despite the invention of a very bad meta-model that totally confuses the notion through the concept of derivation. This artificial split of one layer (the application layer of Archimate) into two layers which dependency is erroneously defined, caused a huge number of French IT projects to fail , and numerous errors of interpretation and understanding for French architects. The book of Christophe Long\u00e9p\u00e9 was at the origin of many misjudgments, errors, confusions and money loss in the French market since the publication of its first edition in 2001. Sample #3: Projects Creating Their Own Modeling Framework In consulting missions, I also saw strange practices, as the one of creating a project-specific enterprise architecture modeling framework that evolved throughout the project. Most of the time, the internal framework was incorporating progressively various inconsistent meta-models coming from various modeling standards (such as UML or BPMN or TOGAF ). For sure, most architecture works in the evolving project-specific EA framework were confusing and not usable by software engineers. This often led or contributed to lead the project to its failure. Archimate, a Consistent Approach Archimate is proposing a consistent meta-model. This is the first fundamental characteristic which is at the heart of the usability of the modeling standard. I deeply encourage the meta-model creators to think about it, because it defines the real power of the multiple-views paradigm: multiple views on the same reality can be used provided there can be consistency between those views , the consistency being in a unique meta-model. Note that proposing a consistent meta-model does not imply that the modeling framework is closed and will not evolve. Archimate proposes, since its version 3, a way to extend the meta-model. We could say that the multiple views consistency-enable graph model is the revolution #0 of the Archimate language. The Many Revolutions of Archimate Revolution #1: The Language Just Works Being consistent is not sufficient for a meta-model. The quality of the meta-model lies in the pertinence of the semantic artifacts that it proposes (being nodes or relationships). Those artifacts must be as semantically clearly defined and must not bring confusion or multiple alternative representations. In particular, semantic overlapping must be avoided as much as possible [3]. In other terms, if the quality of the meta-model is high, then the modeling will be good. However, if the meta-model is bad, the modeling will be very bad and will cause damages in the project. Indeed, Archimate propose artifacts that enable the non-ambiguous description of the enterprise processes, strategy and IT. More: it pushes the architects to describe the reality in a sane way , which means in a way that will push for problem exploration and make visible the possible solutions. For instance, in the case of function positioning, it is easy to count the dependencies between two functions (through derived relationships). If the count is low and always directed in the same way, that probably means the functions are well positioned; If the count if high and/or the directions go both ways, that probably means that the two functions should be one single function. Archimate language helps and in that sense, it is doing the job. In other terms: the language just works . This is, really, a revolution. In some cases, meta-models induce architects to think badly, to force themselves to think in an inconsistent model where the semantics are confusing. In Archimate, this is not the case. Revolution #2: Architects Can Share And Propose Auditable Works Even if Archimate will not guarantee that 2 enterprise architects will produce the same modeling when representing the same things, using the same standardized language enable each of them to understand the modeling of the other, to challenge it and to discuss it. We can forget Visio or Powerpoint schema based modeling, that are very ambiguous at several levels. The works become auditable by other Archimate architects. For sure, documenting textually the views and the artifact themselves is very helpful. As in all modeling languages, without an effort, some views may be a be difficult to understand if we don't know what question it is supposed to answer. This is a revolution in Enterprise Architecture but also in IT or business architecture. We can work on something shareable and auditable and so begin to work the simplistic boxes and arrow diagrams. Enterprise Architecture is becoming more an engineering discipline when it was considered, too often, as a blur esoteric and confusing witchcraft. Revolution #3: Managing Complex Representations In Archimate, you can tackle very complex problems. Due to the fact that you can study several granularity of problems at the same time in the same model [4], the studied complexity can be at several levels: You can address very big IT systems or one company or of a group of companies; You can analyze in a very detailed way sets of very small intertwined functionality; You can study highly distributed systems. Indeed, big models for IT transformation will quickly contain thousands of artifacts. Note that to ensure consistency on those models, there is a huge systematic work to do. This is a revolution. The last big program that I did without Archimate was in 2008-2009. It was big, functionally very complex and with a lot of architecture problems. I struggled with UML and BPMN and had to manage the functional consistency by hand. Archimate would have been of great help at the time. Revolution #4: Aggregate Various Sources of Knowledge When you are doing a big projects, you have many sources of information, some of them oral (like end users or IT people interviews) and some other being documents or diagrams or wikis, or existing code. It is very easy to forget important stuff or weak signals that hide structural constraints. With Archimate, you can define views per source of information and work in back office on the consistency of all the information provided when it begin to touch the same domains or software. At the same time, you can formalize what was said in a specific workshop (process, functions, software, etc.) and recreate this consistent view from the graph model. Despite the fact that it is suite a work, the semantic modeling of Archimate enables to highlight what is consistent and what is not and what complementary information you would need to complete the assessment. Very often in missions, I can say that my model contains all relevant information that I found. Revolution #5: Managing Dependencies Dependencies are the 21rst century problem. Companies have existing IT systems that grew sometimes in a great chaos. Changing stuff is, objectively, complicated, risky, costly. With Archimate, you can work on dependencies, and so address those risks before the project is started. You can even \"objectify\" the difficulty or a certain project. If you change a system that is not connected to many other systems, it can be easier than trying to change the core system without knowing about the impacts and the problems that will be induced by this change. Dependency management are at the heart of the complexity of digital transformation. Because operations must go on, enterprise and IT architects cannot do whatever they want. They have to work on a credible plan that will create a roadmap of transformation taking dependencies into accounts. Revolution #6: Modeling Transformation, Modeling Time Modeling digital transformation is a real piece of work if we want to do a serious job. But it is possible - and that's a revolution that Archimate enables. Yes, it is possible to create, pilot, manage, anticipate, huge and complex enterprise digital transformation with Archimate. And frankly, I cannot see what other approach can do that. At a certain point, reading the forums, I am not convinced that many architects really understood this point. We can now create pretty accurate scenarios of transformation inside Archimate models, taking into account the organization, the strategy, the IT systems, and so on. Maybe we have here the real role of the enterprise architect. Maybe the enterprise architect should be named the enterprise transformation architect . Enterprises should realize that transforming the processes and the IT systems can become an engineering discipline at last , and that using Archimate for this kind of critical topics enables to avoid many troubles and to spend unnecessary money. Modeling transformation is modeling time and so conventions must be chosen to tag artifacts as being existent or not at a certain phase of the transformation process. As of now, the Archimate modeling tools do not support a temporal view of models (like Franz Gruff AllegroGraph visualizer), we have to take conventions [5]. Revolution #7: Using Archimate In Many Software Activities Indeed, many software activities can use Archimate on a regular basis, even if the language is more targeting architecture purposes. When we look carefully at nowadays software, most software are highly interconnected to other software or within the enterprise or in the Cloud. Archimate can help analyzing the structural impacts of those interconnections inside the software itself. Cartography of Systems Archimate can be used to create cartographies of IT systems but many architects should realize that Archimate models are working tools more than poster tools. Conclusion Archimate is, for me, the engineering revolution of digital transformation of the last 15 years. It redefines the enterprise architect role as an enterprise transformation architect role that is able to study with all stakeholders the operational application of a company digital strategy and its impacts on the business, the processes and the IT systems. Once the study is done, Archimate models are of great help to tackle the complex programs, manage the project dependencies and optimize the whole transformation, in terms of time and in terms of efficiency and costs. We must never forget that the enterprise architect should be a crucial change agent that proposes solutions to a strategic problem and should work in the plain knowledge of the business processes. Most digital transformation offices should also carefully consider using Archimate to manage their transformation plans and try to be in control of the costs and schedule. See also Archimate recipes Notes [1] The only modeling approach that look like Archimate is the Aris methodology . Even if the methodology did not cover the full scope of Archimate, the theoretical approach was similar. [2] We could compare a UML design tool with the CAD tools. The use of the modeling language must enable to work on the model of the programs before coding in order to create the best design possible, a design that will be implementable quite straight forwardly. Unfortunately with UML, the objective is not reached. [3] Some big Enterprise Architecture frameworks used in the military world are proposing very complex meta-models that are proposing, at the same time, sets of disconnected meta-models and semantic overlap between artifacts. Those frameworks are often at the center of big project failures in the military industry. [4] In projects, I often use or advise to use \"level\" (or scale) indications of some artifacts like business processes or business functions. The indication can be put in the name of the artifact or as an attribute. For instance, Accounting will become (L1) Accounting to indicate that, in some views, the business functions will be represented in a high level way. All detailed functions inside this one (aggregation or composition link) will be flagged (L2) such as (L2) Centralization . This enables to have various levels of relationships between concepts and to be able to drill down inside the model in order to see more detailed (and more accurate) representations. [5] The temporal conventions can be as the ones we mentioned for scale management, something like (P3) for phase 3. Tagging the artifact name enables to have possibly many instances of the same artifact, instances that will evolve. The conventions must be chosen very carefully to be able to see what changes and what is remaining the same throughout the whole change process. (January 2018)","title":"An Introduction to The Archimate Revolution"},{"location":"articles/archimate-intro/#introduction-to-the-archimate-revolution","text":"In the last decade, a quiet revolution took place in the Enterprise Architecture (EA) domain. This revolution is called Archimate . This article is the first of a series of articles focusing on some specific aspects of Archimate and practices around the standard. All Archimate diagrams of this site are done using Archi . This tool is free and great. If you use it, consider making a donation.","title":"Introduction to the Archimate Revolution"},{"location":"articles/archimate-intro/#a-semantic-modeling-language","text":"","title":"A Semantic Modeling Language"},{"location":"articles/archimate-intro/#brief-introduction-to-the-archimate-meta-model","text":"Archimate is a modeling language that enables to describe and study several aspects of the enterprise: Its strategy and motivations, Its projects, And the 4 core layers of enterprise description: The business layer, The software layer, The technology layer (infrastructure), The physical layer. All those aspects propose: Typed artifacts, Typed relationships between artifacts. Note that several relationships types can interconnect many different types or can be used in many contexts. The detailed description of all artifacts is not in the scope of this article and we can advise the reader to refer to the book Enterprise Architecture At Work from Mark Lankshorst . Archimate also defines interlayer relationships which enables to \"connect\" the various layers together. This is an absolutely fundamental feature of the language.","title":"Brief Introduction To The Archimate Meta-Model"},{"location":"articles/archimate-intro/#a-graph-model","text":"Indeed, an Archimate model is actually a graph . For those who are familiar with the Semantic Web ( RDF, RDFS , and OWL ), any Archimate model is a semantic graph. The graph model is the result of the union of all diagrams implemented with the same set of artifacts. For each artifact, the union of all its incoming and outgoing relationships creates the neighborhood of the artifact in the graph model. When you use Archimate, you represent things in views, using the various artifacts that are available to you. Each element you draw on a certain view (there are many types of views in the standard) will have a certain type, like \"Business Process\" or \"Application Function\". Thus, the resulting model will be a set of views, each of them presenting many interconnected artifacts that all are instances of types that have a specific meaning .","title":"A Graph Model"},{"location":"articles/archimate-intro/#viewpoint-and-meta-model","text":"The creation of a semantic language generally implies the creation a meta-model. Many tools are existing to create meta-models (for instance Eclipse EMF with Sirius , or MetaEdit+ depending if you want to pay or not). In the history of enterprise architecture, many approaches defined the notion of \"view point\": in order to act on the enterprise as a whole (or as a system ), the first step is to describe the enterprise. In order to describe it, due to the complexity of the task, the architect must use several viewpoints. The union of those viewpoints is defining the model of the enterprise. The Zachman framework was one of the first publicly available enterprise architecture \"framework\", enabling to address the many viewpoints describing the enterprise. The problem with this framework, but also with many other enterprise architecture frameworks, is that they propose many viewpoints, generally each of them proposing a complete separate meta-model for each viewpoint (with artifact types and relationship types), but no consistent view of those various viewpoints. This is a major issue for the framework because: Separate meta-models for separate viewpoints will, most often, enable to create a globally ambiguous model; Separate meta-models can create various ways of representing the same reality; There can be semantic overlap between various viewpoints and their respective meta-model; What should be a single viewpoint can artificially be split into different view points with different meta-models (we'll see later in this article the dramatic consequences of the Long\u00e9p\u00e9 Enterprise Architecture model in France). If the consistency is not \"built in\" the framework (i.e. in its meta-model), if it is not part of the primary requirements, then the framework will be very difficult to use. Indeed, interconnecting various meta-models to be able to create something consistent is not an easy task. Indeed, Archimate seems to be the best illustration of it, and I fear I don't know many other samples [1].","title":"Viewpoint And Meta-Model"},{"location":"articles/archimate-intro/#samples-of-non-consistent-modeling-approaches","text":"In IT, we have forgotten about the crucial importance of having a consistent way to describe the \"reality\", or let's say the concepts that we are manipulating frequently.","title":"Samples of Non Consistent Modeling Approaches"},{"location":"articles/archimate-intro/#sample-1-uml","text":"UML can be the first example of the non consistency of a meta-model. UML is not consistent because it proposes many meta-models that are not semantically connected together as a whole. Suppose you made class diagrams and sequence diagrams for a set of classes. You can add a state diagram for a particular process of a specific class A of your model. In UML, there is no way to know if this state diagram is consistent or not with the rest of the diagrams that include A . This is due to the fact that UML proposes a set of various kinds of views that are not linked together . Each view type has its own meta-model. Some of the meta-models are reusing the same artifacts, which enable modeling tools to connect the artifacts to the views they are used into. But many diagram types are just disconnected meta-models. This problem can perhaps explain why modeling in UML was progressively abandoned by many projects; because it was not really suited to express in a consistent way what the code should be. If, during design, doing UML model cannot prevent you from making design mistakes, the use of the modeling language is much less interesting [2].","title":"Sample #1: UML"},{"location":"articles/archimate-intro/#sample-2-longepe","text":"Another dramatic inconsistent model is the Long\u00e9p\u00e9 French model which defined, instead of Archimate \"software layer\", two different layers: One \"functional architecture layer\", that could be interpreted as the Application Function part of the Archimate meta-model, One \"application architecture layer\", that could be interpreted as the Application Component part of the Archimate meta-model. As in Archimate, there is, for Long\u00e9p\u00e9, a business layer and an infrastructure layer, which scopes are almost the same than Archimate's. In the Long\u00e9p\u00e9 model, every layer has a model, and every model can be \"derived\" from the model of the superior layer. This means that the application layer can be \"derived from\" the functional layer. This assertion is obscure, misleading and semantically erroneous. In Archimate, the application function is \"assigned to\" a application component. That enables to manage the good and the bad assignment, what is called the \"urbanisation\" in French, notion of good or bad function positioning in the global IT systems: If some functions are well positioned in the IT systems, that means that they communicate easily and can evolve in a natural way without questioning the full architecture; If some functions are badly positioned in the IT systems (so assigned to the wrong components), we will generate useless dependencies between applications, useless web services, difficulties in terms of evolution, costs and complexity. The problem of \"function positioning\" is not in the scope of this article, but it is at the heart of the Long\u00e9p\u00e9's reflection, despite the invention of a very bad meta-model that totally confuses the notion through the concept of derivation. This artificial split of one layer (the application layer of Archimate) into two layers which dependency is erroneously defined, caused a huge number of French IT projects to fail , and numerous errors of interpretation and understanding for French architects. The book of Christophe Long\u00e9p\u00e9 was at the origin of many misjudgments, errors, confusions and money loss in the French market since the publication of its first edition in 2001.","title":"Sample #2: Long\u00e9p\u00e9"},{"location":"articles/archimate-intro/#sample-3-projects-creating-their-own-modeling-framework","text":"In consulting missions, I also saw strange practices, as the one of creating a project-specific enterprise architecture modeling framework that evolved throughout the project. Most of the time, the internal framework was incorporating progressively various inconsistent meta-models coming from various modeling standards (such as UML or BPMN or TOGAF ). For sure, most architecture works in the evolving project-specific EA framework were confusing and not usable by software engineers. This often led or contributed to lead the project to its failure.","title":"Sample #3: Projects Creating Their Own Modeling Framework"},{"location":"articles/archimate-intro/#archimate-a-consistent-approach","text":"Archimate is proposing a consistent meta-model. This is the first fundamental characteristic which is at the heart of the usability of the modeling standard. I deeply encourage the meta-model creators to think about it, because it defines the real power of the multiple-views paradigm: multiple views on the same reality can be used provided there can be consistency between those views , the consistency being in a unique meta-model. Note that proposing a consistent meta-model does not imply that the modeling framework is closed and will not evolve. Archimate proposes, since its version 3, a way to extend the meta-model. We could say that the multiple views consistency-enable graph model is the revolution #0 of the Archimate language.","title":"Archimate, a Consistent Approach"},{"location":"articles/archimate-intro/#the-many-revolutions-of-archimate","text":"","title":"The Many Revolutions of Archimate"},{"location":"articles/archimate-intro/#revolution-1-the-language-just-works","text":"Being consistent is not sufficient for a meta-model. The quality of the meta-model lies in the pertinence of the semantic artifacts that it proposes (being nodes or relationships). Those artifacts must be as semantically clearly defined and must not bring confusion or multiple alternative representations. In particular, semantic overlapping must be avoided as much as possible [3]. In other terms, if the quality of the meta-model is high, then the modeling will be good. However, if the meta-model is bad, the modeling will be very bad and will cause damages in the project. Indeed, Archimate propose artifacts that enable the non-ambiguous description of the enterprise processes, strategy and IT. More: it pushes the architects to describe the reality in a sane way , which means in a way that will push for problem exploration and make visible the possible solutions. For instance, in the case of function positioning, it is easy to count the dependencies between two functions (through derived relationships). If the count is low and always directed in the same way, that probably means the functions are well positioned; If the count if high and/or the directions go both ways, that probably means that the two functions should be one single function. Archimate language helps and in that sense, it is doing the job. In other terms: the language just works . This is, really, a revolution. In some cases, meta-models induce architects to think badly, to force themselves to think in an inconsistent model where the semantics are confusing. In Archimate, this is not the case.","title":"Revolution #1: The Language Just Works"},{"location":"articles/archimate-intro/#revolution-2-architects-can-share-and-propose-auditable-works","text":"Even if Archimate will not guarantee that 2 enterprise architects will produce the same modeling when representing the same things, using the same standardized language enable each of them to understand the modeling of the other, to challenge it and to discuss it. We can forget Visio or Powerpoint schema based modeling, that are very ambiguous at several levels. The works become auditable by other Archimate architects. For sure, documenting textually the views and the artifact themselves is very helpful. As in all modeling languages, without an effort, some views may be a be difficult to understand if we don't know what question it is supposed to answer. This is a revolution in Enterprise Architecture but also in IT or business architecture. We can work on something shareable and auditable and so begin to work the simplistic boxes and arrow diagrams. Enterprise Architecture is becoming more an engineering discipline when it was considered, too often, as a blur esoteric and confusing witchcraft.","title":"Revolution #2: Architects Can Share And Propose Auditable Works"},{"location":"articles/archimate-intro/#revolution-3-managing-complex-representations","text":"In Archimate, you can tackle very complex problems. Due to the fact that you can study several granularity of problems at the same time in the same model [4], the studied complexity can be at several levels: You can address very big IT systems or one company or of a group of companies; You can analyze in a very detailed way sets of very small intertwined functionality; You can study highly distributed systems. Indeed, big models for IT transformation will quickly contain thousands of artifacts. Note that to ensure consistency on those models, there is a huge systematic work to do. This is a revolution. The last big program that I did without Archimate was in 2008-2009. It was big, functionally very complex and with a lot of architecture problems. I struggled with UML and BPMN and had to manage the functional consistency by hand. Archimate would have been of great help at the time.","title":"Revolution #3: Managing Complex Representations"},{"location":"articles/archimate-intro/#revolution-4-aggregate-various-sources-of-knowledge","text":"When you are doing a big projects, you have many sources of information, some of them oral (like end users or IT people interviews) and some other being documents or diagrams or wikis, or existing code. It is very easy to forget important stuff or weak signals that hide structural constraints. With Archimate, you can define views per source of information and work in back office on the consistency of all the information provided when it begin to touch the same domains or software. At the same time, you can formalize what was said in a specific workshop (process, functions, software, etc.) and recreate this consistent view from the graph model. Despite the fact that it is suite a work, the semantic modeling of Archimate enables to highlight what is consistent and what is not and what complementary information you would need to complete the assessment. Very often in missions, I can say that my model contains all relevant information that I found.","title":"Revolution #4: Aggregate Various Sources of Knowledge"},{"location":"articles/archimate-intro/#revolution-5-managing-dependencies","text":"Dependencies are the 21rst century problem. Companies have existing IT systems that grew sometimes in a great chaos. Changing stuff is, objectively, complicated, risky, costly. With Archimate, you can work on dependencies, and so address those risks before the project is started. You can even \"objectify\" the difficulty or a certain project. If you change a system that is not connected to many other systems, it can be easier than trying to change the core system without knowing about the impacts and the problems that will be induced by this change. Dependency management are at the heart of the complexity of digital transformation. Because operations must go on, enterprise and IT architects cannot do whatever they want. They have to work on a credible plan that will create a roadmap of transformation taking dependencies into accounts.","title":"Revolution #5: Managing Dependencies"},{"location":"articles/archimate-intro/#revolution-6-modeling-transformation-modeling-time","text":"Modeling digital transformation is a real piece of work if we want to do a serious job. But it is possible - and that's a revolution that Archimate enables. Yes, it is possible to create, pilot, manage, anticipate, huge and complex enterprise digital transformation with Archimate. And frankly, I cannot see what other approach can do that. At a certain point, reading the forums, I am not convinced that many architects really understood this point. We can now create pretty accurate scenarios of transformation inside Archimate models, taking into account the organization, the strategy, the IT systems, and so on. Maybe we have here the real role of the enterprise architect. Maybe the enterprise architect should be named the enterprise transformation architect . Enterprises should realize that transforming the processes and the IT systems can become an engineering discipline at last , and that using Archimate for this kind of critical topics enables to avoid many troubles and to spend unnecessary money. Modeling transformation is modeling time and so conventions must be chosen to tag artifacts as being existent or not at a certain phase of the transformation process. As of now, the Archimate modeling tools do not support a temporal view of models (like Franz Gruff AllegroGraph visualizer), we have to take conventions [5].","title":"Revolution #6: Modeling Transformation, Modeling Time"},{"location":"articles/archimate-intro/#revolution-7-using-archimate-in-many-software-activities","text":"Indeed, many software activities can use Archimate on a regular basis, even if the language is more targeting architecture purposes. When we look carefully at nowadays software, most software are highly interconnected to other software or within the enterprise or in the Cloud. Archimate can help analyzing the structural impacts of those interconnections inside the software itself.","title":"Revolution #7: Using Archimate In Many Software Activities"},{"location":"articles/archimate-intro/#cartography-of-systems","text":"Archimate can be used to create cartographies of IT systems but many architects should realize that Archimate models are working tools more than poster tools.","title":"Cartography of Systems"},{"location":"articles/archimate-intro/#conclusion","text":"Archimate is, for me, the engineering revolution of digital transformation of the last 15 years. It redefines the enterprise architect role as an enterprise transformation architect role that is able to study with all stakeholders the operational application of a company digital strategy and its impacts on the business, the processes and the IT systems. Once the study is done, Archimate models are of great help to tackle the complex programs, manage the project dependencies and optimize the whole transformation, in terms of time and in terms of efficiency and costs. We must never forget that the enterprise architect should be a crucial change agent that proposes solutions to a strategic problem and should work in the plain knowledge of the business processes. Most digital transformation offices should also carefully consider using Archimate to manage their transformation plans and try to be in control of the costs and schedule.","title":"Conclusion"},{"location":"articles/archimate-intro/#see-also","text":"Archimate recipes","title":"See also"},{"location":"articles/archimate-intro/#notes","text":"[1] The only modeling approach that look like Archimate is the Aris methodology . Even if the methodology did not cover the full scope of Archimate, the theoretical approach was similar. [2] We could compare a UML design tool with the CAD tools. The use of the modeling language must enable to work on the model of the programs before coding in order to create the best design possible, a design that will be implementable quite straight forwardly. Unfortunately with UML, the objective is not reached. [3] Some big Enterprise Architecture frameworks used in the military world are proposing very complex meta-models that are proposing, at the same time, sets of disconnected meta-models and semantic overlap between artifacts. Those frameworks are often at the center of big project failures in the military industry. [4] In projects, I often use or advise to use \"level\" (or scale) indications of some artifacts like business processes or business functions. The indication can be put in the name of the artifact or as an attribute. For instance, Accounting will become (L1) Accounting to indicate that, in some views, the business functions will be represented in a high level way. All detailed functions inside this one (aggregation or composition link) will be flagged (L2) such as (L2) Centralization . This enables to have various levels of relationships between concepts and to be able to drill down inside the model in order to see more detailed (and more accurate) representations. [5] The temporal conventions can be as the ones we mentioned for scale management, something like (P3) for phase 3. Tagging the artifact name enables to have possibly many instances of the same artifact, instances that will evolve. The conventions must be chosen very carefully to be able to see what changes and what is remaining the same throughout the whole change process. (January 2018)","title":"Notes"},{"location":"articles/archimate-recipes/","text":"Archimate recipes Modeling in Archimate can sometimes present some difficulties. In this article, we will expose some tips and tricks that can facilitate your daily life as an enterprise architect using Archimate in your modeling activities. Prerequisites This article is using Archi . This tool is great. If you are using it, please consider making a donation. Introduction to tags About tags Archimate is a very powerful language but sometimes, it can be very useful to create tags to distinguish artifacts or group them into sets. We will put some tags in the name of the artifact in some cases, and in other cases, we can benefit from the use of properties (for instance to process some calculation based on some property values). The convention we use in this article is to \"tag\" the names of the artifacts by placing a label between parenthesis. Tags are sometimes linked to derived relationships but often they answer to other needs. Why not using stereotypes? Modeling languages like UML or SysML use stereotypes to specialialize model elements and to create new artifacts that derive from a master artifact (one from the standard). Example: If you want to model a graph of data, you may need 2 new artifacts: Node and Relationship instead of Data Objet . You can use the left representation of Figure 1 and create new data objects from the specializations with the UML convention stereotype . Figure 1 : Modeling a graph with and without stereotypes Stereotypes are creating new classes of objects. Mentioning stereotypes can be a good way to indicate to your fellow modelers (having a UML or SysML background) what were your intentions. But you can also use the right representation and make your classes derive directly from Node and Relationship , considered as implicitit stereotypes of Data Objet . In the rest of the page, we will more focus on tags that are not supposed to replace the stereotype specialization. Levels of nesting Basics In an Archimate model, it is quite common to model nested objects like shown in Figure 2. Figure 2 : Sample of nesting in the business layer In Figure 2, we can see that the Accountant role is assigned both to the Accounting business function and to the Closing business process. The Closing artifact is nested into the Accounting function that is, in our model, a business function of the highest level. Being the highest level of \"granularity\" of our processes (in some cases, this level can be the one of the quality system of the company), we can tag the Accounting artifact with the level (L1) to indicate its level. All the business processes and/or business functions that will be nested into it will be tagged (L2) for \"level 2\". All nested objects inside level 2 objects will be tagged level 3 (L3) and so on. An option is to flag the relationship with the level as shown in Figure 2. This can be superfluous if the artifacts are already tagged. Graph view This simple system enables to sort artifacts per level on the Archimate graph view (see Figure 3). Figure 3 : Archimate graph centered in the Accountant artifact When the diagram has many artifacts, it is possible to sort the level of vision that we want to have in the graph view. This kind of need appears for instance when we want to have a good vision of the full coverage of a certain point of view. In the graph view, helped with the tags, we can sort all processes attached to the Accountant role per level in order to determine in, in all our views, we cover what we imagine as being all (L1) processes. Figure 4 : Sorted Archimate graph centered in the Accountant artifact When the model has many views with many artifacts at various different levels, the graph view helps making the model consistent. Working at different levels In Figure 5, working with process levels enabled the enterprise architect to indicate that, at a certain level (L2) , the main role taking in charge the Closing process is the Accountant , but at a lower level, we have other roles taking in charge a part of the process. Figure 5 : Main role and secondary role For sure, all diagrams should be consistent, so possibly not to show the relationships that could be misleading in the view. For instance, in Figure 5, the assignment link between Accountant and (L2) Closing is misleading in the diagram. Because, it this diagram, we should only show the various roles that collaborate to the sub-processes of the (L2) Closing process. Figure 6 : Main role and secondary role (less ambiguous view) Figure 6 is showing a much less ambiguous view. For sure, as soon as we we look at those processes at the (L2) level, the Business controller role will become invisible. Note on derived relationships Semantically, we try to express an ambiguity. Figure 2 could say that the Accountant role is assigned to Closing and \"in a more general way\" to the Accounting function. But we know nothing about the other (L2) functions and if the Accountant role is really assigned to all (L2) functions that would compose the Accounting function. The definition of a relationship is not really helping: If two structural or dependency relationships r:R and s:S are permitted between elements a, b, and c such that r(a,b) and s(b,c), then a structural relationship t:T is also permitted, with t(a,c) and type T being the weakest of R and S. Source Let's say C is the composition relationship (\"composes\") and A the assignment relationship (\"is assigned to\"). We have: R1 = Closing C Accounting R2 = Accountant A Accounting R3 = Accountant A Closing So, in the context of structural relationships derivation rules, we could say that R3 + R1 rarr; R2 . That means that R2 is a derived relationship. In the activity of modeling, derived relationships are not always interesting. Indeed, in Figure 5, we don't want to have Business controller assigned to Closing , even if Closing is composed with Validate provisions . We believe that, in such cases, it is clearer to use tags. When watching a role, we can see immediately at what level of process he is assigned. If we don't use the derive relationship, we can express complex situation of drill down that are, strictly speaking, not complete, but that \"fit\" better to the reality. Functional content of application components A very common requirement that we have in modeling the IT systems in Archimate is the requirement of modeling application functions at different levels. Figure 7 : Accounting system basic view The Figure 7 shows a basic accounting system with implicit assignment and aggregation links. By tagging the functions with their level, we can clarify at what level we are looking at the model. The problem is to represent a zoom on a particular function like shown in Figure 8. Figure 8 : The need to represent an ambiguous assignment This assignment relationship being flagged, the graph view enables a grouping of functions per levels (L1) and (L2) . If we think about coverage, e.g. to answer to the question \"what are, in all views of the model, all the functions of the accounting system?\", we'll have to consider only the (L1) . Figure 9 : The need to represent an ambiguous assignment Indeed the relationship Accounting System to Monthly closing management that we created in a certain viewpoint is a derived relationship, and the consequence of Monthly closing management being aggregated into General ledger management . Without tags, this relation could be ambiguous. With tags, we know right away in Figure 8 that the assignment relation is probably some kind of shortcut, used to highlight a specific point, and that (L1) functions are also assigned to Accounting System , the (L2) Monthly closing management being a (L2) function. We have to be vigilant of those relationships to (L2) elements because they will probably not enable the talk about coverage. If the (L1) functions present a full coverage representation of the application component, the (L2) level may not. Indeed, when using a (L2) level in in (L1) , we should think about the fact that the set of (L2) elements is representing correctly the (L1) . In this sample, the underlying hypothesis is that every application component will be assigned to a hierarchy of (L1) functions. Using tags in this way enables to have a more precise vision of the reality. Alternates solution for nesting An alternate solution was proposed by Philippe Augras in LinkedIn using hierarchy of meanings tagging the various levels of nesting (artifacts and relationships). Another alternate solution was propose by Nicolas Figay in LinkedIn using properties and specific views. Conclusion on nesting Nesting is an important part of big Archimate models. Generally, it is very useful to have certain artifacts (like active structures for instance) attached to several different levels of behaviors depending on the views. Most of the time, tagging artifacts with their level: Enable not to confuse the levels of artifacts, Get interested between the nested relationships between various levels of artifacts, Think about coverage (in our case, processes of a certain level per role, or sub-processes of a certain process). Organisational consequences of nesting If a team of enterprise architects are working on the same model, it is generally required to organize frequent \"consistency meetings\" to ensure the consistency of the model. Indeed, categorizing processes of application functions in a nested environment may have impacts to the global model, and sometimes, the decision is quite structuring for the overall representation of reality we try to create. Don't underestimate this part because, at the end of the day, it can become an enormous (but rewarding) amount of work. In that case, a consistency meeting can re-assess at the team level the various levels of nesting that are attached to artifacts. All consequences can be analyzed at the proper level to ensure the model consistency. For that purpose, graph views of the model are more than needed. In my experience, leading a team of 4 enterprise architects for a global digital transformation projects, our \"consistency workshops\" lasted between one or two days (model of around 6,000 artifacts). We were locked in a room, synthetizing the situation and preparing a list of questions for the business teams and for the IT teams to clarify unclear points. In some complex cases, some entities were moving from one level to another depending on the information consolidated. Tasks would also be generated consequently to those changes in perspective in order to complement or clarify the descriptions (being artifact based or textual). At the end of the workshops, we had collective arguments to represent the reality as it was, and a real consistent global model, that would enable us to really plan the process/IT digital transformation. Ensuring this consistency enables to address core problems to the clients (business and IT), based on the cross-checking of all information they provided, and to bring added-value. It enables to be more pertinent and to understand more deeply the core functional problems that are at the heart of most digital transformation. We must never forget that representing the reality (or the next evolutions of that reality) is a complex exercise, even with a semantic language like Archimate. Quite often, we are bound to introduce bias in the representation, which is normal, but which should be explained at the beginning of the model, in a kind of \"conventions\" part. Several instances of the same software in different contexts Big software such as ERP often propose several modules. Documenting the proposed modules can be of interest to be able to capitalize on the descriptions, especially of some customizations were done inside the off-the-shelf software. The figure 10 shows an illustration of that case: The module 3 of the ERP was customized centrally to the company for the needs of the French subsidiary. Figure 10 : Standard and customized modules The (FR) tag will enable to see directly that the function is customized in a certain context. The complexity comes when we will try to represent the various instances of the ERP running for the various subsidiaries. Figure 11 : Several ERP instances In figure 11, we use the specialization relationship to define the \"instances\" of ERP used by several subsidiaries based on the \"central version\" of the ERP. This figure is ambiguous, however. It lets think that every subsidiary is using all modules of the ERP, whereas it is probably not the case. The (L1) ERP is the description of the \"central version\" of the ERP, the one that will be deployed per subsidiary, and the description of its sub-modules. But we also need sub-modules for the instances of that product, knowing that only the French subsidiary is using the customized (L2) ERP module 3 . Figure 12 : Linking sub modules The Figure 12 proposes a way to reuse the sub-modules defined at the (L1) ERP level. This can be sufficient if only the French subsidiary is using the (L2) ERP module 3 . But if the US subsidiary was to use also the module 3 without the French customization, we would have to reconsider the modeling in order to better represent the reality. Figure 13 : Specializing module 3 The figure 13 shows how to specialize module 3 in order to represent the reality. The US ERP will use the standard ERP module 3 with standard functions, whereas the French subsidiary will use a customized version of the module 3 containing the French customizations. By creating another module, specialized from the original module 3, we were able to express the reality. The tags also enable the identification of the various dimensions that we want to keep track of: The various level of nesting of components, The organizations running the software, The business domains. All the tags used in the model should be documented precisely in the model description. They represent a kind of specific taxonomy of the various artifacts and ease the understanding of complex situations. Managing time and scenarios Quite often in the digital transformation process, we need to establish the \"as-is\" situation and the \"to-be\" situation. This can be quite tricky if some naming conventions were not determined to make understand what are the \"as-is\" artifacts and the \"to-be\" ones. For instance, Figure 13 shows us a customized ERP module 3 for the French subsidiary. Let us suppose that this module is the \"to-be\" situation. Figure 14 : As-is situation The Figure 14 would describe the as-is situation. We know we need customized functions for the French subsidiary. Those functions will have to be implemented in some application component, being within the ERP module 3 or in an external module. That leads us to 2 scenarios. Figure 15 : 2 scenarios The Figure 15 shows a way of representing the 2 scenarios: One being a direct modification of module 3 with the new functions (scenario 1); One being a light modification of module 3 with the access to an external application implementing the new functions. In that case, we have 3 artifacts corresponding to the module 3, each of them having a different temporal tag: (as-is) for the current module 3, (s1) for the first scenario, (s2) for the second one. We decided to keep the link between the evolutions of the module 3 ( (s1) and (s2) tags) and their origin tagged (as-is) by using a specialization relationship tagged with the scenario number. This convention enables to work on alternate scenarios. Le's suppose now that we want to describe the following scenario: Step 1: Implementing the customized functions into the ERP module; Step 2: Getting those customized functions out of the ERP module. We would have the situation presented in Figure 16 with (s1) and (s2) representing the steps 1 and 2. Figure 16 : Evolution of solutions with time This solution has an advantage: the ERP content is always exact, because the tags \"as-is\" and \"to-be\" duplicated the \"ERP\" artifact. The drawback is that the ERP artifact is not unique. Another description choice would have been to keep only one artifact for the ERP component. The component would have aggregated the several various \"versions\" of the module 3. In that case, the links between the ERP and the version of sub-module should have been very explicit in order to keep track of the various modifications of the module 3. Using temporal tags enable to better formalize scenarios and successions of steps in a context of digital transformation. This use must be adapted to the purpose of the project. In all cases, the trade-off analysis result should be explained in the model itself. About tags In this article, we have introduced the use of tags in several cases: Nesting Multiple instances Time management The first case can be used for clarity and consistency. Using tags for nesting does not add semantic information to the model, but enables it to be more readable. The case of multiple instances is more complex, because we often want to represent several meanings at the same time, for instance two companies are using the same software but not really the same functions in it. In that case, a specific trade-off must be done in order to find the correct level of expressiveness. The case of time management is particularly useful to represent the various states of a digital transformation, or the various scenarios to go from one state to another. In that case also, the trade-off analysis is important and depends on what we really want to express. One simple rule can be to use different artifacts in the case when those artifacts can be estimated separately. For instance in Figure 15, the various versions of the module 3 will enable different sizing. The fact that we have only one ERP artifact would take the hypothesis that the version of the module 3 has no impact on the full ERP, whereas having 2 ERP artifacts will lead to think that depending on the version of the module 3, the ERP component could be impacted. We can note that those tags are not adding new semantic content (like UML stereotypes can do for instance). In this situation, tags can really be of help in quite a large number of situations. In Figure 11, we used tags to propagate two Archimate information in the tags of the software: Location Main organization using the software This use should be balanced and challenged. We did that in order to materialize the various instances of the software and the fact that those instances were really depending on the where and the who. On the other hand, native Archimate artifacts are existing which can be ambiguous. Sometimes, tags are just temporary ways to represent a complex reality before using a more standard way to do it. Remember that models are dynamic and often work in progress and that there is no definitive way of modeling complex reality. See also Introduction to Archimate Reports can be generated from the Archimate model. You can find here a report template slightly modified compared to the original Archi one. ( Last update: June 2021 )","title":"Archimate Recipes"},{"location":"articles/archimate-recipes/#archimate-recipes","text":"Modeling in Archimate can sometimes present some difficulties. In this article, we will expose some tips and tricks that can facilitate your daily life as an enterprise architect using Archimate in your modeling activities.","title":"Archimate recipes"},{"location":"articles/archimate-recipes/#prerequisites","text":"This article is using Archi . This tool is great. If you are using it, please consider making a donation.","title":"Prerequisites"},{"location":"articles/archimate-recipes/#introduction-to-tags","text":"","title":"Introduction to tags"},{"location":"articles/archimate-recipes/#about-tags","text":"Archimate is a very powerful language but sometimes, it can be very useful to create tags to distinguish artifacts or group them into sets. We will put some tags in the name of the artifact in some cases, and in other cases, we can benefit from the use of properties (for instance to process some calculation based on some property values). The convention we use in this article is to \"tag\" the names of the artifacts by placing a label between parenthesis. Tags are sometimes linked to derived relationships but often they answer to other needs.","title":"About tags"},{"location":"articles/archimate-recipes/#why-not-using-stereotypes","text":"Modeling languages like UML or SysML use stereotypes to specialialize model elements and to create new artifacts that derive from a master artifact (one from the standard). Example: If you want to model a graph of data, you may need 2 new artifacts: Node and Relationship instead of Data Objet . You can use the left representation of Figure 1 and create new data objects from the specializations with the UML convention stereotype . Figure 1 : Modeling a graph with and without stereotypes Stereotypes are creating new classes of objects. Mentioning stereotypes can be a good way to indicate to your fellow modelers (having a UML or SysML background) what were your intentions. But you can also use the right representation and make your classes derive directly from Node and Relationship , considered as implicitit stereotypes of Data Objet . In the rest of the page, we will more focus on tags that are not supposed to replace the stereotype specialization.","title":"Why not using stereotypes?"},{"location":"articles/archimate-recipes/#levels-of-nesting","text":"","title":"Levels of nesting"},{"location":"articles/archimate-recipes/#basics","text":"In an Archimate model, it is quite common to model nested objects like shown in Figure 2. Figure 2 : Sample of nesting in the business layer In Figure 2, we can see that the Accountant role is assigned both to the Accounting business function and to the Closing business process. The Closing artifact is nested into the Accounting function that is, in our model, a business function of the highest level. Being the highest level of \"granularity\" of our processes (in some cases, this level can be the one of the quality system of the company), we can tag the Accounting artifact with the level (L1) to indicate its level. All the business processes and/or business functions that will be nested into it will be tagged (L2) for \"level 2\". All nested objects inside level 2 objects will be tagged level 3 (L3) and so on. An option is to flag the relationship with the level as shown in Figure 2. This can be superfluous if the artifacts are already tagged.","title":"Basics"},{"location":"articles/archimate-recipes/#graph-view","text":"This simple system enables to sort artifacts per level on the Archimate graph view (see Figure 3). Figure 3 : Archimate graph centered in the Accountant artifact When the diagram has many artifacts, it is possible to sort the level of vision that we want to have in the graph view. This kind of need appears for instance when we want to have a good vision of the full coverage of a certain point of view. In the graph view, helped with the tags, we can sort all processes attached to the Accountant role per level in order to determine in, in all our views, we cover what we imagine as being all (L1) processes. Figure 4 : Sorted Archimate graph centered in the Accountant artifact When the model has many views with many artifacts at various different levels, the graph view helps making the model consistent.","title":"Graph view"},{"location":"articles/archimate-recipes/#working-at-different-levels","text":"In Figure 5, working with process levels enabled the enterprise architect to indicate that, at a certain level (L2) , the main role taking in charge the Closing process is the Accountant , but at a lower level, we have other roles taking in charge a part of the process. Figure 5 : Main role and secondary role For sure, all diagrams should be consistent, so possibly not to show the relationships that could be misleading in the view. For instance, in Figure 5, the assignment link between Accountant and (L2) Closing is misleading in the diagram. Because, it this diagram, we should only show the various roles that collaborate to the sub-processes of the (L2) Closing process. Figure 6 : Main role and secondary role (less ambiguous view) Figure 6 is showing a much less ambiguous view. For sure, as soon as we we look at those processes at the (L2) level, the Business controller role will become invisible.","title":"Working at different levels"},{"location":"articles/archimate-recipes/#note-on-derived-relationships","text":"Semantically, we try to express an ambiguity. Figure 2 could say that the Accountant role is assigned to Closing and \"in a more general way\" to the Accounting function. But we know nothing about the other (L2) functions and if the Accountant role is really assigned to all (L2) functions that would compose the Accounting function. The definition of a relationship is not really helping: If two structural or dependency relationships r:R and s:S are permitted between elements a, b, and c such that r(a,b) and s(b,c), then a structural relationship t:T is also permitted, with t(a,c) and type T being the weakest of R and S. Source Let's say C is the composition relationship (\"composes\") and A the assignment relationship (\"is assigned to\"). We have: R1 = Closing C Accounting R2 = Accountant A Accounting R3 = Accountant A Closing So, in the context of structural relationships derivation rules, we could say that R3 + R1 rarr; R2 . That means that R2 is a derived relationship. In the activity of modeling, derived relationships are not always interesting. Indeed, in Figure 5, we don't want to have Business controller assigned to Closing , even if Closing is composed with Validate provisions . We believe that, in such cases, it is clearer to use tags. When watching a role, we can see immediately at what level of process he is assigned. If we don't use the derive relationship, we can express complex situation of drill down that are, strictly speaking, not complete, but that \"fit\" better to the reality.","title":"Note on derived relationships"},{"location":"articles/archimate-recipes/#functional-content-of-application-components","text":"A very common requirement that we have in modeling the IT systems in Archimate is the requirement of modeling application functions at different levels. Figure 7 : Accounting system basic view The Figure 7 shows a basic accounting system with implicit assignment and aggregation links. By tagging the functions with their level, we can clarify at what level we are looking at the model. The problem is to represent a zoom on a particular function like shown in Figure 8. Figure 8 : The need to represent an ambiguous assignment This assignment relationship being flagged, the graph view enables a grouping of functions per levels (L1) and (L2) . If we think about coverage, e.g. to answer to the question \"what are, in all views of the model, all the functions of the accounting system?\", we'll have to consider only the (L1) . Figure 9 : The need to represent an ambiguous assignment Indeed the relationship Accounting System to Monthly closing management that we created in a certain viewpoint is a derived relationship, and the consequence of Monthly closing management being aggregated into General ledger management . Without tags, this relation could be ambiguous. With tags, we know right away in Figure 8 that the assignment relation is probably some kind of shortcut, used to highlight a specific point, and that (L1) functions are also assigned to Accounting System , the (L2) Monthly closing management being a (L2) function. We have to be vigilant of those relationships to (L2) elements because they will probably not enable the talk about coverage. If the (L1) functions present a full coverage representation of the application component, the (L2) level may not. Indeed, when using a (L2) level in in (L1) , we should think about the fact that the set of (L2) elements is representing correctly the (L1) . In this sample, the underlying hypothesis is that every application component will be assigned to a hierarchy of (L1) functions. Using tags in this way enables to have a more precise vision of the reality.","title":"Functional content of application components"},{"location":"articles/archimate-recipes/#alternates-solution-for-nesting","text":"An alternate solution was proposed by Philippe Augras in LinkedIn using hierarchy of meanings tagging the various levels of nesting (artifacts and relationships). Another alternate solution was propose by Nicolas Figay in LinkedIn using properties and specific views.","title":"Alternates solution for nesting"},{"location":"articles/archimate-recipes/#conclusion-on-nesting","text":"Nesting is an important part of big Archimate models. Generally, it is very useful to have certain artifacts (like active structures for instance) attached to several different levels of behaviors depending on the views. Most of the time, tagging artifacts with their level: Enable not to confuse the levels of artifacts, Get interested between the nested relationships between various levels of artifacts, Think about coverage (in our case, processes of a certain level per role, or sub-processes of a certain process).","title":"Conclusion on nesting"},{"location":"articles/archimate-recipes/#organisational-consequences-of-nesting","text":"If a team of enterprise architects are working on the same model, it is generally required to organize frequent \"consistency meetings\" to ensure the consistency of the model. Indeed, categorizing processes of application functions in a nested environment may have impacts to the global model, and sometimes, the decision is quite structuring for the overall representation of reality we try to create. Don't underestimate this part because, at the end of the day, it can become an enormous (but rewarding) amount of work. In that case, a consistency meeting can re-assess at the team level the various levels of nesting that are attached to artifacts. All consequences can be analyzed at the proper level to ensure the model consistency. For that purpose, graph views of the model are more than needed. In my experience, leading a team of 4 enterprise architects for a global digital transformation projects, our \"consistency workshops\" lasted between one or two days (model of around 6,000 artifacts). We were locked in a room, synthetizing the situation and preparing a list of questions for the business teams and for the IT teams to clarify unclear points. In some complex cases, some entities were moving from one level to another depending on the information consolidated. Tasks would also be generated consequently to those changes in perspective in order to complement or clarify the descriptions (being artifact based or textual). At the end of the workshops, we had collective arguments to represent the reality as it was, and a real consistent global model, that would enable us to really plan the process/IT digital transformation. Ensuring this consistency enables to address core problems to the clients (business and IT), based on the cross-checking of all information they provided, and to bring added-value. It enables to be more pertinent and to understand more deeply the core functional problems that are at the heart of most digital transformation. We must never forget that representing the reality (or the next evolutions of that reality) is a complex exercise, even with a semantic language like Archimate. Quite often, we are bound to introduce bias in the representation, which is normal, but which should be explained at the beginning of the model, in a kind of \"conventions\" part.","title":"Organisational consequences of nesting"},{"location":"articles/archimate-recipes/#several-instances-of-the-same-software-in-different-contexts","text":"Big software such as ERP often propose several modules. Documenting the proposed modules can be of interest to be able to capitalize on the descriptions, especially of some customizations were done inside the off-the-shelf software. The figure 10 shows an illustration of that case: The module 3 of the ERP was customized centrally to the company for the needs of the French subsidiary. Figure 10 : Standard and customized modules The (FR) tag will enable to see directly that the function is customized in a certain context. The complexity comes when we will try to represent the various instances of the ERP running for the various subsidiaries. Figure 11 : Several ERP instances In figure 11, we use the specialization relationship to define the \"instances\" of ERP used by several subsidiaries based on the \"central version\" of the ERP. This figure is ambiguous, however. It lets think that every subsidiary is using all modules of the ERP, whereas it is probably not the case. The (L1) ERP is the description of the \"central version\" of the ERP, the one that will be deployed per subsidiary, and the description of its sub-modules. But we also need sub-modules for the instances of that product, knowing that only the French subsidiary is using the customized (L2) ERP module 3 . Figure 12 : Linking sub modules The Figure 12 proposes a way to reuse the sub-modules defined at the (L1) ERP level. This can be sufficient if only the French subsidiary is using the (L2) ERP module 3 . But if the US subsidiary was to use also the module 3 without the French customization, we would have to reconsider the modeling in order to better represent the reality. Figure 13 : Specializing module 3 The figure 13 shows how to specialize module 3 in order to represent the reality. The US ERP will use the standard ERP module 3 with standard functions, whereas the French subsidiary will use a customized version of the module 3 containing the French customizations. By creating another module, specialized from the original module 3, we were able to express the reality. The tags also enable the identification of the various dimensions that we want to keep track of: The various level of nesting of components, The organizations running the software, The business domains. All the tags used in the model should be documented precisely in the model description. They represent a kind of specific taxonomy of the various artifacts and ease the understanding of complex situations.","title":"Several instances of the same software in different contexts"},{"location":"articles/archimate-recipes/#managing-time-and-scenarios","text":"Quite often in the digital transformation process, we need to establish the \"as-is\" situation and the \"to-be\" situation. This can be quite tricky if some naming conventions were not determined to make understand what are the \"as-is\" artifacts and the \"to-be\" ones. For instance, Figure 13 shows us a customized ERP module 3 for the French subsidiary. Let us suppose that this module is the \"to-be\" situation. Figure 14 : As-is situation The Figure 14 would describe the as-is situation. We know we need customized functions for the French subsidiary. Those functions will have to be implemented in some application component, being within the ERP module 3 or in an external module. That leads us to 2 scenarios. Figure 15 : 2 scenarios The Figure 15 shows a way of representing the 2 scenarios: One being a direct modification of module 3 with the new functions (scenario 1); One being a light modification of module 3 with the access to an external application implementing the new functions. In that case, we have 3 artifacts corresponding to the module 3, each of them having a different temporal tag: (as-is) for the current module 3, (s1) for the first scenario, (s2) for the second one. We decided to keep the link between the evolutions of the module 3 ( (s1) and (s2) tags) and their origin tagged (as-is) by using a specialization relationship tagged with the scenario number. This convention enables to work on alternate scenarios. Le's suppose now that we want to describe the following scenario: Step 1: Implementing the customized functions into the ERP module; Step 2: Getting those customized functions out of the ERP module. We would have the situation presented in Figure 16 with (s1) and (s2) representing the steps 1 and 2. Figure 16 : Evolution of solutions with time This solution has an advantage: the ERP content is always exact, because the tags \"as-is\" and \"to-be\" duplicated the \"ERP\" artifact. The drawback is that the ERP artifact is not unique. Another description choice would have been to keep only one artifact for the ERP component. The component would have aggregated the several various \"versions\" of the module 3. In that case, the links between the ERP and the version of sub-module should have been very explicit in order to keep track of the various modifications of the module 3. Using temporal tags enable to better formalize scenarios and successions of steps in a context of digital transformation. This use must be adapted to the purpose of the project. In all cases, the trade-off analysis result should be explained in the model itself.","title":"Managing time and scenarios"},{"location":"articles/archimate-recipes/#about-tags_1","text":"In this article, we have introduced the use of tags in several cases: Nesting Multiple instances Time management The first case can be used for clarity and consistency. Using tags for nesting does not add semantic information to the model, but enables it to be more readable. The case of multiple instances is more complex, because we often want to represent several meanings at the same time, for instance two companies are using the same software but not really the same functions in it. In that case, a specific trade-off must be done in order to find the correct level of expressiveness. The case of time management is particularly useful to represent the various states of a digital transformation, or the various scenarios to go from one state to another. In that case also, the trade-off analysis is important and depends on what we really want to express. One simple rule can be to use different artifacts in the case when those artifacts can be estimated separately. For instance in Figure 15, the various versions of the module 3 will enable different sizing. The fact that we have only one ERP artifact would take the hypothesis that the version of the module 3 has no impact on the full ERP, whereas having 2 ERP artifacts will lead to think that depending on the version of the module 3, the ERP component could be impacted. We can note that those tags are not adding new semantic content (like UML stereotypes can do for instance). In this situation, tags can really be of help in quite a large number of situations. In Figure 11, we used tags to propagate two Archimate information in the tags of the software: Location Main organization using the software This use should be balanced and challenged. We did that in order to materialize the various instances of the software and the fact that those instances were really depending on the where and the who. On the other hand, native Archimate artifacts are existing which can be ambiguous. Sometimes, tags are just temporary ways to represent a complex reality before using a more standard way to do it. Remember that models are dynamic and often work in progress and that there is no definitive way of modeling complex reality.","title":"About tags"},{"location":"articles/archimate-recipes/#see-also","text":"Introduction to Archimate Reports can be generated from the Archimate model. You can find here a report template slightly modified compared to the original Archi one. ( Last update: June 2021 )","title":"See also"},{"location":"articles/conf-mgt/","text":"Configuration management of industrial products in PDM/PLM This article is the second one of a series on PLM. Please have a look at PLM and Graph Data article for an introduction. Photo by Martin Kessel from FreeImages Configuration management (CM) is at the heart of the industry, being the traditional industry or the software industry. While the software industry massively invested on this area for decades, reaching a certain level of maturity in which CM is no longer a problem, some areas of the traditional industry still use old CM practices that prevent other digital evolutions to be massively used, such as works at the digital twin level. When CM stays a matter of experts and is not streamlined in the industrial process, inefficiencies, productivity losses and bad quality of data are everywhere. This implies, generally, many product defects and customer dissatisfaction. In this article, we will try to examine the two different ways of performing CM: The dynamic one, where the configuration of a product is the result of a calculation (quite often based on a complex filtering); The static one, where the configuration of a product is benefiting from the graph data that we can find in PLMs (see PLM and Graph Data for more information). Strangely, this topic is not massively discussed on the web while it is at the heart of the industry performance (and non performance), being during the design process, the manufacturing phase of product life cycle or during its support phase. The old way: Filtering the product tree We will explore the first kind of industrial configuration management, the dynamic one, based on the the filtering, more or less elaborated, of the product tree. The product tree In Part Data Management (PDM) systems, a product is represented by a tree, which leaves are parts or groups of parts (often called \"design solutions\"). The product tree structure is generally the fruit of several inspirations, amongst which we can find: A functional split of the product (quite often coming from the system engineering view of the product); A regulatory view of the product (e.g. a split of an aircraft product per ATA code of the systems in the aerospace world); An organizational view of the product, e.g. mapping the product in the various organizations of the engineering office that design the product (mechanical engineering, electrical engineering, engine, air conditioning, etc.); A mix between technical PDM systems capabilities (sometimes related to CM) and between the previous constraints. The product tree, or \"product structure\", is containing the product with all its options, which is sometimes called \"the 150% vision of the product\". For sure, all options will not be applicable at the same time on a single instance of the product. The big challenges are, in this context: To be able to get the exact list of applicable parts (or applicable design solutions) for a certain instance of the product; To be able to manage the catalog of applicable options at a certain moment to future instances of the product; To be able to manage change in this universe, for instance when some parts become obsolete and must be exchanged by something else. Moreover, the configurations of products that were manufactured or built must be preserved for maintenance. That can mean being able to create maintenance manuals and spare part lists for all products that were, at some point, manufactured and/or built. That would mean keeping track of the individual configuration of some product at some key moments of their life cycle. Manual version management in the product tree Generally, old PDM systems don't manage natively the version management. When a leaf of the tree changes, let's suppose it is a design solution, a new version of the design solution must be created manually by copying the previous one and its content and name it with a version number in its name. Let's take an example: The brakes of the bicycle product (see Figure 1) are changing because the brakes provider does not manufacture the old brakes anymore. In the PDM system, we must keep the old brake version because those information can be useful for the support of the already manufactured bicycles. We create a new brakes design solution by copying the old one, renaming it, and changing in it what needs to be changed to take into account the new version of the component. From a certain point in time, from a certain instance of the bicycle product, all brakes will be of the new release V2 . Generally, industrially, we will move from the old version V1 to the new one V2 when the stock of old brakes will be empty. This shows that the changes in product configuration can have quickly industrial impacts. In the PDM system, we end up having, at the bottom of one branch, two versions of the brakes. If the product tree was inspired by a systems engineering approach, we should have a generic \"brakes branch\", corresponding to the \"brake function\", with two possible implementations: One that was used from the first instances of the bicycle product up to instance number N (we will often speak about product \"serial number\" or S/N to identify in a unique way the instances of the product), And the second one that will be used from instance number N+1 and for the future (see Figure 1). Figure 1: Basic applicability in PDM Configuration management using links We can provide a quick definition of applicability in this context: For a product with a serial number K , the brakes version 1 are applicable to S/N K if K is inferior or equal to N ; From N+1 , the version 2 of the brake is applicable. In PDM systems, the applicability is traditionally attached to the links as shown in Figure 1. Applicabilities are, in that case, intervals of product S/N: Brakes V1 are applicable to bicycle which S/N is in the range [1, N] , and brakes V2 are applicable to the products which S/N is in the range [N+1, infinity[ . Having defined the applicabilities on all links, it is possible to filter the full product tree with a specific S/N to see what is applicable to this S/N. The filtered tree will correspond to the subset of the product tree corresponding to the specific options that are present in the product which serial number is K . For sure, if we try to filter the product tree with a S/N that does not exist (i.e. in the future), we may retrieve (depending on how the filtering mechanism is implemented) the list of all possible design solutions that are applicable from now . In the case of Figure 1, for sure, the V1 of the brakes will not be applicable anymore for any S/N superior to N . Without any particular definition of our S/N options, we may see more than 100% of a product because we did not defined the very features of that S/N yet, and we may get all available options (product catalog) with this filter. The product catalog In order to maintain a catalog of options and to manage options compatibility, we need to have those data store somewhere (see Figure 2). Figure 2: Basic PDM product catalog In some cases, those crucial data will be maintained outside the PDM, in Excel, or in a separate Access database, which will rely on manual actions for the synchronization (source of errors and inconsistencies); in some other times, those data will be maintained inside the PDM system itself. Compatibility tables are an important element of this product catalog. Very frequently, they implement non trivial rules that depend on one another. The Figure 2 shows a sample of that: the basic breaks in V2 are compatible with both basic wheels in V3 and premium wheels in V2 . The product catalog is crucial because it is enabling: To create a vision of the product options for the marketing and sales department (in a certain way to define the common language between marketing and sales and the engineering departments); To attach to a specific S/N of product a list of options; To guarantee that those options are compatible with one another. The product catalog introduces a S/N vision inside the PDM system. From now on, without any product catalog vision, we could live with just \"S/N intervals\" of applicabilities. But if we make the S/N concrete and attached to a specific set of options, we can ask ourselves what is the best way of managing this information inside the system (we will come back on that point). Managing the changes For, sure, as shown in Figure 2, components have versions, and we must track the changes between those versions. Change tracking is fundamental because it has an industrial impact, especially on stocks, on procurement, on all the manufacturing process, and on the maintenance process. Change management is the set of required actions that are done to justify the change and master its impacts. In some businesses such as aerospace, the change is also tracked to prove to the authority that the aircraft can \"inherit\" from a past certification because the changes performed on it are just \"small\" ones. In case a change is important, a re-certification is required on a part or all the aircraft type. Basically, during the life cycle of the product, all entities will change and will be versioned, including the compatibility tables for the various options. Change objects are interesting to track precisely why we did a change. In complex manufacturing products, change management ensure that the primary objective of the change is respected: fix something, enhance something else, take into account the impact of obsolescence, etc. Special case: Applicability management through changes In some areas such as aerospace, the change is considered so important, because of the fact that it is linked to airworthiness certification of flying machines, that the full product is seen through changes - and not through physical components. To explain the problem, we will simplify a bit the situation. A change is an entity that will describe the replacement of one or several design solutions by one or several new ones. If we consider that an original design solution was \"created by a change\" ( Change 1 in Figure 3 is creating the Brakes in V1 ), we can see the product as a \"stack of changes\". Figure 3: Managing applicabilities with changes In Figure 3, we can see several states of the brakes: The Change 1 is creating the design solution Brakes V1 . The Change 2 does a modification to the Brakes V1 . In our example, this situation is often called a \"retrofit\". The design solution Modification to brakes V1 must be envisaged as being applicable on top of Brakes V1 . This situation occurs when, for instance, a part was forgotten in the original Brakes V1 design solution but, for certain reasons, we can't change directly the design solution Brakes V1 . To apply this retrofit, both Change 1 and Change 2 will be applicable, but maybe some products were manufactured with only the Change 1 and so they may have defects. For sure, the manufacturing and the support will have to be aware that Modification to brakes V1 is not a full design solution but some kind of \"delta\" to be applied to the previous version of the component (here Brakes V1 ). The Change 3 replaces the previous one with a full new design solution that is only pointing to Brakes V2 . It is interesting to note that, in this model, we know if a design solution is applicable or not to a product serial number if the change is applicable to the product serial number . The product is no more a set of components: It is a \"set of changes\". We will call this way of proceeding an \"administrative configuration management method\", administrative because the product is hidden behind the administrative iterative change process that leads to define it and to ensure its certification . The main consequence of this kind of configuration management is that a specific S/N of the product cannot be retrieved from the product tree by applying just a filtering on S/N ranges: We must do some kind of complex \"calculation\" to start from the description of the product as a set of changes to find what are the real design solutions that are applicable to this S/N. We can note that this way of working is complexifying the works of everyone, because everyone in the company needs a static vision of the S/N, at least in a certain version: The design office needs to work in a certain product context to perform relevant modifications; The industry needs to know the exact configuration of the S/N in order to build it; The support needs the exact configuration of all S/N to ensure that the maintenance manuals and spare part lists are covering all the manufactured products. The problem of configuration calculation process is that it embeds business rules . An example of that is the retrofit situation that we have just shown in Figure 3. We could have decided never to use this kind of \"delta\" mechanism and only work by a replacement approach. The set of rules that will apply to find back the configuration of a S/N being business rules, they may change with time. This can lead to real problems because with this change in time, it is not guaranteed at all that when we recalculate the configuration of a S/N in the past, we will find again the exact same configuration that we have delivered top the manufacturing team. That is often interpreted, wrongly, as a non reliability of the PDM system that cannot fond back the configurations of the already delivered products. Stacking the changes This model establishes a \"lineage\" between changes that, most of the time, does not enable \"forks\" of design solutions, like shown in Figure 4. Figure 4: Forking a design solution Brakes V3 are clearly a fork of Brakes V1 , so the change should be done between both. But, in a standard \"stack-oriented change system\", the Change 4 will be associated to the next iteration of Brakes V2, which is not the case. This lineage forces to see the product as a continuous set of changes, a pile or a stack of changes. That means that, in case of wrong modifications in the design solutions, it is not possible to get a design solution of the past and to start at this point, which the fork mechanism is all about. It is mandatory to \"undo\" all modifications you did to come back to the previous state. Undoing is generally a manual, costly and error prone process (incredible to people that come from the software industry, where forking is one very basic action in configuration management). From the PDM system to the CLM system This stacking of changes method transforms the PDM system into what we could call a \"CLM\", standing for Change Lifecycle Management, but not into a PLM (Product Lifecycle Management). This statement is very important because, in a certain way, the administrative CM method based on change stacks is proposing an approach that is diverging from the PLM world where the product is at the center and the change on a secondary level. The CLM system, by putting the change first and the product \"behind\" it, closes the door to static graph data usage in PLM. This way of working, as we will see it in the rest of the article, creates cascading problems in all the manufacturing company. The limitations of filtering-based CM Filtering-based CM systems (being simple applicability oriented or change-based applicability oriented) seem to us as being structural sources of problems. Moreover, when investigating their origin, it appears that they are very linked to the PDM system limitations of the 90s. And the fact is PDM systems evolved into PLM systems leveraging the product graph data and enabling a vision of CM closer to what is found in the software industry, a method that we could name \"100% calculated configuration\" or \"static configuration management\". This way of working is the only one to enable works at the digital twin level. Indeed, how can you work on the digital product if the vision you have of the product is the result of a dynamic filtering calculation that will not be persistent enough to enable the creation of the many links you need to do in the graph data? Work around: Isolate from filtering-based configuration If we try to implement concurrent engineering in an environment where the CM is dynamic, we immediately face problems. Let's suppose we work on component C1 and this component is applicable to our S/N at T0 . At a certain point in time T1 , we discover that the configuration calculation declared C1 not applicable anymore. First of all, we will have to find back what is applicable instead of C1 (let's suppose it is C2 ), which may be not easy, but what would be feasible if we are part of the change board. We probably created some data attached to C1 and the problem will be to adapt those data to the C2 case. Typically, we are in that situation where we created manufacturing instructions or support documentation attached to C1 . We have, in this situation, many chances to be forced to live at the rhythm of the engineering configuration changes. The common work around that we can see is a persistence of configuration in another database as shown in Figure 5. Figure 5: Work around with dynamic CM All intermediate configuration calculation will be stored in a separate area with a time stamp. Every other actor can work on a certain stable version for a while, without being disturbed by the continuous changes in applicability. Then, when a version is ready, it is possible to calculate the deltas between the version I was working on and the last one. Ideally, we can calculate one single delta for many accumulated changes (like the software industry is doing). We can then act on the accumulated changes, analyze the impacts on our linked objects and perform the required changes. The most severe consequence in not having a mechanism of systematic persistence of the dynamic configuration is that the manufacturing and the support are bound to work at the rhythm of the design office, whereas their processes and business objects may require a different lifecycle. The new way: Linking static graph data We will now explore the second king of configuration management: the static one based on persistent non mutable graph data. Reusing components in the industry Those last decades, the vision of the manufactured product changed a lot. If manufacturers were used to build almost all parts of their products, they transformed progressively into assemblers, letting the role of manufacturing subsystems to subcontractors (so-called \"suppliers\"). In a lot of cases, the supplier is not only manufacturing components, but it is also in charge of designing them. The manufacturing industry entered the same constraints as the software industry. In order to build your product, you have to assemble components that you did not manufacture, so you have to master their compatibility with your own products. You must then take conventions with your suppliers about \"interfaces\". To come back to our brakes example, as a bike manufacturer, we buy the brakes from an external supplier that will provide brakes to our company but also to other bicycle manufacturers. We have to define in a contract what will be the interfaces between our bicycle and the brakes. Those interfaces can be mechanical, but also, in the general case, electrical, fluid based, etc. This definition of clear interfaces between components is the condition of component reusability. It also enabled a certain specialization of manufacturers that can perform R D activities on their very domain and be innovative. This also created a component market that can imply large worldwide supply chains. Being in machines, in automotive or in aerospace, the assemblers must control the configuration of the components they use. Like in software, during the lifecycle of a product, there will be many versions of the same component available for series production. For each of them, every actor will need information: In order to integrate it in the product tree (and possibly into the product 3D mock-up); In order to assemble it to the other parts of the product; In order to support it in the long run. The site specialization Even if the assembler is mainly assembling components that are not manufactured internally, many manufacturers tend to specialize sites in order to implement the same component-oriented industrial strategy. Big manufacturers have the same constraints than their suppliers and, if they want to keep an expertise on some areas, it is better that they concentrate this expertise on a set of physical locations. Variants and customizations Consequently, the challenge of modern manufacturing is to manage the product as a set of components, each of them having its own lifecycle, and most of them being more or less mastered by suppliers. This model has many advantages: It is easier to create product variants, or product families, product lines, or different brands, considering that many components can be reused from one model to another; The component reusability enables better innovation on each component level; The industrial process is divided between component assembly process and final assembly processes, which implies a good performance of the logistics to be able to transport components to the different assembly lines. The main drawback of the model is to enter a cycle of incremental component improvements without any product redesign. Indeed, redesigning a product is a much bigger task that can change the scope of the various components. When a supply chain is optimized with many internal and external components, the global innovation may have a tendency to decrease. In this context, new players can enter the industry with more effective approaches. Consequences for the product tree The first consequence of this industrial reality is that the configuration must be managed at different levels: At the component level, the configuration must support variants and options; At the product level, the configuration must support which components are applicable. For some big manufacturing products, such as cars, ships or aircrafts, we can have intermediate levels, sort of \"big components\" that aggregate smaller components. The configuration of the product becomes a multi-level configuration management system. The product tree representation should materialize this tree of components like shown in Figure 6. Figure 6: Hierarchy of components We see that the product is composed of several layers of components. By seeing the diagram, we could analyze the various levels: The blue level is the design solution level. In that sample, that is the only layer the manufacturer of the product P really masters in terms of detailed design. The yellow level is the level of \"Small Assemblies\" (prefixed SA ). Only the SA2 is mastered internally. All other small assembly are coming from suppliers. The purple layer is the layer of \"Big Assemblies\" (prefixed BA ). BA1 and BA2 are mastered by the manufacturing company because they are in charge of assembling the small assemblies that composed them. But BA3 is directly coming from a supplier. We can see that the way of organizing the data implies several things: The split of the product in components generally comes from the early phases of the design, especially from the systems engineering phase, that created a product architecture enabling to define the various components and their interfaces. It is required to frequently test the digital fit of the digital components together before trying to assemble them physically. We can call this process the digital integration of components at the \"digital twin\" level. Note that this process can be quite complicated because it can imply the verification of many interfaces on many levels. Product architecture and digital integration are at the core of modern manufacturing. We can note those two activities are at the core of the software industry for decades, and that the manufacturing area converges progressively to the software industry practices, even if some constraints (at the interface level) are more complex in the industry. Adding versions to the product tree If we add the versions of the various components in the product tree, we obtain Figure 7. Figure 7: Hierarchy of components with versions In this figure is shown the lifecycle of some components and their links together. Let's start by analyzing DS1 . This design solution has many versions. Those versions are linked together by a change having a unique ID (e.g. Change 1a2c3 is tagging the link between DS2 in v1.0 and DS2 in v1.1 . In the schema, the chosen convention is that any change on version on a lower component implies a change on the upper one. DS2 in v1.1 is used by SA2 in v1.18 and v2.0 is used by SA2 in v12.0 and in v12.1 . The structural dependencies are shown with black arrows. The head of the product P is pointing to all heads (last versions) of all components. For sure, it is not because the product head is pointing to those components that they are applicable to every instance of the product. Materializing the S/N in the product tree At the top of the diagram, we have an instance of product P with S/N 1212 using the BA1 and BA2 respectively in v25.4 and v56.6 . Each of the big assemblies points to the small assemblies that are in it, and those small assemblies point to the design solution they are composed of. The first important thing to note is that there are no more calculation mechanism: The S/N 1212 in version v234.3 is pointing to existing versions of big assemblies that are fully defined. If an upgrade would be done at whatever level on the tree (in Figure 7, at the design solution level, the small or big assembly level), the version management would enable to create a new version of product P S/N 1212 to point to those specific versions. Note that when a version is \"wired\" with links, it will never change in the future , and ten years later, this past part of the graph will be exactly the same. This way of creating graph data is another way (compared to the filtering approach), much more efficient , to manage applicabilities, configuration and changes. Applicability The notion of applicability, as we saw it in the first part of the article, is twofold: Indicate that a specific product S/N is using a specific component; Indicate that, for future S/N, from a certain S/N, the component to be used should be a certain component in a certain version (something called sometimes \"effectivity\"). Looking at the tree of versions, for instance for DS2 , we can see that we can decorrelate the versions of DS2 and the versions of SA2 : When SA2 was only upgraded once, the DS2 design solution had 2 intermediate versions that were not used to collaborate to the small assembly ( v1.2 and v1.7 ). In a graph mode, the applicability can be seen under 2 angles: Real usage: A certain component really uses some other component in a certain version. The version used is de facto applicable to the component that aggregates the sub-component. Future applicability: It is defined by the set of components that can be used in the future. In a certain way, this is an open choice because all releases of all components are for ever available in the system. It can also be linked to an explicit feature available in the product catalog (see below). In Figure 7, if we suppose that the components that we can produce are linked to the last one, we will be able to produce products using BA1 in version v25.4 , BA2 in version v56.6 and BA3 in version v4.12 , and all their graph of dependencies. We can note also, in Figure 7, that a specific \"study S/N\" XXX01 points to a fork of a design solution located in the past ( DS2 in version v1.2 study A ). That enables to potentially come back to a design solution that could be better in the past and to start from it. Configuration If the dynamic filtering model was requiring a more or less complex algorithm to find the real S/N, the Figure 7 shows that the S/N (in a certain version) is the head of a tree of components that is defined down to the level that is relevant for the proper industry. And, each time a component is changing of release, an impact study can be led to see if this change should also impact the S/N that must be produced. The graph data approach enables, in that case, that all versions of the S/N configuration are secured forever inside the system. By the way, we can see here that the system can be called a \"Product Lifecycle Management (PLM)\" system because it contains the full history of the product line, of the product components (whatever the level) and of all the serial numbers. In a certain way, all configurations are there, are \"solved\", are calculated\" (because there are no more calculation). The fact that all versions of the configuration are secured inside a PLM is critical for many processes, especially the concurrent engineering ones. Let's take an example. Let us suppose the engineering sent the version v12.0 of SA2 to the manufacturing preparation. Some people will work on the assembling instructions of this version of the small assembly pointing to specific versions of design solution DS1 and DS2 . Meanwhile, the engineering department worked in the version v2.1 of the DS2 . Everyone can work in parallel without having any conflict. That is a much better situation than the one we saw previously. Change management In the graph model, changes will tag the various links between two versions of the same component. The fact that the change is restricted to the same component is very important because it is quite different from a model where a change can aggregate a \"before\" status (composed by several components) and an \"after\" status (composed also by several components). This kid of change does not exist anymore and is replaced potentially by a change at an assembly level. The next version of an assembly can embed different components that the previous version of the same assembly. In this model, the change object is limited to change. For a certain product instance (serial number), finding its delta of changes compared to a certain version in the history (for instance a certified baseline) will consist in aggregating all the changes that led to all versions of all its sub-components. In Figure 7, the component SA2 changes version from v11.8 to v12.0 while skipping two versions of DS2 . In a certain way, the change 3df59 corresponds to the union of changes 12bb5 , aze129 and 1a44ho (because also v11.8 and v12.0 of SA2 are using the same version of DS1 ). In order to be able to commit, at the SA2 level, the change, specific digital tools enabling to compare directly version v2.0 and v1.1 of DS1 are needed. Contrary to the change management we saw in the first part of the article, each real state of each component is secured and there is no need to create tricks to recompose the true component or design solution. Using graph data in PLM enables to put the change behind the product and to enter the world of PLM, rather to keep working in a CLM world. This model should be seen as the real product-oriented model. There are two major differences between this model and the filtering tree model: All configurations are solved and explicit in the graph model; To get the set of changes between a product S/N and a baseline in the past, we have to perform some graph analysis, comparing the graph of the S/N and the graph of the baseline and gathering all changes that enabled to go from the one to the other. In one word, the product comes first and the change comes second. A system with this kind of management can be called a product lifecycle management system. Product catalog An interesting thing to note is that is a \"solved graph\" containing all design solutions (or parts) in all versions, all assemblies, whatever their size, all serial numbers, it is very easy to create a product catalog (see Figure 7). Indeed, the product catalog is: The head of all the applicable elements at a certain point in time, with the desired level of granularity; The compatibility and incompatibility tables between the various options. We can also see that, if we put in place, inside the PLM, a process of choosing the baseline (the major set of components that is applicable to all products, BA1 in Figure 7) and all the options, the product instance definition can create a S/N as a new head in the graph pointing the right versions of all components (like shown in Figure 7). In that model, there is no calculation: The S/N, in its first version, is the direct fruit of the product catalog. It is directly usable by the manufacturing and the support. In a certain way, we have defined a direct and non ambiguous link between marketing and sales, engineering, manufacturing and support. More PLM features Many features that are considered as basic and standard by the software industry would be of great interest in the industries that are not using PLM (or using PLM with a filtering configuration approach). For instance, the fixes on some components (retrofits) can be solved by forking a component (like DS2 in version v1.2 study A ) to fix it in a parallel branch. In order to get back the fix on the main branch, we will create a merge between the version in the branch and the head version of the main branch. Branching can then be used as in the software industry (see branching ). For sure, some merge strategy will have to be defined to be applicable in an environment where design solutions are composed by 3D models, list of parts (so-called bill of materials or \"BOM\"s) and documents. We already saw the capability of studies that graph data offer: At any moment it is possible to create fake products in which specific components, applicable today, are replaced by: New components, fruit of new studies: Very useful for innovation or to answer to a specific unusual customer requirement; Fake components: Very useful when a digital mock-up has to be produced but without giving the exact details of some areas of the products (to be protected from competition piracy); Old components: To make some analysis in specific old configurations (in which we can apply possible retrofits); Any other kind of situation where we need to create a product that we do not intend, at least at first, to industrialize. Playing with the digital twin The full graph in the PLM represents the digital twin of the product line, product line because all possible products are in it. In a certain way, the PLM of the product line is the global digital twin and contains each S/N which is the digital twin of each product. As the PLM contains a graph of data, it is possible to make any link at whatever level we want, and so add links to data that are related to a certain \"node\". Conclusion We saw, in this article, that there were two ways of doing configuration management in PDM/PLM systems: The dynamic filter-based one, The static graph-based one. We believe the first one is bringing many problems and inefficiencies in the engineering department an much more in the manufacturing and the support departments. The second one however is securing all data, makes things simple and enables concurrent engineering. The migration from the first to the second is a complicated but very interesting task we have been working on (see Madics conference 2019 ). It can bring a lot of value, simplicity and productivity to the industry. It also opens the door to more product architecture activities, more digital integration works, more digital delta tools between several versions of components. The graph data are at the core of the PLM systems. See also The series of articles about PLM: Article 1: PLM and graph data Article 3: The four core functions showing you need a PLM Other connected articles: The real nature of data (July 2021)","title":"Configuration management of industrial products in PDM/PLM"},{"location":"articles/conf-mgt/#configuration-management-of-industrial-products-in-pdmplm","text":"This article is the second one of a series on PLM. Please have a look at PLM and Graph Data article for an introduction. Photo by Martin Kessel from FreeImages Configuration management (CM) is at the heart of the industry, being the traditional industry or the software industry. While the software industry massively invested on this area for decades, reaching a certain level of maturity in which CM is no longer a problem, some areas of the traditional industry still use old CM practices that prevent other digital evolutions to be massively used, such as works at the digital twin level. When CM stays a matter of experts and is not streamlined in the industrial process, inefficiencies, productivity losses and bad quality of data are everywhere. This implies, generally, many product defects and customer dissatisfaction. In this article, we will try to examine the two different ways of performing CM: The dynamic one, where the configuration of a product is the result of a calculation (quite often based on a complex filtering); The static one, where the configuration of a product is benefiting from the graph data that we can find in PLMs (see PLM and Graph Data for more information). Strangely, this topic is not massively discussed on the web while it is at the heart of the industry performance (and non performance), being during the design process, the manufacturing phase of product life cycle or during its support phase.","title":"Configuration management of industrial products in PDM/PLM"},{"location":"articles/conf-mgt/#the-old-way-filtering-the-product-tree","text":"We will explore the first kind of industrial configuration management, the dynamic one, based on the the filtering, more or less elaborated, of the product tree.","title":"The old way: Filtering the product tree"},{"location":"articles/conf-mgt/#the-product-tree","text":"In Part Data Management (PDM) systems, a product is represented by a tree, which leaves are parts or groups of parts (often called \"design solutions\"). The product tree structure is generally the fruit of several inspirations, amongst which we can find: A functional split of the product (quite often coming from the system engineering view of the product); A regulatory view of the product (e.g. a split of an aircraft product per ATA code of the systems in the aerospace world); An organizational view of the product, e.g. mapping the product in the various organizations of the engineering office that design the product (mechanical engineering, electrical engineering, engine, air conditioning, etc.); A mix between technical PDM systems capabilities (sometimes related to CM) and between the previous constraints. The product tree, or \"product structure\", is containing the product with all its options, which is sometimes called \"the 150% vision of the product\". For sure, all options will not be applicable at the same time on a single instance of the product. The big challenges are, in this context: To be able to get the exact list of applicable parts (or applicable design solutions) for a certain instance of the product; To be able to manage the catalog of applicable options at a certain moment to future instances of the product; To be able to manage change in this universe, for instance when some parts become obsolete and must be exchanged by something else. Moreover, the configurations of products that were manufactured or built must be preserved for maintenance. That can mean being able to create maintenance manuals and spare part lists for all products that were, at some point, manufactured and/or built. That would mean keeping track of the individual configuration of some product at some key moments of their life cycle.","title":"The product tree"},{"location":"articles/conf-mgt/#manual-version-management-in-the-product-tree","text":"Generally, old PDM systems don't manage natively the version management. When a leaf of the tree changes, let's suppose it is a design solution, a new version of the design solution must be created manually by copying the previous one and its content and name it with a version number in its name. Let's take an example: The brakes of the bicycle product (see Figure 1) are changing because the brakes provider does not manufacture the old brakes anymore. In the PDM system, we must keep the old brake version because those information can be useful for the support of the already manufactured bicycles. We create a new brakes design solution by copying the old one, renaming it, and changing in it what needs to be changed to take into account the new version of the component. From a certain point in time, from a certain instance of the bicycle product, all brakes will be of the new release V2 . Generally, industrially, we will move from the old version V1 to the new one V2 when the stock of old brakes will be empty. This shows that the changes in product configuration can have quickly industrial impacts. In the PDM system, we end up having, at the bottom of one branch, two versions of the brakes. If the product tree was inspired by a systems engineering approach, we should have a generic \"brakes branch\", corresponding to the \"brake function\", with two possible implementations: One that was used from the first instances of the bicycle product up to instance number N (we will often speak about product \"serial number\" or S/N to identify in a unique way the instances of the product), And the second one that will be used from instance number N+1 and for the future (see Figure 1). Figure 1: Basic applicability in PDM","title":"Manual version management in the product tree"},{"location":"articles/conf-mgt/#configuration-management-using-links","text":"We can provide a quick definition of applicability in this context: For a product with a serial number K , the brakes version 1 are applicable to S/N K if K is inferior or equal to N ; From N+1 , the version 2 of the brake is applicable. In PDM systems, the applicability is traditionally attached to the links as shown in Figure 1. Applicabilities are, in that case, intervals of product S/N: Brakes V1 are applicable to bicycle which S/N is in the range [1, N] , and brakes V2 are applicable to the products which S/N is in the range [N+1, infinity[ . Having defined the applicabilities on all links, it is possible to filter the full product tree with a specific S/N to see what is applicable to this S/N. The filtered tree will correspond to the subset of the product tree corresponding to the specific options that are present in the product which serial number is K . For sure, if we try to filter the product tree with a S/N that does not exist (i.e. in the future), we may retrieve (depending on how the filtering mechanism is implemented) the list of all possible design solutions that are applicable from now . In the case of Figure 1, for sure, the V1 of the brakes will not be applicable anymore for any S/N superior to N . Without any particular definition of our S/N options, we may see more than 100% of a product because we did not defined the very features of that S/N yet, and we may get all available options (product catalog) with this filter.","title":"Configuration management using links"},{"location":"articles/conf-mgt/#the-product-catalog","text":"In order to maintain a catalog of options and to manage options compatibility, we need to have those data store somewhere (see Figure 2). Figure 2: Basic PDM product catalog In some cases, those crucial data will be maintained outside the PDM, in Excel, or in a separate Access database, which will rely on manual actions for the synchronization (source of errors and inconsistencies); in some other times, those data will be maintained inside the PDM system itself. Compatibility tables are an important element of this product catalog. Very frequently, they implement non trivial rules that depend on one another. The Figure 2 shows a sample of that: the basic breaks in V2 are compatible with both basic wheels in V3 and premium wheels in V2 . The product catalog is crucial because it is enabling: To create a vision of the product options for the marketing and sales department (in a certain way to define the common language between marketing and sales and the engineering departments); To attach to a specific S/N of product a list of options; To guarantee that those options are compatible with one another. The product catalog introduces a S/N vision inside the PDM system. From now on, without any product catalog vision, we could live with just \"S/N intervals\" of applicabilities. But if we make the S/N concrete and attached to a specific set of options, we can ask ourselves what is the best way of managing this information inside the system (we will come back on that point).","title":"The product catalog"},{"location":"articles/conf-mgt/#managing-the-changes","text":"For, sure, as shown in Figure 2, components have versions, and we must track the changes between those versions. Change tracking is fundamental because it has an industrial impact, especially on stocks, on procurement, on all the manufacturing process, and on the maintenance process. Change management is the set of required actions that are done to justify the change and master its impacts. In some businesses such as aerospace, the change is also tracked to prove to the authority that the aircraft can \"inherit\" from a past certification because the changes performed on it are just \"small\" ones. In case a change is important, a re-certification is required on a part or all the aircraft type. Basically, during the life cycle of the product, all entities will change and will be versioned, including the compatibility tables for the various options. Change objects are interesting to track precisely why we did a change. In complex manufacturing products, change management ensure that the primary objective of the change is respected: fix something, enhance something else, take into account the impact of obsolescence, etc.","title":"Managing the changes"},{"location":"articles/conf-mgt/#special-case-applicability-management-through-changes","text":"In some areas such as aerospace, the change is considered so important, because of the fact that it is linked to airworthiness certification of flying machines, that the full product is seen through changes - and not through physical components. To explain the problem, we will simplify a bit the situation. A change is an entity that will describe the replacement of one or several design solutions by one or several new ones. If we consider that an original design solution was \"created by a change\" ( Change 1 in Figure 3 is creating the Brakes in V1 ), we can see the product as a \"stack of changes\". Figure 3: Managing applicabilities with changes In Figure 3, we can see several states of the brakes: The Change 1 is creating the design solution Brakes V1 . The Change 2 does a modification to the Brakes V1 . In our example, this situation is often called a \"retrofit\". The design solution Modification to brakes V1 must be envisaged as being applicable on top of Brakes V1 . This situation occurs when, for instance, a part was forgotten in the original Brakes V1 design solution but, for certain reasons, we can't change directly the design solution Brakes V1 . To apply this retrofit, both Change 1 and Change 2 will be applicable, but maybe some products were manufactured with only the Change 1 and so they may have defects. For sure, the manufacturing and the support will have to be aware that Modification to brakes V1 is not a full design solution but some kind of \"delta\" to be applied to the previous version of the component (here Brakes V1 ). The Change 3 replaces the previous one with a full new design solution that is only pointing to Brakes V2 . It is interesting to note that, in this model, we know if a design solution is applicable or not to a product serial number if the change is applicable to the product serial number . The product is no more a set of components: It is a \"set of changes\". We will call this way of proceeding an \"administrative configuration management method\", administrative because the product is hidden behind the administrative iterative change process that leads to define it and to ensure its certification . The main consequence of this kind of configuration management is that a specific S/N of the product cannot be retrieved from the product tree by applying just a filtering on S/N ranges: We must do some kind of complex \"calculation\" to start from the description of the product as a set of changes to find what are the real design solutions that are applicable to this S/N. We can note that this way of working is complexifying the works of everyone, because everyone in the company needs a static vision of the S/N, at least in a certain version: The design office needs to work in a certain product context to perform relevant modifications; The industry needs to know the exact configuration of the S/N in order to build it; The support needs the exact configuration of all S/N to ensure that the maintenance manuals and spare part lists are covering all the manufactured products. The problem of configuration calculation process is that it embeds business rules . An example of that is the retrofit situation that we have just shown in Figure 3. We could have decided never to use this kind of \"delta\" mechanism and only work by a replacement approach. The set of rules that will apply to find back the configuration of a S/N being business rules, they may change with time. This can lead to real problems because with this change in time, it is not guaranteed at all that when we recalculate the configuration of a S/N in the past, we will find again the exact same configuration that we have delivered top the manufacturing team. That is often interpreted, wrongly, as a non reliability of the PDM system that cannot fond back the configurations of the already delivered products.","title":"Special case: Applicability management through changes"},{"location":"articles/conf-mgt/#stacking-the-changes","text":"This model establishes a \"lineage\" between changes that, most of the time, does not enable \"forks\" of design solutions, like shown in Figure 4. Figure 4: Forking a design solution Brakes V3 are clearly a fork of Brakes V1 , so the change should be done between both. But, in a standard \"stack-oriented change system\", the Change 4 will be associated to the next iteration of Brakes V2, which is not the case. This lineage forces to see the product as a continuous set of changes, a pile or a stack of changes. That means that, in case of wrong modifications in the design solutions, it is not possible to get a design solution of the past and to start at this point, which the fork mechanism is all about. It is mandatory to \"undo\" all modifications you did to come back to the previous state. Undoing is generally a manual, costly and error prone process (incredible to people that come from the software industry, where forking is one very basic action in configuration management).","title":"Stacking the changes"},{"location":"articles/conf-mgt/#from-the-pdm-system-to-the-clm-system","text":"This stacking of changes method transforms the PDM system into what we could call a \"CLM\", standing for Change Lifecycle Management, but not into a PLM (Product Lifecycle Management). This statement is very important because, in a certain way, the administrative CM method based on change stacks is proposing an approach that is diverging from the PLM world where the product is at the center and the change on a secondary level. The CLM system, by putting the change first and the product \"behind\" it, closes the door to static graph data usage in PLM. This way of working, as we will see it in the rest of the article, creates cascading problems in all the manufacturing company.","title":"From the PDM system to the CLM system"},{"location":"articles/conf-mgt/#the-limitations-of-filtering-based-cm","text":"Filtering-based CM systems (being simple applicability oriented or change-based applicability oriented) seem to us as being structural sources of problems. Moreover, when investigating their origin, it appears that they are very linked to the PDM system limitations of the 90s. And the fact is PDM systems evolved into PLM systems leveraging the product graph data and enabling a vision of CM closer to what is found in the software industry, a method that we could name \"100% calculated configuration\" or \"static configuration management\". This way of working is the only one to enable works at the digital twin level. Indeed, how can you work on the digital product if the vision you have of the product is the result of a dynamic filtering calculation that will not be persistent enough to enable the creation of the many links you need to do in the graph data?","title":"The limitations of filtering-based CM"},{"location":"articles/conf-mgt/#work-around-isolate-from-filtering-based-configuration","text":"If we try to implement concurrent engineering in an environment where the CM is dynamic, we immediately face problems. Let's suppose we work on component C1 and this component is applicable to our S/N at T0 . At a certain point in time T1 , we discover that the configuration calculation declared C1 not applicable anymore. First of all, we will have to find back what is applicable instead of C1 (let's suppose it is C2 ), which may be not easy, but what would be feasible if we are part of the change board. We probably created some data attached to C1 and the problem will be to adapt those data to the C2 case. Typically, we are in that situation where we created manufacturing instructions or support documentation attached to C1 . We have, in this situation, many chances to be forced to live at the rhythm of the engineering configuration changes. The common work around that we can see is a persistence of configuration in another database as shown in Figure 5. Figure 5: Work around with dynamic CM All intermediate configuration calculation will be stored in a separate area with a time stamp. Every other actor can work on a certain stable version for a while, without being disturbed by the continuous changes in applicability. Then, when a version is ready, it is possible to calculate the deltas between the version I was working on and the last one. Ideally, we can calculate one single delta for many accumulated changes (like the software industry is doing). We can then act on the accumulated changes, analyze the impacts on our linked objects and perform the required changes. The most severe consequence in not having a mechanism of systematic persistence of the dynamic configuration is that the manufacturing and the support are bound to work at the rhythm of the design office, whereas their processes and business objects may require a different lifecycle.","title":"Work around: Isolate from filtering-based configuration"},{"location":"articles/conf-mgt/#the-new-way-linking-static-graph-data","text":"We will now explore the second king of configuration management: the static one based on persistent non mutable graph data.","title":"The new way: Linking static graph data"},{"location":"articles/conf-mgt/#reusing-components-in-the-industry","text":"Those last decades, the vision of the manufactured product changed a lot. If manufacturers were used to build almost all parts of their products, they transformed progressively into assemblers, letting the role of manufacturing subsystems to subcontractors (so-called \"suppliers\"). In a lot of cases, the supplier is not only manufacturing components, but it is also in charge of designing them. The manufacturing industry entered the same constraints as the software industry. In order to build your product, you have to assemble components that you did not manufacture, so you have to master their compatibility with your own products. You must then take conventions with your suppliers about \"interfaces\". To come back to our brakes example, as a bike manufacturer, we buy the brakes from an external supplier that will provide brakes to our company but also to other bicycle manufacturers. We have to define in a contract what will be the interfaces between our bicycle and the brakes. Those interfaces can be mechanical, but also, in the general case, electrical, fluid based, etc. This definition of clear interfaces between components is the condition of component reusability. It also enabled a certain specialization of manufacturers that can perform R D activities on their very domain and be innovative. This also created a component market that can imply large worldwide supply chains. Being in machines, in automotive or in aerospace, the assemblers must control the configuration of the components they use. Like in software, during the lifecycle of a product, there will be many versions of the same component available for series production. For each of them, every actor will need information: In order to integrate it in the product tree (and possibly into the product 3D mock-up); In order to assemble it to the other parts of the product; In order to support it in the long run.","title":"Reusing components in the industry"},{"location":"articles/conf-mgt/#the-site-specialization","text":"Even if the assembler is mainly assembling components that are not manufactured internally, many manufacturers tend to specialize sites in order to implement the same component-oriented industrial strategy. Big manufacturers have the same constraints than their suppliers and, if they want to keep an expertise on some areas, it is better that they concentrate this expertise on a set of physical locations.","title":"The site specialization"},{"location":"articles/conf-mgt/#variants-and-customizations","text":"Consequently, the challenge of modern manufacturing is to manage the product as a set of components, each of them having its own lifecycle, and most of them being more or less mastered by suppliers. This model has many advantages: It is easier to create product variants, or product families, product lines, or different brands, considering that many components can be reused from one model to another; The component reusability enables better innovation on each component level; The industrial process is divided between component assembly process and final assembly processes, which implies a good performance of the logistics to be able to transport components to the different assembly lines. The main drawback of the model is to enter a cycle of incremental component improvements without any product redesign. Indeed, redesigning a product is a much bigger task that can change the scope of the various components. When a supply chain is optimized with many internal and external components, the global innovation may have a tendency to decrease. In this context, new players can enter the industry with more effective approaches.","title":"Variants and customizations"},{"location":"articles/conf-mgt/#consequences-for-the-product-tree","text":"The first consequence of this industrial reality is that the configuration must be managed at different levels: At the component level, the configuration must support variants and options; At the product level, the configuration must support which components are applicable. For some big manufacturing products, such as cars, ships or aircrafts, we can have intermediate levels, sort of \"big components\" that aggregate smaller components. The configuration of the product becomes a multi-level configuration management system. The product tree representation should materialize this tree of components like shown in Figure 6. Figure 6: Hierarchy of components We see that the product is composed of several layers of components. By seeing the diagram, we could analyze the various levels: The blue level is the design solution level. In that sample, that is the only layer the manufacturer of the product P really masters in terms of detailed design. The yellow level is the level of \"Small Assemblies\" (prefixed SA ). Only the SA2 is mastered internally. All other small assembly are coming from suppliers. The purple layer is the layer of \"Big Assemblies\" (prefixed BA ). BA1 and BA2 are mastered by the manufacturing company because they are in charge of assembling the small assemblies that composed them. But BA3 is directly coming from a supplier. We can see that the way of organizing the data implies several things: The split of the product in components generally comes from the early phases of the design, especially from the systems engineering phase, that created a product architecture enabling to define the various components and their interfaces. It is required to frequently test the digital fit of the digital components together before trying to assemble them physically. We can call this process the digital integration of components at the \"digital twin\" level. Note that this process can be quite complicated because it can imply the verification of many interfaces on many levels. Product architecture and digital integration are at the core of modern manufacturing. We can note those two activities are at the core of the software industry for decades, and that the manufacturing area converges progressively to the software industry practices, even if some constraints (at the interface level) are more complex in the industry.","title":"Consequences for the product tree"},{"location":"articles/conf-mgt/#adding-versions-to-the-product-tree","text":"If we add the versions of the various components in the product tree, we obtain Figure 7. Figure 7: Hierarchy of components with versions In this figure is shown the lifecycle of some components and their links together. Let's start by analyzing DS1 . This design solution has many versions. Those versions are linked together by a change having a unique ID (e.g. Change 1a2c3 is tagging the link between DS2 in v1.0 and DS2 in v1.1 . In the schema, the chosen convention is that any change on version on a lower component implies a change on the upper one. DS2 in v1.1 is used by SA2 in v1.18 and v2.0 is used by SA2 in v12.0 and in v12.1 . The structural dependencies are shown with black arrows. The head of the product P is pointing to all heads (last versions) of all components. For sure, it is not because the product head is pointing to those components that they are applicable to every instance of the product.","title":"Adding versions to the product tree"},{"location":"articles/conf-mgt/#materializing-the-sn-in-the-product-tree","text":"At the top of the diagram, we have an instance of product P with S/N 1212 using the BA1 and BA2 respectively in v25.4 and v56.6 . Each of the big assemblies points to the small assemblies that are in it, and those small assemblies point to the design solution they are composed of. The first important thing to note is that there are no more calculation mechanism: The S/N 1212 in version v234.3 is pointing to existing versions of big assemblies that are fully defined. If an upgrade would be done at whatever level on the tree (in Figure 7, at the design solution level, the small or big assembly level), the version management would enable to create a new version of product P S/N 1212 to point to those specific versions. Note that when a version is \"wired\" with links, it will never change in the future , and ten years later, this past part of the graph will be exactly the same. This way of creating graph data is another way (compared to the filtering approach), much more efficient , to manage applicabilities, configuration and changes.","title":"Materializing the S/N in the product tree"},{"location":"articles/conf-mgt/#applicability","text":"The notion of applicability, as we saw it in the first part of the article, is twofold: Indicate that a specific product S/N is using a specific component; Indicate that, for future S/N, from a certain S/N, the component to be used should be a certain component in a certain version (something called sometimes \"effectivity\"). Looking at the tree of versions, for instance for DS2 , we can see that we can decorrelate the versions of DS2 and the versions of SA2 : When SA2 was only upgraded once, the DS2 design solution had 2 intermediate versions that were not used to collaborate to the small assembly ( v1.2 and v1.7 ). In a graph mode, the applicability can be seen under 2 angles: Real usage: A certain component really uses some other component in a certain version. The version used is de facto applicable to the component that aggregates the sub-component. Future applicability: It is defined by the set of components that can be used in the future. In a certain way, this is an open choice because all releases of all components are for ever available in the system. It can also be linked to an explicit feature available in the product catalog (see below). In Figure 7, if we suppose that the components that we can produce are linked to the last one, we will be able to produce products using BA1 in version v25.4 , BA2 in version v56.6 and BA3 in version v4.12 , and all their graph of dependencies. We can note also, in Figure 7, that a specific \"study S/N\" XXX01 points to a fork of a design solution located in the past ( DS2 in version v1.2 study A ). That enables to potentially come back to a design solution that could be better in the past and to start from it.","title":"Applicability"},{"location":"articles/conf-mgt/#configuration","text":"If the dynamic filtering model was requiring a more or less complex algorithm to find the real S/N, the Figure 7 shows that the S/N (in a certain version) is the head of a tree of components that is defined down to the level that is relevant for the proper industry. And, each time a component is changing of release, an impact study can be led to see if this change should also impact the S/N that must be produced. The graph data approach enables, in that case, that all versions of the S/N configuration are secured forever inside the system. By the way, we can see here that the system can be called a \"Product Lifecycle Management (PLM)\" system because it contains the full history of the product line, of the product components (whatever the level) and of all the serial numbers. In a certain way, all configurations are there, are \"solved\", are calculated\" (because there are no more calculation). The fact that all versions of the configuration are secured inside a PLM is critical for many processes, especially the concurrent engineering ones. Let's take an example. Let us suppose the engineering sent the version v12.0 of SA2 to the manufacturing preparation. Some people will work on the assembling instructions of this version of the small assembly pointing to specific versions of design solution DS1 and DS2 . Meanwhile, the engineering department worked in the version v2.1 of the DS2 . Everyone can work in parallel without having any conflict. That is a much better situation than the one we saw previously.","title":"Configuration"},{"location":"articles/conf-mgt/#change-management","text":"In the graph model, changes will tag the various links between two versions of the same component. The fact that the change is restricted to the same component is very important because it is quite different from a model where a change can aggregate a \"before\" status (composed by several components) and an \"after\" status (composed also by several components). This kid of change does not exist anymore and is replaced potentially by a change at an assembly level. The next version of an assembly can embed different components that the previous version of the same assembly. In this model, the change object is limited to change. For a certain product instance (serial number), finding its delta of changes compared to a certain version in the history (for instance a certified baseline) will consist in aggregating all the changes that led to all versions of all its sub-components. In Figure 7, the component SA2 changes version from v11.8 to v12.0 while skipping two versions of DS2 . In a certain way, the change 3df59 corresponds to the union of changes 12bb5 , aze129 and 1a44ho (because also v11.8 and v12.0 of SA2 are using the same version of DS1 ). In order to be able to commit, at the SA2 level, the change, specific digital tools enabling to compare directly version v2.0 and v1.1 of DS1 are needed. Contrary to the change management we saw in the first part of the article, each real state of each component is secured and there is no need to create tricks to recompose the true component or design solution. Using graph data in PLM enables to put the change behind the product and to enter the world of PLM, rather to keep working in a CLM world. This model should be seen as the real product-oriented model. There are two major differences between this model and the filtering tree model: All configurations are solved and explicit in the graph model; To get the set of changes between a product S/N and a baseline in the past, we have to perform some graph analysis, comparing the graph of the S/N and the graph of the baseline and gathering all changes that enabled to go from the one to the other. In one word, the product comes first and the change comes second. A system with this kind of management can be called a product lifecycle management system.","title":"Change management"},{"location":"articles/conf-mgt/#product-catalog","text":"An interesting thing to note is that is a \"solved graph\" containing all design solutions (or parts) in all versions, all assemblies, whatever their size, all serial numbers, it is very easy to create a product catalog (see Figure 7). Indeed, the product catalog is: The head of all the applicable elements at a certain point in time, with the desired level of granularity; The compatibility and incompatibility tables between the various options. We can also see that, if we put in place, inside the PLM, a process of choosing the baseline (the major set of components that is applicable to all products, BA1 in Figure 7) and all the options, the product instance definition can create a S/N as a new head in the graph pointing the right versions of all components (like shown in Figure 7). In that model, there is no calculation: The S/N, in its first version, is the direct fruit of the product catalog. It is directly usable by the manufacturing and the support. In a certain way, we have defined a direct and non ambiguous link between marketing and sales, engineering, manufacturing and support.","title":"Product catalog"},{"location":"articles/conf-mgt/#more-plm-features","text":"Many features that are considered as basic and standard by the software industry would be of great interest in the industries that are not using PLM (or using PLM with a filtering configuration approach). For instance, the fixes on some components (retrofits) can be solved by forking a component (like DS2 in version v1.2 study A ) to fix it in a parallel branch. In order to get back the fix on the main branch, we will create a merge between the version in the branch and the head version of the main branch. Branching can then be used as in the software industry (see branching ). For sure, some merge strategy will have to be defined to be applicable in an environment where design solutions are composed by 3D models, list of parts (so-called bill of materials or \"BOM\"s) and documents. We already saw the capability of studies that graph data offer: At any moment it is possible to create fake products in which specific components, applicable today, are replaced by: New components, fruit of new studies: Very useful for innovation or to answer to a specific unusual customer requirement; Fake components: Very useful when a digital mock-up has to be produced but without giving the exact details of some areas of the products (to be protected from competition piracy); Old components: To make some analysis in specific old configurations (in which we can apply possible retrofits); Any other kind of situation where we need to create a product that we do not intend, at least at first, to industrialize.","title":"More PLM features"},{"location":"articles/conf-mgt/#playing-with-the-digital-twin","text":"The full graph in the PLM represents the digital twin of the product line, product line because all possible products are in it. In a certain way, the PLM of the product line is the global digital twin and contains each S/N which is the digital twin of each product. As the PLM contains a graph of data, it is possible to make any link at whatever level we want, and so add links to data that are related to a certain \"node\".","title":"Playing with the digital twin"},{"location":"articles/conf-mgt/#conclusion","text":"We saw, in this article, that there were two ways of doing configuration management in PDM/PLM systems: The dynamic filter-based one, The static graph-based one. We believe the first one is bringing many problems and inefficiencies in the engineering department an much more in the manufacturing and the support departments. The second one however is securing all data, makes things simple and enables concurrent engineering. The migration from the first to the second is a complicated but very interesting task we have been working on (see Madics conference 2019 ). It can bring a lot of value, simplicity and productivity to the industry. It also opens the door to more product architecture activities, more digital integration works, more digital delta tools between several versions of components. The graph data are at the core of the PLM systems.","title":"Conclusion"},{"location":"articles/conf-mgt/#see-also","text":"The series of articles about PLM: Article 1: PLM and graph data Article 3: The four core functions showing you need a PLM Other connected articles: The real nature of data (July 2021)","title":"See also"},{"location":"articles/data-interop/","text":"The real nature of data \"Data is the new gold\". \"Data drives the world\". \"We are in a data driven world\". All medias, and not especially the IT medias, seem obsessed with data, especially big data, and the value they are representing or should represent. For decades now, we are running on data. Big data is just another step in a long-term trend that is just beginning. As always, the marketing speeches around big data are over-selling promises as they did in the past with relational databases, datawarehouses, datamarts, etc.). This article explains that data is not a simple absolute usable IT \"product\" that can be used, in all cases, as a source material . On the contrary, it is an elaborated product, fruit of a certain set of use cases and technical constraints, which makes its usability complex in many cases, which explains why some concepts like datalakes are not working in any case. Describing the business Software development is all about representation One of the main objectives of IT is to automate the business processes and enable a better business management. This is achieved by storing the business concepts and manage the business rules associated to them and that control their life cycle. In order to do a good job, the IT people must create a working representation of the business, both static and dynamic to be able to create the software applications that will automate the business. Application development is, consequently, all about representating . IT representations are like all representations: They are relative and changing. This fact has many consequences on data, as we will see. Business concepts and business processes In every business domain, we can exhibit business concepts. Those business concepts are linked together with various kinds of relationships. They establish a first representation of the business. Business processes are manipulating those business concepts, creating them, changing their state, linking or unlinking them together. Business processes establish the second dimension of the representation of business (to simplify, we will consider that business rules are part of the business processes). From this double representation, software designers can do many things: Create applications that manipulate those concepts and the concepts life cycles; Create databases that store those concepts; Create business workflows that will be a way to assist the business processes with software. For each business, we can define the set of all business processes and business concepts of the business domain as the business semantic space . This set is a theoretical concept that contains all possible IT representations of a particular business . Every software attached to a business domain is covering a part of the related business semantic space. Enriching the representations With time and business digital maturity, the business concepts and business processes are enriched. This phenomenon is a big constant of the IT world. Let us take an example: accounting. In the 80s, every company that had the financial means had their own accounting system, developed by themselves. The legislation was, often, not very detailed and each company had interpreted it in slightly different ways. With time, the legislation became more detailed and companies found it hard to implement the concepts and processes. Software companies provided generic accounting software that were taking in charge all concepts and processes of the accounting business and that could be parameterized to the special case of each company. Figure 1 : The evolution of the business concepts and processes With time and maturity, the considered business discovers progressively how to structure its business processes and business concepts: it is shaping its business by creating the most adequate representations at a certain point in time. That activity is called modeling . The Fig. 1 tries to represent this progressive representation enrichment in the case of a regulated business. At the beginning, both company 1 and company 2 have implemented their interpretation of the regulation: They both cover it (and more), but in different ways. At this stage, one company can have a competitive advantage on the other, if its interpretation is having better business impact. Then, the regulation expands (step 2 in Fig. 1): Company 1 covers it but company 2 has a hard time to do it. After a certain time, companies 1 and 2 will use an off-the-shelf software to cover the regulation (step n). Company 2 will still have a specific application connected to the off-the-shelf software. In that case, company 2 may have a better digitization of its business domain than company 1 by covering more of the business semantic space with a specific software. This maturity cycle is a crucial dynamic process in IT, and every IT architect should be aware of it. There are several signs that enable to to estimate the maturity of a business in terms of knowledge representation (business concepts and business processes). The starting point: A paper/Excel-based process As long as the business concepts are not known for what they really are, the business is not represented (at least from an IT standpoint): it is not conceptualized. At this moment, every user can get satisfied with generic multi-purpose tools like Excel. In that case, the business knowledge can be: Located in procedures, written in documents, and that everyone has to follow (but with no way to control the real application of those procedures); People-driven: everyone is doing the business as he wants, provided the end result is obtained (which is causing recurring troubles to work in teams). This kind of processes is both dangerous for the company because it relies on individuals and can be lost when knowledgeable people are no more in charge, and not scalable, so not able to run with more business activity. The business teams need a software to manage their business: it will be the first generation of software. This generation will embed the first IT representation of the business (business concepts and business processes). It will address the first business use cases of identified business roles. As soon as the business concepts are represented, they have a structure, they generally have fields of a certain type, they have links with one another and it is no more possible to manage them with Excel. The evolution of the business representation Progressively, the business have a better understanding of its business, and some concepts will become more detailed and new concepts will appear (or just will be \"named\"). In those cases, the business representation will evolve: instead of having one business concept, the business team realizes that 2 concepts are indeed hidden in the original one; or the reverse, that 2 different concepts are, at last, the same. This evolution enables to make business processes evolve and, quite often, gain in productivity. For sure, we have to insist: the new representation is different from the previous one, generally covering more space in the business semantic space. But it is still relative to the use cases and the roles taken in charge. It is still a representation, and a representation is not absolute. This phase is interesting because it can also exhibits a new kind of concept: management concepts , like \"task\", \"folder\", \"affair\", more generic concepts that could be used in another business domain, and that are used to manage things. Next generation software The next generation of software is generally built on both the experience of the previous one and on the new ways of working of the domain (for instance imposed by the regulation). Its perspective is often larger and encompasses a larger set of use cases. It becomes necessary when the previous system shows limitations in terms of evolutions. Indeed, in software, the way of representing the business concepts and business processes is largely sub-optimal (see our works on the graph-oriented programming ), which implies the necessity to rewrite the business application on a larger set of hypothesis, to keep on gaining in efficiency and productivity. The representation can be deeply modified, being on a business process level and on a business concept level. That means that: There is a need for change management to adapt the processes; There is a need for data migration to transform the old concepts into the new ones; There is a need to reconsider the interfaces between the system that evolves and its ecosystem. All mature businesses went to several generations of software. Let us take back the accounting sample. Many companies are now running their 3rd or 4th generation of accounting software. We can consider that the business is quite mature, and for sure, if we see the large functional scope of modern accounting software, we see that most concepts, along with the regulation, are quite mature. Connecting business domains An important sign of business maturity is the presence in the business of a multi-company standard exchange format , like the IATA transactions in the travel business, the Swift system in the banking area or the Step and ASD standards of the aerospace industry. Those standards are generally the fruits of impressive collective efforts to provide a multi-company shared vision of one or several business domains, in terms of business processes and in terms of business concepts. The presence of exchange standards are very important, not only because they define common business concepts and business processes, but because they define a frontier between the different business semantic spaces. Figure 2 : Connecting several semantic spaces The Fig. 2 is representing this situation of 2 business semantic spaces exchange concepts through an exchange format. Note that Fig. 2 represents 2 applications, application 1 and 2 respectively in the semantic spaces A and B. For them, for sure, the exchange format will be a subset of the concepts and processes they manipulate. Indeed, even if the business semantic space is hard to define in terms of exact scope, collective efforts to define interchange formats lead to the conclusion that several business: Are different; Need to share some concepts and be part of a larger process. These 2 very basic statements should not be taken lightly, because they have deep consequences on the shape of global IT systems. Indeed, there are several business domains in the world, each of them having their own concepts and processes. They sometimes have to be part of a larger process and have to share concepts in that purpose. But being able to exchange concepts does not mean that they are in the same semantic space. The optimal frontiers between different semantic spaces are the root problem behind an efficient set of IT systems, a global company performance, and an optimal business function placement (in French: \"urbanisation\"). Optimal business function placement is one of the most important role of the enterprise architect. If the systems are respecting the business semantic spaces, the company will be efficient, and every business will be able to evolve independently from the others provided the exchange formats are respected. If the business functions of various business semantic spaces are mixed together, there will be a huge confusion in processes and responsibilities and it will be very complicated for every business to evolve without impact the full company. Because, with time, business representations are changing, while staying in the same business semantic spaces, businesses deserve to have the capability to mature without being too tight to other businesses, even if they share a large number of business concepts (but not represented the same way). That is why exchange standards are so important: because they define the perimeter of the various business semantic spaces and enable to build large processes among them; and because this representation is the fruit of a collective effort of experts in the particular business domains. The real nature of data A attempt to define \"data\" As we explain, business concepts are representations. They can have many representations in the same semantic space. We can define \"data\" as follows: Data are projections of the business concepts on technical constraints . Figure 3 : Data are viewpoints on business concepts of the semantic space This simple representation of Fig. 3 is trying to put in focus several points: An application is defining a viewpoint on the business semantic space; This viewpoint is not absolute, it is a representation of business processes and business concepts that is the fruit of business roles and business use cases; Data are a projection of the business concepts in the space of technical constraints; This projection is a transformation of the business concepts; It is not absolute, and it depends upon the programming languages, the database technology, the performance requirements, etc. In other words, data are a technical transformation of a business representation of a business concept. The relativity of data As we saw in the Connecting domains section and in Fig. 2 , business domains can communicate together with the help of exchange formats. Let us examine this process at the data level by considering Fig. 4. Figure 4 : Data models are relative At the data level we can see 3 different representations of the same interchange concepts: The are 4 business objects in the data model of application 1; There are only 3 business objects in the data model of application 2; In the exchange format, there is no explicit mention of the links between A , B , C and D , but a grouping of those business objects and a cardinality grammar. Those 3 different representations of data are semantically equivalent , but they are different representations of the same reality. Two more comments can be added to that description: The data model we see for application 1 and application 2 is just a subset of their complete data model that will manipulate concepts that are not known from the other application (because they are in 2 different business domains); The interchange format is transported in a service way, within the concept of a verb (see our article on web services here and here . Aggregating data Let us suppose, taking the sample of Fig. 4 , that the application 1 publishes data to application 2. If we were to aggregate data from application 1 and application 2, we would be facing a problem: what data should we consider? It is not an easy problem. The application 1 offers more details in modeling A and C as different entities, whereas application 2 is using AunionC instead. But in terms of sequence, if data in application 2 is fresher than data in application 1 (because it is after in the flow), should not we take data in application 2 instead of data in application 1? But who is really the master in those data? And what about we need data connected to C ? Is it equivalent to connect those data to AunionC ? Figure 5 : What data should be in the data lake? Fig. 5 shows the problem graphically. It is possible to solve this issue on two conditions: To explicitly determine what we intend to do by aggregating the data from those two applications; What is the point in doing that? What treatments we want to implement? For what functional purpose? To have a minimum of functional and technical knowledge about applications 1 and 2, why are they communicating together, to what purpose, at what moment, etc. Aggregating data is not an end, it is a solution answering to a business question. So what is the business question? About data lakes The data lake for real use cases In some business, the interest of manipulating a huge amount of data is obvious: In Internet retail for instance: for big websites, you have a massive amount of data for each customer visiting your website with all information related to the consultations and to the purchases; Those data are being largely used for years; In the industry: Most production assembly lines generate a lot of data that are used to monitor the quality of the product and the productivity; In public services: Analyzing in details at the country level the various business trends, employment data and so on, can be done now from the real low level real data instead of using representative samples; Etc. What is common between all those samples is that the use cases of those big data are very clear and the big data technologies are just a way to answer to them. In a certain way, we are just talking about classical software applications using big data technology to store many more data than classical applications used to do. The data lake for no use cases The fact is most data lakes projects are not focusing on real use cases: they do the reverse. They are infrastructure projects to enable future use cases . They are called enablers which is a nice word to say: \"let's do a technical project and we'll see afterwards if it can be used on real use cases\" (1) . Big data vendors are so powerful (as datawarehouse vendors decades ago) and so trendy, that they convince enterprises to put in place big data infrastructure prior to having real use cases. The speech is seductive: In order to realize whatever use case you will want to implement in the future, your company needs to put all company data in a data lake; This will enable you to correlate all data you want and find value in your sleeping data. This speech has the advantage of seducing end users that understand the concept, and IT people that are thrilled by the introduction of new technology (2) . For sure, the fact that data lakes with real use cases work are confusing the situation. For data lakes like for other topics, providing technology without a real use case is a very risky operation, that generally ends badly: Millions invested for nothing. ure out what system is master at what point in time. Data lake as an application Indeed, working data lakes are applications (with big data). But, if this reasoning is true, that would mean that a data lake is an application rather than a data store , that it has some data ingestion business logic to be able to ingest data that fit in a global data landscape without introducing ambiguities or erroneous data links. But, if that is the case, that means that the data lake is also creating its own viewpoint on the semantic space, like shown if Fig. 6, which most of the time is built on the pivot models we find in exchange formats. Figure 6 : The data lake as another kind of application Semantic technology applied to data lakes The most recent flavor of the ever lasting concept of data warehouse is the semantic data lake . The idea can be summarized as follows: with the proper ontological description (ontologies being Semantic Web way of describing concepts), it is possible to aggregate all sorts of data and link them together. Fig. 4 is showing that, for the same concepts, we have 3 ways of representing it. The ontology approach will unfortunately not change this fact. For each representation, we can define a specific ontology that will not be the same and that will show knowledge representation choices (or modeling choices). For sure, we could define graph transformations that would make those 3 ontologies equivalent: but they are not the same. So Semantic Web technologies will not help in defining the \"data lake for no use cases\". As we saw, the modeling formalism is not important because business concepts are representations, and so are relative to the particular point of view of the application (roles and use cases). Conclusion The big data technologies are impressive. They come from the Internet age and from the solutions Cloud companies found to manage volumes of data that are much bigger than anything the humanity saw in the 20th century. Those technologies can be very useful for identified use cases, such as the industrial use cases (to better the product design, to monitor the product product, to manage the in-service life of products) or the Internet retail. But those technologies should not be envisaged outside of clearly defined use cases, first of all because all big data technologies cannot answer to all big data problems (e.g. large time series oriented data processing), and second of all because data is a relative object, quite often product of complex upstream processes. Furthermore, big data will always be expensive, and as all expensive technologies, the business case of using it should be carefully analyzed before investing. See Also Considerations About Rest And Web Services About GraphQL Notes (1) - The modern vocabulary of IT is a real regression compared to UML for instance. By talking about \"features\" and \"enablers\", it opens the door to develop \"features\" for unknown business roles and business processes, and to put in place \"enablers\", so technical infrastructure for the day when people will need it. This semantic shift is the result of a large-scale technical marketing of the large Cloud companies: instead of thinking about enhancing the business, most IT people and more and more business people think about technology. Back to text . (2) - Most IT people find it hard to enter into the knowledge of the business. They prefer working on technological assets. That is why, it is an important role of the IT management to continuously refocus IT people to answer business requirements or to help businesses use wisely the IT resources to solve their business problems. Without this continuous refocus, the gap between business and IT people is growing, opening the door to \"IT done by the business\" syndrome, sign of that impossibility to communicate. Back to text . ( February 2020 )","title":"The real nature of data"},{"location":"articles/data-interop/#the-real-nature-of-data","text":"\"Data is the new gold\". \"Data drives the world\". \"We are in a data driven world\". All medias, and not especially the IT medias, seem obsessed with data, especially big data, and the value they are representing or should represent. For decades now, we are running on data. Big data is just another step in a long-term trend that is just beginning. As always, the marketing speeches around big data are over-selling promises as they did in the past with relational databases, datawarehouses, datamarts, etc.). This article explains that data is not a simple absolute usable IT \"product\" that can be used, in all cases, as a source material . On the contrary, it is an elaborated product, fruit of a certain set of use cases and technical constraints, which makes its usability complex in many cases, which explains why some concepts like datalakes are not working in any case.","title":"The real nature of data"},{"location":"articles/data-interop/#describing-the-business","text":"","title":"Describing the business"},{"location":"articles/data-interop/#software-development-is-all-about-representation","text":"One of the main objectives of IT is to automate the business processes and enable a better business management. This is achieved by storing the business concepts and manage the business rules associated to them and that control their life cycle. In order to do a good job, the IT people must create a working representation of the business, both static and dynamic to be able to create the software applications that will automate the business. Application development is, consequently, all about representating . IT representations are like all representations: They are relative and changing. This fact has many consequences on data, as we will see.","title":"Software development is all about representation"},{"location":"articles/data-interop/#business-concepts-and-business-processes","text":"In every business domain, we can exhibit business concepts. Those business concepts are linked together with various kinds of relationships. They establish a first representation of the business. Business processes are manipulating those business concepts, creating them, changing their state, linking or unlinking them together. Business processes establish the second dimension of the representation of business (to simplify, we will consider that business rules are part of the business processes). From this double representation, software designers can do many things: Create applications that manipulate those concepts and the concepts life cycles; Create databases that store those concepts; Create business workflows that will be a way to assist the business processes with software. For each business, we can define the set of all business processes and business concepts of the business domain as the business semantic space . This set is a theoretical concept that contains all possible IT representations of a particular business . Every software attached to a business domain is covering a part of the related business semantic space.","title":"Business concepts and business processes"},{"location":"articles/data-interop/#enriching-the-representations","text":"With time and business digital maturity, the business concepts and business processes are enriched. This phenomenon is a big constant of the IT world. Let us take an example: accounting. In the 80s, every company that had the financial means had their own accounting system, developed by themselves. The legislation was, often, not very detailed and each company had interpreted it in slightly different ways. With time, the legislation became more detailed and companies found it hard to implement the concepts and processes. Software companies provided generic accounting software that were taking in charge all concepts and processes of the accounting business and that could be parameterized to the special case of each company. Figure 1 : The evolution of the business concepts and processes With time and maturity, the considered business discovers progressively how to structure its business processes and business concepts: it is shaping its business by creating the most adequate representations at a certain point in time. That activity is called modeling . The Fig. 1 tries to represent this progressive representation enrichment in the case of a regulated business. At the beginning, both company 1 and company 2 have implemented their interpretation of the regulation: They both cover it (and more), but in different ways. At this stage, one company can have a competitive advantage on the other, if its interpretation is having better business impact. Then, the regulation expands (step 2 in Fig. 1): Company 1 covers it but company 2 has a hard time to do it. After a certain time, companies 1 and 2 will use an off-the-shelf software to cover the regulation (step n). Company 2 will still have a specific application connected to the off-the-shelf software. In that case, company 2 may have a better digitization of its business domain than company 1 by covering more of the business semantic space with a specific software. This maturity cycle is a crucial dynamic process in IT, and every IT architect should be aware of it. There are several signs that enable to to estimate the maturity of a business in terms of knowledge representation (business concepts and business processes).","title":"Enriching the representations"},{"location":"articles/data-interop/#the-starting-point-a-paperexcel-based-process","text":"As long as the business concepts are not known for what they really are, the business is not represented (at least from an IT standpoint): it is not conceptualized. At this moment, every user can get satisfied with generic multi-purpose tools like Excel. In that case, the business knowledge can be: Located in procedures, written in documents, and that everyone has to follow (but with no way to control the real application of those procedures); People-driven: everyone is doing the business as he wants, provided the end result is obtained (which is causing recurring troubles to work in teams). This kind of processes is both dangerous for the company because it relies on individuals and can be lost when knowledgeable people are no more in charge, and not scalable, so not able to run with more business activity. The business teams need a software to manage their business: it will be the first generation of software. This generation will embed the first IT representation of the business (business concepts and business processes). It will address the first business use cases of identified business roles. As soon as the business concepts are represented, they have a structure, they generally have fields of a certain type, they have links with one another and it is no more possible to manage them with Excel.","title":"The starting point: A paper/Excel-based process"},{"location":"articles/data-interop/#the-evolution-of-the-business-representation","text":"Progressively, the business have a better understanding of its business, and some concepts will become more detailed and new concepts will appear (or just will be \"named\"). In those cases, the business representation will evolve: instead of having one business concept, the business team realizes that 2 concepts are indeed hidden in the original one; or the reverse, that 2 different concepts are, at last, the same. This evolution enables to make business processes evolve and, quite often, gain in productivity. For sure, we have to insist: the new representation is different from the previous one, generally covering more space in the business semantic space. But it is still relative to the use cases and the roles taken in charge. It is still a representation, and a representation is not absolute. This phase is interesting because it can also exhibits a new kind of concept: management concepts , like \"task\", \"folder\", \"affair\", more generic concepts that could be used in another business domain, and that are used to manage things.","title":"The evolution of the business representation"},{"location":"articles/data-interop/#next-generation-software","text":"The next generation of software is generally built on both the experience of the previous one and on the new ways of working of the domain (for instance imposed by the regulation). Its perspective is often larger and encompasses a larger set of use cases. It becomes necessary when the previous system shows limitations in terms of evolutions. Indeed, in software, the way of representing the business concepts and business processes is largely sub-optimal (see our works on the graph-oriented programming ), which implies the necessity to rewrite the business application on a larger set of hypothesis, to keep on gaining in efficiency and productivity. The representation can be deeply modified, being on a business process level and on a business concept level. That means that: There is a need for change management to adapt the processes; There is a need for data migration to transform the old concepts into the new ones; There is a need to reconsider the interfaces between the system that evolves and its ecosystem. All mature businesses went to several generations of software. Let us take back the accounting sample. Many companies are now running their 3rd or 4th generation of accounting software. We can consider that the business is quite mature, and for sure, if we see the large functional scope of modern accounting software, we see that most concepts, along with the regulation, are quite mature.","title":"Next generation software"},{"location":"articles/data-interop/#connecting-business-domains","text":"An important sign of business maturity is the presence in the business of a multi-company standard exchange format , like the IATA transactions in the travel business, the Swift system in the banking area or the Step and ASD standards of the aerospace industry. Those standards are generally the fruits of impressive collective efforts to provide a multi-company shared vision of one or several business domains, in terms of business processes and in terms of business concepts. The presence of exchange standards are very important, not only because they define common business concepts and business processes, but because they define a frontier between the different business semantic spaces. Figure 2 : Connecting several semantic spaces The Fig. 2 is representing this situation of 2 business semantic spaces exchange concepts through an exchange format. Note that Fig. 2 represents 2 applications, application 1 and 2 respectively in the semantic spaces A and B. For them, for sure, the exchange format will be a subset of the concepts and processes they manipulate. Indeed, even if the business semantic space is hard to define in terms of exact scope, collective efforts to define interchange formats lead to the conclusion that several business: Are different; Need to share some concepts and be part of a larger process. These 2 very basic statements should not be taken lightly, because they have deep consequences on the shape of global IT systems. Indeed, there are several business domains in the world, each of them having their own concepts and processes. They sometimes have to be part of a larger process and have to share concepts in that purpose. But being able to exchange concepts does not mean that they are in the same semantic space. The optimal frontiers between different semantic spaces are the root problem behind an efficient set of IT systems, a global company performance, and an optimal business function placement (in French: \"urbanisation\"). Optimal business function placement is one of the most important role of the enterprise architect. If the systems are respecting the business semantic spaces, the company will be efficient, and every business will be able to evolve independently from the others provided the exchange formats are respected. If the business functions of various business semantic spaces are mixed together, there will be a huge confusion in processes and responsibilities and it will be very complicated for every business to evolve without impact the full company. Because, with time, business representations are changing, while staying in the same business semantic spaces, businesses deserve to have the capability to mature without being too tight to other businesses, even if they share a large number of business concepts (but not represented the same way). That is why exchange standards are so important: because they define the perimeter of the various business semantic spaces and enable to build large processes among them; and because this representation is the fruit of a collective effort of experts in the particular business domains.","title":"Connecting business domains"},{"location":"articles/data-interop/#the-real-nature-of-data_1","text":"","title":"The real nature of data"},{"location":"articles/data-interop/#a-attempt-to-define-data","text":"As we explain, business concepts are representations. They can have many representations in the same semantic space. We can define \"data\" as follows: Data are projections of the business concepts on technical constraints . Figure 3 : Data are viewpoints on business concepts of the semantic space This simple representation of Fig. 3 is trying to put in focus several points: An application is defining a viewpoint on the business semantic space; This viewpoint is not absolute, it is a representation of business processes and business concepts that is the fruit of business roles and business use cases; Data are a projection of the business concepts in the space of technical constraints; This projection is a transformation of the business concepts; It is not absolute, and it depends upon the programming languages, the database technology, the performance requirements, etc. In other words, data are a technical transformation of a business representation of a business concept.","title":"A attempt to define \"data\""},{"location":"articles/data-interop/#the-relativity-of-data","text":"As we saw in the Connecting domains section and in Fig. 2 , business domains can communicate together with the help of exchange formats. Let us examine this process at the data level by considering Fig. 4. Figure 4 : Data models are relative At the data level we can see 3 different representations of the same interchange concepts: The are 4 business objects in the data model of application 1; There are only 3 business objects in the data model of application 2; In the exchange format, there is no explicit mention of the links between A , B , C and D , but a grouping of those business objects and a cardinality grammar. Those 3 different representations of data are semantically equivalent , but they are different representations of the same reality. Two more comments can be added to that description: The data model we see for application 1 and application 2 is just a subset of their complete data model that will manipulate concepts that are not known from the other application (because they are in 2 different business domains); The interchange format is transported in a service way, within the concept of a verb (see our article on web services here and here .","title":"The relativity of data"},{"location":"articles/data-interop/#aggregating-data","text":"Let us suppose, taking the sample of Fig. 4 , that the application 1 publishes data to application 2. If we were to aggregate data from application 1 and application 2, we would be facing a problem: what data should we consider? It is not an easy problem. The application 1 offers more details in modeling A and C as different entities, whereas application 2 is using AunionC instead. But in terms of sequence, if data in application 2 is fresher than data in application 1 (because it is after in the flow), should not we take data in application 2 instead of data in application 1? But who is really the master in those data? And what about we need data connected to C ? Is it equivalent to connect those data to AunionC ? Figure 5 : What data should be in the data lake? Fig. 5 shows the problem graphically. It is possible to solve this issue on two conditions: To explicitly determine what we intend to do by aggregating the data from those two applications; What is the point in doing that? What treatments we want to implement? For what functional purpose? To have a minimum of functional and technical knowledge about applications 1 and 2, why are they communicating together, to what purpose, at what moment, etc. Aggregating data is not an end, it is a solution answering to a business question. So what is the business question?","title":"Aggregating data"},{"location":"articles/data-interop/#about-data-lakes","text":"","title":"About data lakes"},{"location":"articles/data-interop/#the-data-lake-for-real-use-cases","text":"In some business, the interest of manipulating a huge amount of data is obvious: In Internet retail for instance: for big websites, you have a massive amount of data for each customer visiting your website with all information related to the consultations and to the purchases; Those data are being largely used for years; In the industry: Most production assembly lines generate a lot of data that are used to monitor the quality of the product and the productivity; In public services: Analyzing in details at the country level the various business trends, employment data and so on, can be done now from the real low level real data instead of using representative samples; Etc. What is common between all those samples is that the use cases of those big data are very clear and the big data technologies are just a way to answer to them. In a certain way, we are just talking about classical software applications using big data technology to store many more data than classical applications used to do.","title":"The data lake for real use cases"},{"location":"articles/data-interop/#the-data-lake-for-no-use-cases","text":"The fact is most data lakes projects are not focusing on real use cases: they do the reverse. They are infrastructure projects to enable future use cases . They are called enablers which is a nice word to say: \"let's do a technical project and we'll see afterwards if it can be used on real use cases\" (1) . Big data vendors are so powerful (as datawarehouse vendors decades ago) and so trendy, that they convince enterprises to put in place big data infrastructure prior to having real use cases. The speech is seductive: In order to realize whatever use case you will want to implement in the future, your company needs to put all company data in a data lake; This will enable you to correlate all data you want and find value in your sleeping data. This speech has the advantage of seducing end users that understand the concept, and IT people that are thrilled by the introduction of new technology (2) . For sure, the fact that data lakes with real use cases work are confusing the situation. For data lakes like for other topics, providing technology without a real use case is a very risky operation, that generally ends badly: Millions invested for nothing. ure out what system is master at what point in time.","title":"The data lake for no use cases"},{"location":"articles/data-interop/#data-lake-as-an-application","text":"Indeed, working data lakes are applications (with big data). But, if this reasoning is true, that would mean that a data lake is an application rather than a data store , that it has some data ingestion business logic to be able to ingest data that fit in a global data landscape without introducing ambiguities or erroneous data links. But, if that is the case, that means that the data lake is also creating its own viewpoint on the semantic space, like shown if Fig. 6, which most of the time is built on the pivot models we find in exchange formats. Figure 6 : The data lake as another kind of application","title":"Data lake as an application"},{"location":"articles/data-interop/#semantic-technology-applied-to-data-lakes","text":"The most recent flavor of the ever lasting concept of data warehouse is the semantic data lake . The idea can be summarized as follows: with the proper ontological description (ontologies being Semantic Web way of describing concepts), it is possible to aggregate all sorts of data and link them together. Fig. 4 is showing that, for the same concepts, we have 3 ways of representing it. The ontology approach will unfortunately not change this fact. For each representation, we can define a specific ontology that will not be the same and that will show knowledge representation choices (or modeling choices). For sure, we could define graph transformations that would make those 3 ontologies equivalent: but they are not the same. So Semantic Web technologies will not help in defining the \"data lake for no use cases\". As we saw, the modeling formalism is not important because business concepts are representations, and so are relative to the particular point of view of the application (roles and use cases).","title":"Semantic technology applied to data lakes"},{"location":"articles/data-interop/#conclusion","text":"The big data technologies are impressive. They come from the Internet age and from the solutions Cloud companies found to manage volumes of data that are much bigger than anything the humanity saw in the 20th century. Those technologies can be very useful for identified use cases, such as the industrial use cases (to better the product design, to monitor the product product, to manage the in-service life of products) or the Internet retail. But those technologies should not be envisaged outside of clearly defined use cases, first of all because all big data technologies cannot answer to all big data problems (e.g. large time series oriented data processing), and second of all because data is a relative object, quite often product of complex upstream processes. Furthermore, big data will always be expensive, and as all expensive technologies, the business case of using it should be carefully analyzed before investing.","title":"Conclusion"},{"location":"articles/data-interop/#see-also","text":"Considerations About Rest And Web Services About GraphQL","title":"See Also"},{"location":"articles/data-interop/#notes","text":"(1) - The modern vocabulary of IT is a real regression compared to UML for instance. By talking about \"features\" and \"enablers\", it opens the door to develop \"features\" for unknown business roles and business processes, and to put in place \"enablers\", so technical infrastructure for the day when people will need it. This semantic shift is the result of a large-scale technical marketing of the large Cloud companies: instead of thinking about enhancing the business, most IT people and more and more business people think about technology. Back to text . (2) - Most IT people find it hard to enter into the knowledge of the business. They prefer working on technological assets. That is why, it is an important role of the IT management to continuously refocus IT people to answer business requirements or to help businesses use wisely the IT resources to solve their business problems. Without this continuous refocus, the gap between business and IT people is growing, opening the door to \"IT done by the business\" syndrome, sign of that impossibility to communicate. Back to text . ( February 2020 )","title":"Notes"},{"location":"articles/five-levels/","text":"The Five Levels of Conceptual Maturity for IT Teams IT is a strange activity because it is an industry that has troubles getting out of a craft industry approach. Constantly moving to new exciting technologies and constantly influenced by massive technology centered marketing, IT people stay durably too technical and have recurring troubles to achieve IT projects. History of the Model Years ago, I was consulting a CEO and was trying to explain to him why his teams were failing big IT projects. A lot of executives today are digitally aware in their daily business practices but, when it comes to organizing new IT projects inside their company, they frequently discover that there is a huge gap between what they wanted to achieve and what is really done. Money is not the problem, and many big companies invest massive amounts of money in IT projects without getting the most of it. The fact is, when you invest too much in an IT project, your chances of success decrease drastically. I came to the CEO with this maturity model. I don't proclaim it to be perfect, far from it: it's just a representation of the problem. But the model is quite useful to explain and diagnose the capability of teams to perform IT projects and the levels of complexity attached to those projects. A Model of Conceptual Maturity IT is an abstract discipline manipulating abstract concepts. In IT, it is always possible to \"make the code work\", even if the cost and the delay to do that are unacceptable. The problem of IT is that you can pay the price of bad choices years (even decades) after you did the investment - and you pay each year, possibly more and more year after year for the exact same modification. Bad IT choices imply increasing the technical debt and jeopardize the capability of achieving future projects (at least you will continuously increase their cost). In that sense, IT is quite near from traditional industry. The fact is, in tradition industry, people are specialized in jobs: not every automotive worker will design the brakes or the engine. In IT, the situation is blurrier. Most IT people don't know what they are capable of doing, because specialization of jobs has not yet reached an industry agreement, and because it seems always possible to \"make the code work\" (despite of any consideration about quality/evolutions/money/timing/etc.). However, in IT, there are several levels of concerns that require different skills and different conceptual approaches. Over my 20 years in the IT business, I identified five levels in this conceptual world of software systems creation. Generally, in their professional life, people climb the rungs of the ladder in the same order, which makes the model a \"kind of\" maturity model. The Levels of Conceptual Maturity I will describe briefly the most important aspects of the various levels. I don't pretend to be exhaustive and I use more detailed materials and samples when I play the consultant for executives. Level 1: Development Most IT people know how to code, how to produce programs. For that purpose, IT people use programming languages (like Java, Cobol, C++, C, etc.). Behind the coding level, we can add the mastering of the development environment: development standards, debugging, versionning and branching management, build management, testing management, etc.. All those skills are crucial to good developments. Considering all that, if we rated people from 0 to 1 on this topic, I would say that most developers in the market are between 0.5 and 0.8 in development. Level 2: Design IT uses object orientation programming (OOP) commonly for more than two decades. However, OOP is conceptually complicated. To be explained to the non-IT people, design is the activity of building a set of programs structure and knowing where to put the code inside the program structure. Depending on how the programs are internally structured, you will face more or less troubles in the future maintenance and evolutions. Design activity uses lots of design patterns (level 2) which are generally badly mastered by IT teams because IT teams think too often at the \"code level \" (level 1). With years, and with Agile methodologies, IT people were pushed back to level 1. Design is complicated. IT people have to think before they code (very hard for developers); they have to forecast what will be the maintenance. Few people are really capable of good designs. Level 3: Application Architecture Architecting an application today is not easy, because they are a lot of trendy \"off-the-shelf\" components, a lot of competing \"sexy\" technologies. The hype around technologies is extreme and IT people can change frameworks very quickly to follow the last trend. Architecting an application is the skill to choose what components (reusable or not) will be used into an application. It enables the application to be developed correctly by several people, to isolate \"modules\", to manage customizable dependencies inside the IT systems. This is not an easy job. The objective is not just to realize software but to be able to maintain them on the long run and whatever their functional perimeter (that can become quite large with years). In order to build good application architecture, you have to think about \"component borders\" and be able to estimate the pros and cons of using a trendy component in the market. That's quite a hard job. Level 4: IT Architecture IT is everywhere and companies, even SMEs, have quite quickly a lot of IT systems. Being able to make those systems communicate together without being too hardly coupled, being able to make the IT systems as a whole evolving in a reasonable timing for reasonable costs, being able to communicate with the outside of the company in a safe manner, all that is part of IT architecture. In IT architecture, you also manage the positioning of features inside softwares: when a function is implemented in the wrong software, you can loose millions of dollars. In IT architecture problems are also the integration problems: a good IT architecture helps decreasing the QA costs and improving the TTM of the projects. Good IT architects are very rare in the market. It seems a pity because, for large companies, IT systems are one of the biggest problems, for instance in M A phases. To add to the complexity, the IT market sells a lot of \"off-the-shelf solutions\" in that area, solutions that often bring confusion in IT systems, when put in place by non level 4 people. Level 5: Enterprise Architecture The term \"Enterprise Architecture\" (EA) is at the center of a lot of discussions because the industry is not clear about what an enterprise architect is supposed to do. I will provide my own pragmatic vision (which can be challenged). An enterprise architect must primarily ensure that company strategic intentions are concretely declined into the IT systems through auditable projects and programs. Secondly he/she must ensure that the changes brought to the softwares can be managed by end users, that software changes are not destroying efficient business processes but improving them. This is quite a work that requires a lot of IT skills but also a good business understanding and enhanced communication skills. The Usual Maturity Path One common path for IT-related designers or architects is to go from level 1 to level 5. Each level is requiring, following my experience, in average, at least 2 years of real life practice. Most often, when you work at a certain level, you have to consider also the impacts of what you are doing in all other lowest levels (that's why enterprise architects that never coded often make me uncomfortable, because they cannot imagine the consequences of their decisions). Most people will find happiness in a certain layer and will excel in that layer because it fits their mind and taste and it is perfectly adapted to the level of complexity they like to manage. Without a proper project experience and without training and/or coaching, it is quite hard for people to change levels without having to work for months in real life at the concerned level. A Maturity Model Related to Complexity The model describes the capability to solve IT problems at the proper level, to manage the complexity of the IT problem with the proper approach. For instance, you can take a level 4 problem with a level 1 approach but you have 70% of chances of never solving the problem completely and 95% of chances to spend millions of dollars fixing \"things that don't work\". The \"things that don't work\" were created by people because of their problem solving approach (level 1 instead of level 4). Using the proper approach would never have generated those costs and troubles. What is the Required Level of Maturity for my Project? Regarding this maturity model, executives should appreciate the level of maturity of their internal IT organization. They are then able to ask themselves the question: what's the adequate level of maturity required to achieve the project I want? Or, we can ask the question the other way round: what projects can I safely achieve considering the maturity of my internal IT organization? Those questions are crucial. When I look at failed projects, I often see people having a maturity level between 1 and 3 trying to master a multi-million dollars IT projects requiring a level 5. Integrating external consultants can do the job but not in all cases. If your business is complex and quite specific, it is preferable to bet on internal skills in the long term for IT people to understand your business in depth. Moreover, who chooses the casting of your external consultants? Your internal IT organization, and few organizations are able to hire consultants that know more than they do. This is exactly what happened when big companies got out of the mainframe. Mainframe teams were mostly level 1, some of them were level 2 (level 3 is not relevant in old mainframe application design and level 4 is often just about producing files for the outside or consuming them). Giving such a team an IT program with distributed applications in various technologies to build is doomed to failure, whatever the money and whatever the quality and dedication of people. For sure, with time and the appropriate training, most people can climb rungs in the ladder. But without time and training, people will keep on reproducing failures. Suppose the management is not aware of the core problem and people get fired for project failure, you can end up taking big outsourcing decisions based on a situation that nobody really understood. IT Industry Should Target Maturity Because it is only \"soft\"-ware and not hardware, the IT industry seems to like the level 1 so much that every serious attempt to raise to level 2 or 3 is pushed back to level 1 by the IT providers. IT providers love to focus their customers on the \"code level\": because they cannot see the bigger picture and buy their products even if they don't need them. If the IT industry created models for IT project management (like PMP), the increasing complexity of existing systems and new projects require the appropriate level of maturity in the project designers/architects teams. The problem today, in most companies, is not only to \"code\" nor only to \"manage projects\": it is to also to maintain the running systems and make the huge amount of legacy applications evolve without risks and conforming to the company strategic intentions. Most IT systems are a pile of geological layers of outdated technological hypes. The code of today becomes the legacy of tomorrow. If some industries (like aviation) forecast their investments for decades, IT should begin to do the same, to work on the adequate long-term skills that are required to ensure durable and quality IT services. IT industry should target maturity and maturity comes with the identification of our level of mastering of IT complexity. ( April 2015 )","title":"The Five Levels of Conceptual Maturity for IT Teams"},{"location":"articles/five-levels/#the-five-levels-of-conceptual-maturity-for-it-teams","text":"IT is a strange activity because it is an industry that has troubles getting out of a craft industry approach. Constantly moving to new exciting technologies and constantly influenced by massive technology centered marketing, IT people stay durably too technical and have recurring troubles to achieve IT projects.","title":"The Five Levels of Conceptual Maturity for IT Teams"},{"location":"articles/five-levels/#history-of-the-model","text":"Years ago, I was consulting a CEO and was trying to explain to him why his teams were failing big IT projects. A lot of executives today are digitally aware in their daily business practices but, when it comes to organizing new IT projects inside their company, they frequently discover that there is a huge gap between what they wanted to achieve and what is really done. Money is not the problem, and many big companies invest massive amounts of money in IT projects without getting the most of it. The fact is, when you invest too much in an IT project, your chances of success decrease drastically. I came to the CEO with this maturity model. I don't proclaim it to be perfect, far from it: it's just a representation of the problem. But the model is quite useful to explain and diagnose the capability of teams to perform IT projects and the levels of complexity attached to those projects.","title":"History of the Model"},{"location":"articles/five-levels/#a-model-of-conceptual-maturity","text":"IT is an abstract discipline manipulating abstract concepts. In IT, it is always possible to \"make the code work\", even if the cost and the delay to do that are unacceptable. The problem of IT is that you can pay the price of bad choices years (even decades) after you did the investment - and you pay each year, possibly more and more year after year for the exact same modification. Bad IT choices imply increasing the technical debt and jeopardize the capability of achieving future projects (at least you will continuously increase their cost). In that sense, IT is quite near from traditional industry. The fact is, in tradition industry, people are specialized in jobs: not every automotive worker will design the brakes or the engine. In IT, the situation is blurrier. Most IT people don't know what they are capable of doing, because specialization of jobs has not yet reached an industry agreement, and because it seems always possible to \"make the code work\" (despite of any consideration about quality/evolutions/money/timing/etc.). However, in IT, there are several levels of concerns that require different skills and different conceptual approaches. Over my 20 years in the IT business, I identified five levels in this conceptual world of software systems creation. Generally, in their professional life, people climb the rungs of the ladder in the same order, which makes the model a \"kind of\" maturity model.","title":"A Model of Conceptual Maturity"},{"location":"articles/five-levels/#the-levels-of-conceptual-maturity","text":"I will describe briefly the most important aspects of the various levels. I don't pretend to be exhaustive and I use more detailed materials and samples when I play the consultant for executives.","title":"The Levels of Conceptual Maturity"},{"location":"articles/five-levels/#level-1-development","text":"Most IT people know how to code, how to produce programs. For that purpose, IT people use programming languages (like Java, Cobol, C++, C, etc.). Behind the coding level, we can add the mastering of the development environment: development standards, debugging, versionning and branching management, build management, testing management, etc.. All those skills are crucial to good developments. Considering all that, if we rated people from 0 to 1 on this topic, I would say that most developers in the market are between 0.5 and 0.8 in development.","title":"Level 1: Development"},{"location":"articles/five-levels/#level-2-design","text":"IT uses object orientation programming (OOP) commonly for more than two decades. However, OOP is conceptually complicated. To be explained to the non-IT people, design is the activity of building a set of programs structure and knowing where to put the code inside the program structure. Depending on how the programs are internally structured, you will face more or less troubles in the future maintenance and evolutions. Design activity uses lots of design patterns (level 2) which are generally badly mastered by IT teams because IT teams think too often at the \"code level \" (level 1). With years, and with Agile methodologies, IT people were pushed back to level 1. Design is complicated. IT people have to think before they code (very hard for developers); they have to forecast what will be the maintenance. Few people are really capable of good designs.","title":"Level 2: Design"},{"location":"articles/five-levels/#level-3-application-architecture","text":"Architecting an application today is not easy, because they are a lot of trendy \"off-the-shelf\" components, a lot of competing \"sexy\" technologies. The hype around technologies is extreme and IT people can change frameworks very quickly to follow the last trend. Architecting an application is the skill to choose what components (reusable or not) will be used into an application. It enables the application to be developed correctly by several people, to isolate \"modules\", to manage customizable dependencies inside the IT systems. This is not an easy job. The objective is not just to realize software but to be able to maintain them on the long run and whatever their functional perimeter (that can become quite large with years). In order to build good application architecture, you have to think about \"component borders\" and be able to estimate the pros and cons of using a trendy component in the market. That's quite a hard job.","title":"Level 3: Application Architecture"},{"location":"articles/five-levels/#level-4-it-architecture","text":"IT is everywhere and companies, even SMEs, have quite quickly a lot of IT systems. Being able to make those systems communicate together without being too hardly coupled, being able to make the IT systems as a whole evolving in a reasonable timing for reasonable costs, being able to communicate with the outside of the company in a safe manner, all that is part of IT architecture. In IT architecture, you also manage the positioning of features inside softwares: when a function is implemented in the wrong software, you can loose millions of dollars. In IT architecture problems are also the integration problems: a good IT architecture helps decreasing the QA costs and improving the TTM of the projects. Good IT architects are very rare in the market. It seems a pity because, for large companies, IT systems are one of the biggest problems, for instance in M A phases. To add to the complexity, the IT market sells a lot of \"off-the-shelf solutions\" in that area, solutions that often bring confusion in IT systems, when put in place by non level 4 people.","title":"Level 4: IT Architecture"},{"location":"articles/five-levels/#level-5-enterprise-architecture","text":"The term \"Enterprise Architecture\" (EA) is at the center of a lot of discussions because the industry is not clear about what an enterprise architect is supposed to do. I will provide my own pragmatic vision (which can be challenged). An enterprise architect must primarily ensure that company strategic intentions are concretely declined into the IT systems through auditable projects and programs. Secondly he/she must ensure that the changes brought to the softwares can be managed by end users, that software changes are not destroying efficient business processes but improving them. This is quite a work that requires a lot of IT skills but also a good business understanding and enhanced communication skills.","title":"Level 5: Enterprise Architecture"},{"location":"articles/five-levels/#the-usual-maturity-path","text":"One common path for IT-related designers or architects is to go from level 1 to level 5. Each level is requiring, following my experience, in average, at least 2 years of real life practice. Most often, when you work at a certain level, you have to consider also the impacts of what you are doing in all other lowest levels (that's why enterprise architects that never coded often make me uncomfortable, because they cannot imagine the consequences of their decisions). Most people will find happiness in a certain layer and will excel in that layer because it fits their mind and taste and it is perfectly adapted to the level of complexity they like to manage. Without a proper project experience and without training and/or coaching, it is quite hard for people to change levels without having to work for months in real life at the concerned level.","title":"The Usual Maturity Path"},{"location":"articles/five-levels/#a-maturity-model-related-to-complexity","text":"The model describes the capability to solve IT problems at the proper level, to manage the complexity of the IT problem with the proper approach. For instance, you can take a level 4 problem with a level 1 approach but you have 70% of chances of never solving the problem completely and 95% of chances to spend millions of dollars fixing \"things that don't work\". The \"things that don't work\" were created by people because of their problem solving approach (level 1 instead of level 4). Using the proper approach would never have generated those costs and troubles.","title":"A Maturity Model Related to Complexity"},{"location":"articles/five-levels/#what-is-the-required-level-of-maturity-for-my-project","text":"Regarding this maturity model, executives should appreciate the level of maturity of their internal IT organization. They are then able to ask themselves the question: what's the adequate level of maturity required to achieve the project I want? Or, we can ask the question the other way round: what projects can I safely achieve considering the maturity of my internal IT organization? Those questions are crucial. When I look at failed projects, I often see people having a maturity level between 1 and 3 trying to master a multi-million dollars IT projects requiring a level 5. Integrating external consultants can do the job but not in all cases. If your business is complex and quite specific, it is preferable to bet on internal skills in the long term for IT people to understand your business in depth. Moreover, who chooses the casting of your external consultants? Your internal IT organization, and few organizations are able to hire consultants that know more than they do. This is exactly what happened when big companies got out of the mainframe. Mainframe teams were mostly level 1, some of them were level 2 (level 3 is not relevant in old mainframe application design and level 4 is often just about producing files for the outside or consuming them). Giving such a team an IT program with distributed applications in various technologies to build is doomed to failure, whatever the money and whatever the quality and dedication of people. For sure, with time and the appropriate training, most people can climb rungs in the ladder. But without time and training, people will keep on reproducing failures. Suppose the management is not aware of the core problem and people get fired for project failure, you can end up taking big outsourcing decisions based on a situation that nobody really understood.","title":"What is the Required Level of Maturity for my Project?"},{"location":"articles/five-levels/#it-industry-should-target-maturity","text":"Because it is only \"soft\"-ware and not hardware, the IT industry seems to like the level 1 so much that every serious attempt to raise to level 2 or 3 is pushed back to level 1 by the IT providers. IT providers love to focus their customers on the \"code level\": because they cannot see the bigger picture and buy their products even if they don't need them. If the IT industry created models for IT project management (like PMP), the increasing complexity of existing systems and new projects require the appropriate level of maturity in the project designers/architects teams. The problem today, in most companies, is not only to \"code\" nor only to \"manage projects\": it is to also to maintain the running systems and make the huge amount of legacy applications evolve without risks and conforming to the company strategic intentions. Most IT systems are a pile of geological layers of outdated technological hypes. The code of today becomes the legacy of tomorrow. If some industries (like aviation) forecast their investments for decades, IT should begin to do the same, to work on the adequate long-term skills that are required to ensure durable and quality IT services. IT industry should target maturity and maturity comes with the identification of our level of mastering of IT complexity. ( April 2015 )","title":"IT Industry Should Target Maturity"},{"location":"articles/graphql-web-services/","text":"GraphQL And Classic Web Services Facebook produced the GraphQL specification in order to be an alternative to Rest. We already explained what we thought of Rest services . GraphQL appears to us as a new way to do the same thing that what the industry is doing for decades. This article is an analysis of GraphQL principles, their consequences and a comparison with traditional ways of doing web services. The Basics of Services Various Standards, Same Spirit The underlying basis of services is RPC (Remote Procedure Call, see the dedicated article on the subject ). For decades, this concept is reinvented again and again with various flavors. Apart from RPC protocols or proprietary protocols (like the Tuxedo ones), Edifact was for a long time an industry standard in some business domains like travel, banking, medical and logistics. Edifact was replaced by XML then by JSON . Every of those standards aimed to provide the same functionality: exchanging structured data between two different systems, probably implemented with different programming languages and/or paradigms, and having a client system benefiting from a service provided by a server system. Basic Requirements For a Service Oriented Language The first requirement is to have a \"verb\", meaning the name of the \"transaction\" we expect the remote system to process. The second requirement is to have a type system to be able to send structured data under the form of a tree of objects. This is generally achieved by proposing basic types (like string, integer, float, etc.) and the capability of defining fields that have those properties and that can be grouped into classes. Classes will generally have to indicate what fields are required and what fields are optional. This is generally done within the context of one particular transaction. Once classes are defined, they can be grouped in larger structures or other classes to define groups of object structures than define larger entities, most of the time, better suitable to express the business semantics. Optionally, those definitions are expressed in \"grammars\" or \"schemas\". This is the case in RPC systems, in Edifact and in XML, and this is optional for JSON (even if the JSON schema specification is very usable). Note that, even if there is no explicit schema mechanism, the fact of defining objects and structures leads to the use of implicit schemas. The difference is really what part of the code is \"validating\" that the messages are well formed: With an explicit schema definition, we can generate the validation code and create a kind of serializer/unserializer layer (wrapper/unwrapper or proxy/stub layer); Without an explicit schema definition, the validation code is generally hand written. All those requirements are defined in the OSI model in the \"presentation layer\". Don't forget the protocol The last requirement is to have some kind of underlying protocol to: Send and receive messages between systems; Create a naming convention that enables to use the name of the service (or name of the \"verb\"); Find the network configuration that enable to route the request message to the proper entry point; Possibly associate the request and the response with identifiers; Possibly include information about the sender and its security credentials. Once again, the OSI model defines a certain split of responsibilities between all those requirements. Quickly explained: The transport layer (layer 4) is defining the addressing scheme and primitives that enables several systems to communicate together; The session layer (layer 5) enables to manage the sessions and conversations between the client and the server; this layer manages the succession of names messages that the two peers will exchange; The presentation layer (layer 6) manages the content of data, their structure and their values. In the current JSON world, JSON-RPC is presenting a very basic protocol that can manage the basic single statement and request response messages. GraphQL Answers To Service Oriented Requirements GraphQL seems to us as a new flavor of the same old story. It brings some interesting new stuff (and we will come back on that), but the important point is that we can implement traditional services with this language. The \"Verb\" Each GraphQL service is an named http entry point. The get method will be used to access the query part of the service and the post method will access the mutation part of it. This looks like Rest but this is rather different, because the idea seems not to access to a single resource (even if it is possible), but to perform a service. The Type System GraphQL proposes an extended type system that proposes: Basic types, Classes, Groups of classes, Fragments (kind of filters on classes), Queries. With all the available semantics, it is very easy to implement all the existing web services that are used currently in the business. Moreover, it is possible to have a richer interface on existing services and a more adaptative way of performing transactions. This seems rather promising for mobile or chatbot accesses for instance. Those two UIs manipulate a small amount of data and may require some server services to adapt their reply to their particular use. For sure, we could argue saying: with standard web services in JSON, as a client, we can always extract the subset of the response that is interesting to us. The fact is this feature goes beyond the subset approach and enables to ask the server to adapt the contents of its response to the client. And The Protocol? In the Web, the protocol is http and the approach seems inspired by both Rest and Ajax kind of calls. A Promising Standard An Intermediate Between JSON-RPC and Rest? GraphQL seems to target the intermediate path between JSON-RPC (which is a simplified SOAP) and Rest. The fact of publishing a schema enables both approaches: Perform elaborated services (like before); Access business objects (like the Rest or CRUD approach). GraphQL seems to propose a way to benefit from both approaches. The fact of being able to perform complex queries (with a lot of flavours) also adds intelligence to the protocol, an intelligence driven by the client. But What Impact On The Server Design? This seems very interesting but the problem caused by this new protocol is located on the backend part. Some questions must be answered: What kind of models and/or application this protocol is targeting? All applications? Applications that should be able to disclose a part of their model? Do we know the extend of the server impacts in terms of design? Model Your Business Domain As A Graph We are entering here into delicate topics. For those how already performed some semantic modeling with RDF , RDFS or OWL , I think you're convinced about the fact that we can model a business domain with a graph. For those who already used graph databases such as Neo or Orient in Big Data use cases or fraud recognition or social applications, may be convinced that a business model can be modeled as a graph. But for the vast majority of IT people, this assertion is not obvious. Well, we'll come back on this particular topic in future articles, so we'll take for granted that a business domain can be modeled with a graph. Graph Data Means Graph Database What Facebook is proposing us is a protocol that enables to query the internal structure of the graph of data. So, the first consequences seem to be: Your data are modeled as a graph, You probably use a graph database of some sort. Consequently, your GraphQL schema is more or less the same thing as your graph database schema (when this sort of things exists such as in OrientDB ). Open Heart Data Model The second consequence is also double: You published your graph business model to your clients; You use your database model as the presentation layer. This second point is very problematic. Indeed, it explains why the protocol is proposing so much complexity in some of its part: because it is supposed to be, at the same time, a protocol and a graph query language (like Cypher ). In a certain sense, it is supposed to be the new SQL*Net or ODBC but for graphs. And this is where the approach is questionable. The Myth Of The Single Representation Of Reality There are many myths in IT, and this one is a big one. Many people believe that there is only one way of representing the reality-theirs, indeed. And the reality is, they are wrong. Note that, throughout the history of science, many great scientists had the same debate as the intuitionist debate of the beginning of the 20th Century . Any software designer that worked sufficiently in the domain should have seen several facts: Whatever the efforts and the design patterns used, you cannot predict how the business will evolve; Two designer confronted to the same problem will produce two different designs. Indeed, design is very subjective and, for a lot of reasons we won't explain here, it is not a bad thing that there is no unique model for a particular business domain, on the contrary. When Client and Server Share Everything GraphQL is proposing a way for the customer to be very near from its server, so near that its data model is perfectly known, right from the client. This can be very useful when you work with yourself, when you develop your own client and server in a kind of symbiosis. If you are on an Internet application, you also must know that you core model will be known from the outside, which can be a problems in terms of IP and in terms of security. So: GraphQL is OK if your API is internal ; GraphQL seems not OK if your API is external. Publishing a GraphQL API Well, if you want to publish a GraphQL API, you have to consider several things: You will impose to your client the graph model of your business domain, and maybe it is relevant and maybe not (see next part); You will disclose a graph representation of your business model, which is not the case in JSON-RPC where you only disclose the interoperability formats ; This can represent a potential threat on your IT because your software design is very often at the heart of your business value; You will have to have a complex security system, which is the security system of the graph exploration, and this will not be obvious to address; You will be bound to implement the protocol complexity that can open more security issues in your software. For sure, there are cases where all those arguments may be irrelevant: You can work in a domain where the graph model has no value or is very well known (for instance the social media); You can work for non profit organizations; You can have a system that will not cause any loss of money, loss of IP or loss of value if hacked. The Fundamental Principle Of Interoperability In complement to the article on REST , we will explain the fundamental principle of interoperability. The context of interoperability is the following: Systems communicate when they have some interest in doing so: interoperability is business-oriented. To establish system interconnection costs money and time, this in the project phase but also in the recurring phase: to realize those investments, the client and the server generally have a good business reason. When a server is publishing an API, it cannot imagine the list of clients he will have tomorrow, nor make assumptions on their programming language or even design approach. In this context, the fundamental principle of interoperability is that the client and the server contracting the exchange should define the common format that is proposing the less semantic constraints possible on both ends . Because the client and the server don't have to commit on their internal structure and functional aspects, the interchange is described with a semantic that must be the simplest possible and that can enable the client and server to define different modeling of the same \"exchanged data model\". The figure above shows the core problem of interoperability: The client and the server have no reason to model the interchange data the same way. Indeed, they don't. The consequence is that they must adapt their internal formats (memory or database) to the network presentation. The way they store the data is their problem, and their storage representation can be even quite different from their memory representation (that's why, generally, people use a data access layer to adapt their business objects to the database). Very commonly, in the industry, we have 3 representations of the data: the client's, the server's and the network presentation. Those adaptation are painful to develop, for sure, but they are the conditions of isolation, and who says isolation says separation of concerns. This is because the client and the server are different software parts that there is no need to impose more constraints. In this context, we do not recommend using GraphQL for an external business domain interface, but we can recommend it if the application architecture needs a kinf of ODBC or SQL*Net directly from the UI, and in a context where the API is not made to be exposed to unknown third parties. Note that we did not really enter into the core details of \"how do we really build a graph-oriented business application\"? This will come in later articles. A Correct Intuition? If we try to step back one minute, we can say that the Facebook team may have had an intuition that, with the graph-oriented modeling approach, this fundamental principle or interoperability could be declared obsolete (and consequently graphs could be in the client, in the server, in the database, with exactly the same structure). In some cases, that's true: if you master both ends of the wire, that's probably one very efficient way to do it. But it looks like ODBC or SQL*Net anyway. One thing is sure, the graph-oriented modeling of business domains will revolution the IT world, but perhaps not the way the Facebook team imagined it. We'll come back on that in the future. Conclusion GraphQL is a very interesting attempt to propose a middle term between REST and JSON-RPC, as the following diagram is showing it. However, this diagram is very misleading because the 3 inbound protocols have many different impacts on the server design, contrary to what's presented. Indeed, both REST and GraphQL imply a very specific programming model: REST is imposing a hard resource orientation that is unnatural to business applications (see here ), GraphQL proposes a graph-oriented ODBC-like protocol that will have the tendency to tie-up strongly the client and the server. The conclusion is it seems to us that, so far, only RPC enables to design and deliver reliable and secure business applications, and to do it the way you want. RPC defines a contract that can lead to many various programming paradigms, which is not the case for REST or for GraphQL. The GraphQL has, however, opened publicly the case of graph-orientation in the design of business applications. This area is really a core game changer for the IT business and it will be a topic described and explained in the future in this site. See also About Rest ( December 2017 )","title":"GraphQL And Classic Web Services"},{"location":"articles/graphql-web-services/#graphql-and-classic-web-services","text":"Facebook produced the GraphQL specification in order to be an alternative to Rest. We already explained what we thought of Rest services . GraphQL appears to us as a new way to do the same thing that what the industry is doing for decades. This article is an analysis of GraphQL principles, their consequences and a comparison with traditional ways of doing web services.","title":"GraphQL And Classic Web Services"},{"location":"articles/graphql-web-services/#the-basics-of-services","text":"","title":"The Basics of Services"},{"location":"articles/graphql-web-services/#various-standards-same-spirit","text":"The underlying basis of services is RPC (Remote Procedure Call, see the dedicated article on the subject ). For decades, this concept is reinvented again and again with various flavors. Apart from RPC protocols or proprietary protocols (like the Tuxedo ones), Edifact was for a long time an industry standard in some business domains like travel, banking, medical and logistics. Edifact was replaced by XML then by JSON . Every of those standards aimed to provide the same functionality: exchanging structured data between two different systems, probably implemented with different programming languages and/or paradigms, and having a client system benefiting from a service provided by a server system.","title":"Various Standards, Same Spirit"},{"location":"articles/graphql-web-services/#basic-requirements-for-a-service-oriented-language","text":"The first requirement is to have a \"verb\", meaning the name of the \"transaction\" we expect the remote system to process. The second requirement is to have a type system to be able to send structured data under the form of a tree of objects. This is generally achieved by proposing basic types (like string, integer, float, etc.) and the capability of defining fields that have those properties and that can be grouped into classes. Classes will generally have to indicate what fields are required and what fields are optional. This is generally done within the context of one particular transaction. Once classes are defined, they can be grouped in larger structures or other classes to define groups of object structures than define larger entities, most of the time, better suitable to express the business semantics. Optionally, those definitions are expressed in \"grammars\" or \"schemas\". This is the case in RPC systems, in Edifact and in XML, and this is optional for JSON (even if the JSON schema specification is very usable). Note that, even if there is no explicit schema mechanism, the fact of defining objects and structures leads to the use of implicit schemas. The difference is really what part of the code is \"validating\" that the messages are well formed: With an explicit schema definition, we can generate the validation code and create a kind of serializer/unserializer layer (wrapper/unwrapper or proxy/stub layer); Without an explicit schema definition, the validation code is generally hand written. All those requirements are defined in the OSI model in the \"presentation layer\".","title":"Basic Requirements For a Service Oriented Language"},{"location":"articles/graphql-web-services/#dont-forget-the-protocol","text":"The last requirement is to have some kind of underlying protocol to: Send and receive messages between systems; Create a naming convention that enables to use the name of the service (or name of the \"verb\"); Find the network configuration that enable to route the request message to the proper entry point; Possibly associate the request and the response with identifiers; Possibly include information about the sender and its security credentials. Once again, the OSI model defines a certain split of responsibilities between all those requirements. Quickly explained: The transport layer (layer 4) is defining the addressing scheme and primitives that enables several systems to communicate together; The session layer (layer 5) enables to manage the sessions and conversations between the client and the server; this layer manages the succession of names messages that the two peers will exchange; The presentation layer (layer 6) manages the content of data, their structure and their values. In the current JSON world, JSON-RPC is presenting a very basic protocol that can manage the basic single statement and request response messages.","title":"Don't forget the protocol"},{"location":"articles/graphql-web-services/#graphql-answers-to-service-oriented-requirements","text":"GraphQL seems to us as a new flavor of the same old story. It brings some interesting new stuff (and we will come back on that), but the important point is that we can implement traditional services with this language.","title":"GraphQL Answers To Service Oriented Requirements"},{"location":"articles/graphql-web-services/#the-verb","text":"Each GraphQL service is an named http entry point. The get method will be used to access the query part of the service and the post method will access the mutation part of it. This looks like Rest but this is rather different, because the idea seems not to access to a single resource (even if it is possible), but to perform a service.","title":"The \"Verb\""},{"location":"articles/graphql-web-services/#the-type-system","text":"GraphQL proposes an extended type system that proposes: Basic types, Classes, Groups of classes, Fragments (kind of filters on classes), Queries. With all the available semantics, it is very easy to implement all the existing web services that are used currently in the business. Moreover, it is possible to have a richer interface on existing services and a more adaptative way of performing transactions. This seems rather promising for mobile or chatbot accesses for instance. Those two UIs manipulate a small amount of data and may require some server services to adapt their reply to their particular use. For sure, we could argue saying: with standard web services in JSON, as a client, we can always extract the subset of the response that is interesting to us. The fact is this feature goes beyond the subset approach and enables to ask the server to adapt the contents of its response to the client.","title":"The Type System"},{"location":"articles/graphql-web-services/#and-the-protocol","text":"In the Web, the protocol is http and the approach seems inspired by both Rest and Ajax kind of calls.","title":"And The Protocol?"},{"location":"articles/graphql-web-services/#a-promising-standard","text":"","title":"A Promising Standard"},{"location":"articles/graphql-web-services/#an-intermediate-between-json-rpc-and-rest","text":"GraphQL seems to target the intermediate path between JSON-RPC (which is a simplified SOAP) and Rest. The fact of publishing a schema enables both approaches: Perform elaborated services (like before); Access business objects (like the Rest or CRUD approach). GraphQL seems to propose a way to benefit from both approaches. The fact of being able to perform complex queries (with a lot of flavours) also adds intelligence to the protocol, an intelligence driven by the client.","title":"An Intermediate Between JSON-RPC and Rest?"},{"location":"articles/graphql-web-services/#but-what-impact-on-the-server-design","text":"This seems very interesting but the problem caused by this new protocol is located on the backend part. Some questions must be answered: What kind of models and/or application this protocol is targeting? All applications? Applications that should be able to disclose a part of their model? Do we know the extend of the server impacts in terms of design?","title":"But What Impact On The Server Design?"},{"location":"articles/graphql-web-services/#model-your-business-domain-as-a-graph","text":"We are entering here into delicate topics. For those how already performed some semantic modeling with RDF , RDFS or OWL , I think you're convinced about the fact that we can model a business domain with a graph. For those who already used graph databases such as Neo or Orient in Big Data use cases or fraud recognition or social applications, may be convinced that a business model can be modeled as a graph. But for the vast majority of IT people, this assertion is not obvious. Well, we'll come back on this particular topic in future articles, so we'll take for granted that a business domain can be modeled with a graph.","title":"Model Your Business Domain As A Graph"},{"location":"articles/graphql-web-services/#graph-data-means-graph-database","text":"What Facebook is proposing us is a protocol that enables to query the internal structure of the graph of data. So, the first consequences seem to be: Your data are modeled as a graph, You probably use a graph database of some sort. Consequently, your GraphQL schema is more or less the same thing as your graph database schema (when this sort of things exists such as in OrientDB ).","title":"Graph Data Means Graph Database"},{"location":"articles/graphql-web-services/#open-heart-data-model","text":"The second consequence is also double: You published your graph business model to your clients; You use your database model as the presentation layer. This second point is very problematic. Indeed, it explains why the protocol is proposing so much complexity in some of its part: because it is supposed to be, at the same time, a protocol and a graph query language (like Cypher ). In a certain sense, it is supposed to be the new SQL*Net or ODBC but for graphs. And this is where the approach is questionable.","title":"Open Heart Data Model"},{"location":"articles/graphql-web-services/#the-myth-of-the-single-representation-of-reality","text":"There are many myths in IT, and this one is a big one. Many people believe that there is only one way of representing the reality-theirs, indeed. And the reality is, they are wrong. Note that, throughout the history of science, many great scientists had the same debate as the intuitionist debate of the beginning of the 20th Century . Any software designer that worked sufficiently in the domain should have seen several facts: Whatever the efforts and the design patterns used, you cannot predict how the business will evolve; Two designer confronted to the same problem will produce two different designs. Indeed, design is very subjective and, for a lot of reasons we won't explain here, it is not a bad thing that there is no unique model for a particular business domain, on the contrary.","title":"The Myth Of The Single Representation Of Reality"},{"location":"articles/graphql-web-services/#when-client-and-server-share-everything","text":"GraphQL is proposing a way for the customer to be very near from its server, so near that its data model is perfectly known, right from the client. This can be very useful when you work with yourself, when you develop your own client and server in a kind of symbiosis. If you are on an Internet application, you also must know that you core model will be known from the outside, which can be a problems in terms of IP and in terms of security. So: GraphQL is OK if your API is internal ; GraphQL seems not OK if your API is external.","title":"When Client and Server Share Everything"},{"location":"articles/graphql-web-services/#publishing-a-graphql-api","text":"Well, if you want to publish a GraphQL API, you have to consider several things: You will impose to your client the graph model of your business domain, and maybe it is relevant and maybe not (see next part); You will disclose a graph representation of your business model, which is not the case in JSON-RPC where you only disclose the interoperability formats ; This can represent a potential threat on your IT because your software design is very often at the heart of your business value; You will have to have a complex security system, which is the security system of the graph exploration, and this will not be obvious to address; You will be bound to implement the protocol complexity that can open more security issues in your software. For sure, there are cases where all those arguments may be irrelevant: You can work in a domain where the graph model has no value or is very well known (for instance the social media); You can work for non profit organizations; You can have a system that will not cause any loss of money, loss of IP or loss of value if hacked.","title":"Publishing a GraphQL API"},{"location":"articles/graphql-web-services/#the-fundamental-principle-of-interoperability","text":"In complement to the article on REST , we will explain the fundamental principle of interoperability. The context of interoperability is the following: Systems communicate when they have some interest in doing so: interoperability is business-oriented. To establish system interconnection costs money and time, this in the project phase but also in the recurring phase: to realize those investments, the client and the server generally have a good business reason. When a server is publishing an API, it cannot imagine the list of clients he will have tomorrow, nor make assumptions on their programming language or even design approach. In this context, the fundamental principle of interoperability is that the client and the server contracting the exchange should define the common format that is proposing the less semantic constraints possible on both ends . Because the client and the server don't have to commit on their internal structure and functional aspects, the interchange is described with a semantic that must be the simplest possible and that can enable the client and server to define different modeling of the same \"exchanged data model\". The figure above shows the core problem of interoperability: The client and the server have no reason to model the interchange data the same way. Indeed, they don't. The consequence is that they must adapt their internal formats (memory or database) to the network presentation. The way they store the data is their problem, and their storage representation can be even quite different from their memory representation (that's why, generally, people use a data access layer to adapt their business objects to the database). Very commonly, in the industry, we have 3 representations of the data: the client's, the server's and the network presentation. Those adaptation are painful to develop, for sure, but they are the conditions of isolation, and who says isolation says separation of concerns. This is because the client and the server are different software parts that there is no need to impose more constraints. In this context, we do not recommend using GraphQL for an external business domain interface, but we can recommend it if the application architecture needs a kinf of ODBC or SQL*Net directly from the UI, and in a context where the API is not made to be exposed to unknown third parties. Note that we did not really enter into the core details of \"how do we really build a graph-oriented business application\"? This will come in later articles.","title":"The Fundamental Principle Of Interoperability"},{"location":"articles/graphql-web-services/#a-correct-intuition","text":"If we try to step back one minute, we can say that the Facebook team may have had an intuition that, with the graph-oriented modeling approach, this fundamental principle or interoperability could be declared obsolete (and consequently graphs could be in the client, in the server, in the database, with exactly the same structure). In some cases, that's true: if you master both ends of the wire, that's probably one very efficient way to do it. But it looks like ODBC or SQL*Net anyway. One thing is sure, the graph-oriented modeling of business domains will revolution the IT world, but perhaps not the way the Facebook team imagined it. We'll come back on that in the future.","title":"A Correct Intuition?"},{"location":"articles/graphql-web-services/#conclusion","text":"GraphQL is a very interesting attempt to propose a middle term between REST and JSON-RPC, as the following diagram is showing it. However, this diagram is very misleading because the 3 inbound protocols have many different impacts on the server design, contrary to what's presented. Indeed, both REST and GraphQL imply a very specific programming model: REST is imposing a hard resource orientation that is unnatural to business applications (see here ), GraphQL proposes a graph-oriented ODBC-like protocol that will have the tendency to tie-up strongly the client and the server. The conclusion is it seems to us that, so far, only RPC enables to design and deliver reliable and secure business applications, and to do it the way you want. RPC defines a contract that can lead to many various programming paradigms, which is not the case for REST or for GraphQL. The GraphQL has, however, opened publicly the case of graph-orientation in the design of business applications. This area is really a core game changer for the IT business and it will be a topic described and explained in the future in this site.","title":"Conclusion"},{"location":"articles/graphql-web-services/#see-also","text":"About Rest ( December 2017 )","title":"See also"},{"location":"articles/mbse-vs-ea/","text":"Military frameworks, systems engineering and enterprise architecture (Courtesy of Pixaby on https://www.pexels.com) In the world of architecture modeling, the military organizations like the US Department of Defense (DoD), the UK Ministry of Defence (MoD) or the NATO organization created enterprise architecture frameworks adapted to their needs. On the market today, 3 main frameworks are existing and used in military projects: DoDAF , the US MoD Architecture Framework, that was historically the first military architecture framework ever (it is originated in the 90s); MoDAF , the UK MoD architecture framework, which is both an adaptation and an abstraction of DoDAF; NAF , the NATO Architecture Framework that is also an adaptation and an abstraction of DoDAF and MoDAF. Objectives and dimensions of the military frameworks The main objective searched by military organization is to be able to master a global project of several billions of dollars/euros from the design phase to the support and operational phase. Because those projects/programs can be very large and very complex, the idea was that everyone (military organization and supplier organizations) share the same set of \"views\" on the program in order to share understanding of what was to be done, how requirements were taken into account, what is the full lifecycle of the product and how the project should be managed. Table 1: Structure of military requirements # Dimension Details 1 Product definition Can be an aircraft, weapon, complex system, etc. and all the related IT systems to manage them. The objective is to use a systems engineering approach (model-based in that case), based on requirements. 2 Product support and operations The objective of this phase is to detail all the systems that will permit the support of the product. This is generally a wide topic including maintenance, parts procurement, etc. Moreover, the product use must be envisaged in operations preparation and in battle field in coordination with other systems and organizations. 3 Digital transformation The objective of this phase is to evaluate and deal with the impacts of the product and product support in the operational organization, processes and IT of the army. 4 Project plan The objective is to be able to describe the the plan to deliver the product conforming to the requirements (phases, milestones and so on), to permit project steering, and product support procurement. We can classify the military enterprise architecture frameworks as pursuing 4 objectives, as shown in table 1. Modeling domains Those 4 objectives are traditionally modeled with 3 different modeling techniques: Systems engineering for the product definition, Enterprise architecture for: Product support, Digital transformation, Project modeling tools for project management. Figure 1: The 3 domains Military framework concerns in the industrial V-model The figure 1 shows the 3 modeling modeling domains that they are traditionally covered in military frameworks. In terms of modeling, we could say that the core objective of the military frameworks is to have, in a single repository, the systems engineering (MBE/MBSE), the enterprise architecture and the project management gathered together . Positioning in the V-model Those 4 areas of concern can be mapped on an industrial V-model as shown in Figure 2. Figure 2: Military framework concerns in the industrial V-model The first dimension, product definition, is in the left part of the V and corresponds to the product architecture, from its operational conditions, requirements to its system design. The second dimension is corresponding to product support (we often speak about \"support concept\"). The two last dimensions are running all along the project, being at the project/program level or at the digital transformation level (integrate the new product into the current organizations). We can see in Figure 2 how an integrated modeling of those two highest branches of the V-model can be important for the military customer: It ensures a full lifecycle modeling of the product, It enables simulation scenarios linked to the requirement phase, It enables to anticipate the impact of the new project on army's existing organizations, It enables to manage the project/program. Most of all, on top of being a common modeling language, it enables complex tradeoffs, including performance, finance, supportability, etc. MBSE/MBE The RFLP process In manufacturing, the objective is to create products. The starting point of a product manufacturing is generally a set of requirements. To go from the requirements to the product is generally a long process involving the following phases: Requirements engineering : The objective of this phase is to work on requirements in order to make them non ambiguous, to sort them by importance and to remove inconsistency between them; In this phase, all the operational conditions of the product are described, which is a crucial phase of the process; Functional analysis : This phase objective is to determine the functions of the product, the information flows and dependencies between functions; Generally, the functions are worked at different levels of granularity (functions, sub-functions, sub-sub-functions and so on); Logical system analysis : Once the various functions at various levels are known, the assignment of functions to logical elements can be done; The objective of this phase is to create the appropriate logical systems to get the nearest possible from a possible industrial split of the realization of the product; When the logical analysis is finished, we have the detailed specification of each system ; Physical design : Once the product has been split on several components, several teams can work in parallel on component design (at the level of granularity of the component); The product architecture will ensure that all detailed components fit together conforming to the specifications. This view is, for sure, simplified and some industries (or specific businesses inside the industries like embedded software, electronics, or mechatronics) are often using more complex processes, with dedicated sub-processes for every discipline. However, seen from a high level standpoint, the various phases of the MBSE are the 4 we mentioned, leading some people to call the MBSE process a \"RFLP\" process (RFLP standing for Requirements, Functional, Logical, Physical). Currently, we find more and more people skipping the \"S\" of MBSE to talk about MBE, for model-based engineering. The term becomes larger because it does not have this embedded system connotation anymore. Maintaining the digital thread For sure, a RFLP process aims to create a digital \"thread\" or \"link\" between the various entities, at least between the requirements, the functions and the systems (plus all the connectivity semantics). The system entities are a solution matching the requirements, and so is their \"physical\" description; but this solution is the result of various levels of trade-off analysis. In other terms, solutions can (and will) evolve with time. The full digital thread of information will have to be maintained through changes. Changes in requirements (or operational conditions) will lead to changes in systems and changes in systems (like obsolescence) must be compliant with requirements. RFLP processes with many variable dimensions The RFLP processes that we can find in the industry are generally describing many parameters attached to the various elements (requirements, functions, systems, etc.): the performance, the price, the various criteria of manufacturability, the respect of regulation and standards are among the most important of parameters (the weight also in aerospace). Those parameters are crucial because, during the various design phases: They permit to use requirement quantification (with values and ranges) and not only requirement qualification; They enable to simulate the various elements working together (functional simulation, system simulation, or full product simulation in some known conditions); They enable complex trade-offs that lead to design the best product \"under a certain set of constraints\". Modeling in MBSE The topic is quite vast but we will, once again, try a simplification. Modeling can be done mainly with 3 different approaches: The first one is to do \"manual\" systems engineering with Office documents (Word and Excel mainly); This way of doing things is still very present in the industry; It is error prone, and very painful for the engineering teams; The second one is to use \"standard MBSE modeling language\" like SysML or Capella to run the \"RFL\" part of the MBSE process (the \"P\" part will be done with specific tools like 3D design tools or electricity engineering tools); The objective is to model graphically the \"RFL\" part of the process and to keep links with the \"P\" parts; The third one is to use a simulation/optimization-oriented domain-specific language (DSL, the domain being systems engineering) for requirements and systems like PMM or Form-L . In the first style of systems engineering, the digital thread is extremely difficult to maintain or trace, and generally, to be able to do it, a lot of manual and painful efforts are required. Most of the time, the requirements are vague, not measurable, and the trade-offs analysis are not recorded. With time, we see a growing consciousness of the advantages of the third solution to be able to do complex trade-offs very early in the product design phase (see the system engineering book of knowledge here ). MBE/MBSE seen from the military The MBE/MBSE modeling techniques are a way for military organizations to ensure that their requirements are correctly understood and are refined the right way. Especially, the requirements being tightly coupled with operational conditions (often called CONOPS for CONcept of OPerationS), the graphical modeling in a unified language is an enabler for common understanding, sharing of decisions in complex trade-offs and sharing of information. For sure, this is being true for the design phase of the system. About the complexity of modeling hybrid systems The fact is systems engineering is very adapted to design the early phases of a product and do the right structuring trade-offs. But the \"products\" of today are much more complex than the products of yesterday. If we take the sample of aerospace, the \"ground segment\", or the set of software that enable to manage the machines from the ground is much bigger and much more complex than before. Let us take a sample in a non military domain: the constellations of satellites. To be able to operate a constellation of more than 100 satellites in a crowded space, the ground segment complexity is very high. The thing is it is not easy to design the product (the satellite) as the set of software required to operate many instances of this very satellite. More and more military systems are facing the same kind of problems: they must tightly couple together the product definition and the product support design phases. How to ensure that all the requirements are taken in charge by all sub-systems in the global system, above all when the way of model management software systems is more related to IT techniques such as enterprise architecture, IT architecture or software architecture? Modeling enterprise architecture The enterprise architecture domain is very wide and our objective is not here to describe it extensively. We will just mention that this domain is mainly split in 2 concerns: What methodology should be used to create a good IT architecture serving the business process transformation according to the strategic intentions? What are the relevant views and artifacts to use to model the digital transformation, starting with the strategy, going through the business layer, the application layer and the infrastructure layer? There are many enterprise architecture methodology frameworks, but the most used nowadays is TOGAF . The most used enterprise architecture modeling language is Archimate (see our introductory article and our recommended recipes ). This language is particularly adapted to model most of the other dimensions of interest of the military project, especially the organizations, strategy, processes and IT systems, whatever the complexity. It proposes 13 basic viewpoints (see here ) but its global metamodel makes it adapted for many complex representations. Traditionally enterprise architecture is information systems oriented and aims at reintroducing the business common sense in IT. In order to tackle the strategic intentions of a company that wants to transform, process and IT transformation must be considered together as a system in order to avoid the 2 common traps: The biased process-driven project able to change the company processes but without any efficiency for the company (for instance by bringing more Excel spreadsheets, more blocking steps, multiple keying and bureaucracy than the company already has); The biased IT-driven project able to change the IT systems without being synchronized with new processes (and generating more troubles, data inconsistencies, process problems, double keying, etc.). Samples of those traps are numerous. For the first case, transformation projects led by process experts often lead to a huge loss of money because they create more Excel tools and often add layers of procedures to already overly complex environments. For the second case, a typical example is the \"tool project approach\": when users have a need, let's buy a tool and deploy it. Those two sample lead to the same result: inefficient, error prone and expensive processes, and inefficient, error prone and expensive IT systems. In one word: more recurring costs, less productivity and a future cost of change that increased. Enterprise architecture, by its way of considering the processes and IT systems together enables to perform the proper tradeoffs in projects and to plan a real digital transformation, meaning a process and IT systems joint transformation targeting a better efficiency (cost reduction or productivity enhancement). Various solutions for military projects Considering the military requirements of table 1 , let us dig into the various frameworks that enable to model military projects. The best of breed solutions If we look at the market and take the best of breed modeling solutions adapted to the requirements of table 1 , we could propose the solutions presented in Table 2. Table 2: Best of breed solutions for military programs # Dimension Solution 1 Product definition SysML systems engineering modeling language 2 Product support and operations Archimate enterprise architecture modeling language 3 Digital transformation Archimate enterprise architecture modeling language 4 Project plan Complex planning tools for programs (set of projects with dependencies) + Archimate with the implementation and migration package As we can see in the table 2, we have 3 ways of modeling the full program: SysML, Archimate and a planning tool. The 3 metamodels not being integrated together, we may have problems at the \"interfaces\" of each domain (see later in the article). SysML proposes 9 kinds of diagrams and Archimate 13 basic viewpoints, what corresponds to approximately 22 types of diagrams. If we had 2 or 3 viewpoints for planning, we would end up with a solution presenting around 25 viewpoints. The military frameworks DoDAF The military frameworks, for decades, defined a set of artifacts and views to have it all covered. The Figure 3 shows the structure of views of DoDAF version 2. Figure 3: DoDAF viewpoint structure (52 viewpoints) MoDAF The Figure 4 shows the structure of views of MoDAF. Figure 4: MoDAF viewpoint structure (46 viewpoints) NAF The Figure 5 shows the structure of views in NAF v4. Figure 5: NAF viewpoint structure (46 viewpoints) UAF An attempt of \"unification\" of those three frameworks was done by the OMG with the so-called Unified Architecture Framework ( UAF ). The Figure 6 shows the structure of views in UAF v1. Figure 6: UAF viewpoint structure (+70 viewpoints) Views and metamodel All frameworks propose a metamodel that is often very rich, and also very complex. With time, the metamodels got more abstract which makes their use quite difficult. Genericity, specialization and taxonomy The core advantage - and the core problem - of those frameworks is that they are generic . With the same semantics (or almost), it is possible to describe a project for a new military aircraft, or a project for a new military device or a project for a new IT application. The fact is that, in every of those projects, some notions can be called the same even if they don't cover the same reality. Let us take the sample of \"service\" (which is very emblematic of those frameworks). Some of those framework describe it as being a \"service in the broadest sense of the term\". An aircraft can provide a service, so is a military device, so is an application. For each of those \"products\", the notion of service will represent a different reality: the aircraft can provide a high level service to the army, the device a service to other devices and the IT application a service to a set of users. The connectivity between those services will depend on the context and on what we are talking about. If we imagine a more complex project such as an aircraft, its ground segment and all support processes, it will be very complex to represent all those various objects and their interconnectivity with generic concepts. To address this problem of \"specialization\" of very general concepts (or meta-concepts), the military frameworks propose the \"taxonomy\" approach. By defining for the various kind of views the appropriate taxonomy, the architects define the content of the views which removes the ambiguity. In this spirit, each project will \"instantiate\" the military architecture framework according to its needs: the taxonomy of artifacts will clarify the meaning of abstract artifacts in the context of the project. This way of working is very common in military projects where a \"guidance conference\" at project start defines the various implementation options that are taken by the project. This way of working can be quite easy in a simple project but can quickly become very complicated when the project is suppose to build several \"products\" of several kinds (which is the case in big military programs). For instance, to be able to create meaningful and useful views in a new aircraft project containing the aircraft, the ground segment and the support system, complex and different conventions will have to be taken for each big \"chunk\" of the program. But, if we specialize each part of the program with a specific meaning of artifacts per program part, why not using domain-specific modeling languages such as presented in the \"best of breed\" part above? Bridging several semantic domains together The advantage of having an integrated framework is the bridges between various domains. In a best of breed solution, the bridging are almost impossible unless the modeling tool is unifying several modeling languages (see later in the article). Bridges can be very important in the lifecycle of a program, because they can link the two parts of the V-model (as shown in figure 2 ). Samples are numerous and at the heart of many industrial projects: Interlinking product support to product definition enables a better product maturity; Interlinking all phases with an integrated planning is very interesting (product-based planning) especially if the product is an assembly of other subproducts; Interlinking digital transformation to the two branches of the V-model enables to prepare the efficient changes required in the organizations, their processes and their IT systems; Etc. When the modeling language and its metamodel enables to create those connections, the complexity of big programs can be described with many details [ 1 ]. A problem of semantics The problem, as we saw it in figure 2 , is that, under the military frameworks are \"concrete\" semantic domains underlying. We identified 4 semantic areas in table 1 and figure 1 and 2 : Product definition, Product support and operations, Digital transformation, Project management. We can add that the target of military frameworks is to support a certain kind of projects that are belonging to a limited number of categories: full device, partial device and IT systems, more or less. In those conditions, the advantages brought by the chosen framework must be balanced with its drawbacks. Among them we can name: A non natural way of modeling of already standardized semantic domains, which will make the information understanding and sharing quite difficult, even for experienced professionals; A complexity in the use of taxonomies and a risk of heterogeneity of framework interpretation, between various projects but also within the same project; The risk of introducing ambiguities that will have a negative impact on the project; The problem of finding appropriate skills to manipulate the complexity of the framework and its various interpretations; The cost of training people outside the domain-specific standards; The requirement of creating a tool implementing the metamodel of the framework (and maintaining it over time). Syncretism or unification? Evolutions and work-arounds It is important to note that, with time, the military frameworks opened to some modeling standards, while keeping their own core metamodel, which does not appear as fully consistent. Using Archimate for everything For instance, NAF v4 indicates that Archimate and UAF can be used for modeling. Clearly, Archimate can be used for product support and digital transformation (lines 2 and 3 of table 1 ) but it seems a bit light to model product definition (even with the physical package introduced in version 3). Concerning the project management Archimate has basic artifacts but that cannot enable the management of a complex program, in the way NAF or other military frameworks picture it. That means that Archimate can be used in NAF for certain kinds of projects, but cannot cover all that NAF intends to cover [ 2 ]. Figure 7 shows the classic use of Archimate which deliverables are processes and IT systems. Using SysML for everything Another option in the market is to consider that SysML can be used for everything. We propose to look at figure 7 a comparison between Archimate and SysML on an abstract RFLP process, declined respectively in a product definition process for SysML and on a set of processes and IT systems definition process for Archimate. Figure 7: SysML and Archimate comparison As we said earlier, the systems engineering area is introducing now more and more executable domain specific languages (DSL) to be able to simulate, optimize and perform early tradeoffs for system architecture. That is showing that SysML will never be the only systems engineering modeling language. Second of all, if we compare to Archimate, we can see basically that the two modeling languages are not targeting the same problem. As we said in the best of breed solution, each project which scope encompasses a physical product plus processes and IT would definitely need both approaches. Using UAF for everything UAF on the other hand pretends to cover it all. Its metamodel being an extension of SySML, it seems well suited for product definition. As the meta-model integrates many more artifacts, the coverage seems one of the most extensive of the market. The problem of NAF is, for us, the one we already described and that is attached to every military framework: in order to use it concretely, you have to instantiate it with the precise taxonomy of elements you will use. This is the way the military are doing things, but this way may not be adapted for every company. Using a custom architecture framework for everything Some companies are inventing their own modeling frameworks, quite often by sticking various semantically incompatible metamodels together, or by redefining from scratch a large set of artifacts in a brand new metamodel. In 25 years, we never saw this approach work: worse, we always saw this approach end badly. The drawbacks of this approach are numerous: The new framework is generally not documented enough, so architects are interpreting it in a chaotic and non consistent way; This implies that generally diagrams made by one architect are not understandable by another, which generates errors, misunderstanding and inconsistency in the rest of the chain; The new framework knowledge is concentrated in methodology gurus who know better ; They are on the critical path of everything and have, most of the time, no real experience in modeling on large scale projects (or they would not create a new metamodel on their own); They love methodology and metamodels but are not realizing that the quality of a metamodel is its semantic non ambiguity; They generally won't listen to any comment from operational people; The learning curve on the new framework is very long and complex, because there is not enough document, because the gurus are not reachable and because, as time passes, some errors in the metamodel are being corrected, which makes the old models incompatible with the new version of the framework; The new framework is generally based on a tool implementation with the metamodel; Generally, not everyone has access to the tool, which makes the adoption of the framework difficult for many users; There will always be experienced people that saw that the new framework is not working well (generally bringing semantic confusion) and that will keep on using standard methodologies and tools, that proved to be efficient and helpful in projects. This word is very important: helpful. The modeling framework aims to help architects to build better systems, not to glorify the ego of the gurus. The path of creating a new framework with a new metamodel is very dangerous and always ends badly, so we do not recommend it at all. It is a waste of time, energy and brains. That's why standardization groups were created: to put a lot of people with skills and large experience to define frameworks that work in many contexts and projects. You can look at our article on Archimate that explains why Archimate works . One or many? We have to say that we don't believe in unification, in the single modeling approach that covers any use case, in the \"one size fits all\" universal modeling language. That means we must use a syncretic approach: taking several modeling languages for what they do best. The problem of interlinking models from different modeling languages remains. It can be solved by two approaches: A tool based approach, integrating several metamodels together (with the risk of creating many semantic ambiguities by sticking together metamodels that have semantic overlap); A semantic approach where each modeling universe shares some concepts with other modeling universes, in an interoperability way (semantic web techniques can be very usefull in that area [ 3 ]). Figure 8: Sharing concepts between two modeling languages The second approach is shown in figure 8. Doing the exercise of concept sharing between different modeling languages is very healthy, because it will define what we want to really pilot at the project level. As the modeling languages are domain specific, they will be used at specific moments of the project and so the interfaces between the various modeling activities can be formalized. With the help of the common semantic concepts and relationships, we can define quite easily a maturity process of exchanges between the various poles of expertise of the project during all the phases of its execution. Conclusion Using military frameworks for military contracts When the military customer requires it, the use of those frameworks is mandatory. Here are pieces of advice for a pertinent use of those frameworks: Identify from the scope of the project what views are really necessary, what they represent, who will look at them and if they are here just to communicate between customer and supplier or if they have an operational impact; That means preparing a lot the guidance conference of the project start; In case of operational impact, try to see if the military framework cannot be mapped to another modeling language, better suited for the the particular views; propose this kind of adaptations in the guidance conference and maintain cautiously the taxonomy views explaining those mappings; Identify in the military frameworks the modeling intentions (artifacts and relationships) that are relevant to the project and have them translated in the language you propose to use; that would create a \"pattern library\" of important topics to keep in the radar, even if the chosen working modeling language is not proposing it; Don't hesitate to use inter-domain views, views located in the round rectangle of figure 8 ; keep track of that in the taxonomy; Be opened to concept duplication in various domains if needed but be sure to trace those choices and to create reconciliation views. In all cases, discuss, debate and negotiate in order to find the best compromise between the interests of the customer and the ones of the supplier. Big projects are successful when people share and work together on complexity. Using military frameworks in the industry Military frameworks are tuned to be used on the customer side: they rely on the hypothesis that the army is the customer and the industrial company (or companies) is (or are) the supplier(s). Table 1 explains the foundations of the military concerns. Military preoccupations are very concrete: features and performance, money, schedule, operational model. Those concerns are not the ones of the supplier company, as shown in Figure 2 in the V-model. The supplier company would target an integration at other levels to be able to perform other tradeoffs than the military, who is often an important customer but not the only one. All military frameworks, even UAF, are the result of the works and experience of the military organizations trying to manage projects of billions of dollars/euros. Those military frameworks are born from the military as a customer constraints. When the set of constraints is different, which is the case for the supplier company as an industrial company, the framework needed will not be the same. The industrial world becoming more and more complex, many industrial companies are searching to define or reuse big modeling frameworks to ensure that every aspect of theirs problems is covered, and that they can do all the panels of their required tradeoffs. Currently, in the market, there seem to be no obvious integrated enterprise framework suiting the full range of their needs. For the industry currently and unfortunately, syncretism of modeling languages seems the only solution. Notes [1] On the other hand, concerning the product lifecycle and specifically the links between engineering and support, some other industry-specific standards are covering the process aspect of such a challenge: the ASD standards - Back to text . [2] See the first NAF to Archimate mapping in the online article of Mark Lankhorst - Back to text . [3] Please see also the extensive works of Nicolas Figay on PLM interoperability and Archimate models interoperability - Back to text . ( November 2019 )","title":"Military frameworks, systems engineering and enterprise architecture"},{"location":"articles/mbse-vs-ea/#military-frameworks-systems-engineering-and-enterprise-architecture","text":"(Courtesy of Pixaby on https://www.pexels.com) In the world of architecture modeling, the military organizations like the US Department of Defense (DoD), the UK Ministry of Defence (MoD) or the NATO organization created enterprise architecture frameworks adapted to their needs. On the market today, 3 main frameworks are existing and used in military projects: DoDAF , the US MoD Architecture Framework, that was historically the first military architecture framework ever (it is originated in the 90s); MoDAF , the UK MoD architecture framework, which is both an adaptation and an abstraction of DoDAF; NAF , the NATO Architecture Framework that is also an adaptation and an abstraction of DoDAF and MoDAF.","title":"Military frameworks, systems engineering and enterprise architecture"},{"location":"articles/mbse-vs-ea/#objectives-and-dimensions-of-the-military-frameworks","text":"The main objective searched by military organization is to be able to master a global project of several billions of dollars/euros from the design phase to the support and operational phase. Because those projects/programs can be very large and very complex, the idea was that everyone (military organization and supplier organizations) share the same set of \"views\" on the program in order to share understanding of what was to be done, how requirements were taken into account, what is the full lifecycle of the product and how the project should be managed. Table 1: Structure of military requirements # Dimension Details 1 Product definition Can be an aircraft, weapon, complex system, etc. and all the related IT systems to manage them. The objective is to use a systems engineering approach (model-based in that case), based on requirements. 2 Product support and operations The objective of this phase is to detail all the systems that will permit the support of the product. This is generally a wide topic including maintenance, parts procurement, etc. Moreover, the product use must be envisaged in operations preparation and in battle field in coordination with other systems and organizations. 3 Digital transformation The objective of this phase is to evaluate and deal with the impacts of the product and product support in the operational organization, processes and IT of the army. 4 Project plan The objective is to be able to describe the the plan to deliver the product conforming to the requirements (phases, milestones and so on), to permit project steering, and product support procurement. We can classify the military enterprise architecture frameworks as pursuing 4 objectives, as shown in table 1.","title":"Objectives and dimensions of the military frameworks"},{"location":"articles/mbse-vs-ea/#modeling-domains","text":"Those 4 objectives are traditionally modeled with 3 different modeling techniques: Systems engineering for the product definition, Enterprise architecture for: Product support, Digital transformation, Project modeling tools for project management. Figure 1: The 3 domains Military framework concerns in the industrial V-model The figure 1 shows the 3 modeling modeling domains that they are traditionally covered in military frameworks. In terms of modeling, we could say that the core objective of the military frameworks is to have, in a single repository, the systems engineering (MBE/MBSE), the enterprise architecture and the project management gathered together .","title":"Modeling domains"},{"location":"articles/mbse-vs-ea/#positioning-in-the-v-model","text":"Those 4 areas of concern can be mapped on an industrial V-model as shown in Figure 2. Figure 2: Military framework concerns in the industrial V-model The first dimension, product definition, is in the left part of the V and corresponds to the product architecture, from its operational conditions, requirements to its system design. The second dimension is corresponding to product support (we often speak about \"support concept\"). The two last dimensions are running all along the project, being at the project/program level or at the digital transformation level (integrate the new product into the current organizations). We can see in Figure 2 how an integrated modeling of those two highest branches of the V-model can be important for the military customer: It ensures a full lifecycle modeling of the product, It enables simulation scenarios linked to the requirement phase, It enables to anticipate the impact of the new project on army's existing organizations, It enables to manage the project/program. Most of all, on top of being a common modeling language, it enables complex tradeoffs, including performance, finance, supportability, etc.","title":"Positioning in the V-model"},{"location":"articles/mbse-vs-ea/#mbsembe","text":"","title":"MBSE/MBE"},{"location":"articles/mbse-vs-ea/#the-rflp-process","text":"In manufacturing, the objective is to create products. The starting point of a product manufacturing is generally a set of requirements. To go from the requirements to the product is generally a long process involving the following phases: Requirements engineering : The objective of this phase is to work on requirements in order to make them non ambiguous, to sort them by importance and to remove inconsistency between them; In this phase, all the operational conditions of the product are described, which is a crucial phase of the process; Functional analysis : This phase objective is to determine the functions of the product, the information flows and dependencies between functions; Generally, the functions are worked at different levels of granularity (functions, sub-functions, sub-sub-functions and so on); Logical system analysis : Once the various functions at various levels are known, the assignment of functions to logical elements can be done; The objective of this phase is to create the appropriate logical systems to get the nearest possible from a possible industrial split of the realization of the product; When the logical analysis is finished, we have the detailed specification of each system ; Physical design : Once the product has been split on several components, several teams can work in parallel on component design (at the level of granularity of the component); The product architecture will ensure that all detailed components fit together conforming to the specifications. This view is, for sure, simplified and some industries (or specific businesses inside the industries like embedded software, electronics, or mechatronics) are often using more complex processes, with dedicated sub-processes for every discipline. However, seen from a high level standpoint, the various phases of the MBSE are the 4 we mentioned, leading some people to call the MBSE process a \"RFLP\" process (RFLP standing for Requirements, Functional, Logical, Physical). Currently, we find more and more people skipping the \"S\" of MBSE to talk about MBE, for model-based engineering. The term becomes larger because it does not have this embedded system connotation anymore.","title":"The RFLP process"},{"location":"articles/mbse-vs-ea/#maintaining-the-digital-thread","text":"For sure, a RFLP process aims to create a digital \"thread\" or \"link\" between the various entities, at least between the requirements, the functions and the systems (plus all the connectivity semantics). The system entities are a solution matching the requirements, and so is their \"physical\" description; but this solution is the result of various levels of trade-off analysis. In other terms, solutions can (and will) evolve with time. The full digital thread of information will have to be maintained through changes. Changes in requirements (or operational conditions) will lead to changes in systems and changes in systems (like obsolescence) must be compliant with requirements.","title":"Maintaining the digital thread"},{"location":"articles/mbse-vs-ea/#rflp-processes-with-many-variable-dimensions","text":"The RFLP processes that we can find in the industry are generally describing many parameters attached to the various elements (requirements, functions, systems, etc.): the performance, the price, the various criteria of manufacturability, the respect of regulation and standards are among the most important of parameters (the weight also in aerospace). Those parameters are crucial because, during the various design phases: They permit to use requirement quantification (with values and ranges) and not only requirement qualification; They enable to simulate the various elements working together (functional simulation, system simulation, or full product simulation in some known conditions); They enable complex trade-offs that lead to design the best product \"under a certain set of constraints\".","title":"RFLP processes with many variable dimensions"},{"location":"articles/mbse-vs-ea/#modeling-in-mbse","text":"The topic is quite vast but we will, once again, try a simplification. Modeling can be done mainly with 3 different approaches: The first one is to do \"manual\" systems engineering with Office documents (Word and Excel mainly); This way of doing things is still very present in the industry; It is error prone, and very painful for the engineering teams; The second one is to use \"standard MBSE modeling language\" like SysML or Capella to run the \"RFL\" part of the MBSE process (the \"P\" part will be done with specific tools like 3D design tools or electricity engineering tools); The objective is to model graphically the \"RFL\" part of the process and to keep links with the \"P\" parts; The third one is to use a simulation/optimization-oriented domain-specific language (DSL, the domain being systems engineering) for requirements and systems like PMM or Form-L . In the first style of systems engineering, the digital thread is extremely difficult to maintain or trace, and generally, to be able to do it, a lot of manual and painful efforts are required. Most of the time, the requirements are vague, not measurable, and the trade-offs analysis are not recorded. With time, we see a growing consciousness of the advantages of the third solution to be able to do complex trade-offs very early in the product design phase (see the system engineering book of knowledge here ).","title":"Modeling in MBSE"},{"location":"articles/mbse-vs-ea/#mbembse-seen-from-the-military","text":"The MBE/MBSE modeling techniques are a way for military organizations to ensure that their requirements are correctly understood and are refined the right way. Especially, the requirements being tightly coupled with operational conditions (often called CONOPS for CONcept of OPerationS), the graphical modeling in a unified language is an enabler for common understanding, sharing of decisions in complex trade-offs and sharing of information. For sure, this is being true for the design phase of the system.","title":"MBE/MBSE seen from the military"},{"location":"articles/mbse-vs-ea/#about-the-complexity-of-modeling-hybrid-systems","text":"The fact is systems engineering is very adapted to design the early phases of a product and do the right structuring trade-offs. But the \"products\" of today are much more complex than the products of yesterday. If we take the sample of aerospace, the \"ground segment\", or the set of software that enable to manage the machines from the ground is much bigger and much more complex than before. Let us take a sample in a non military domain: the constellations of satellites. To be able to operate a constellation of more than 100 satellites in a crowded space, the ground segment complexity is very high. The thing is it is not easy to design the product (the satellite) as the set of software required to operate many instances of this very satellite. More and more military systems are facing the same kind of problems: they must tightly couple together the product definition and the product support design phases. How to ensure that all the requirements are taken in charge by all sub-systems in the global system, above all when the way of model management software systems is more related to IT techniques such as enterprise architecture, IT architecture or software architecture?","title":"About the complexity of modeling hybrid systems"},{"location":"articles/mbse-vs-ea/#modeling-enterprise-architecture","text":"The enterprise architecture domain is very wide and our objective is not here to describe it extensively. We will just mention that this domain is mainly split in 2 concerns: What methodology should be used to create a good IT architecture serving the business process transformation according to the strategic intentions? What are the relevant views and artifacts to use to model the digital transformation, starting with the strategy, going through the business layer, the application layer and the infrastructure layer? There are many enterprise architecture methodology frameworks, but the most used nowadays is TOGAF . The most used enterprise architecture modeling language is Archimate (see our introductory article and our recommended recipes ). This language is particularly adapted to model most of the other dimensions of interest of the military project, especially the organizations, strategy, processes and IT systems, whatever the complexity. It proposes 13 basic viewpoints (see here ) but its global metamodel makes it adapted for many complex representations. Traditionally enterprise architecture is information systems oriented and aims at reintroducing the business common sense in IT. In order to tackle the strategic intentions of a company that wants to transform, process and IT transformation must be considered together as a system in order to avoid the 2 common traps: The biased process-driven project able to change the company processes but without any efficiency for the company (for instance by bringing more Excel spreadsheets, more blocking steps, multiple keying and bureaucracy than the company already has); The biased IT-driven project able to change the IT systems without being synchronized with new processes (and generating more troubles, data inconsistencies, process problems, double keying, etc.). Samples of those traps are numerous. For the first case, transformation projects led by process experts often lead to a huge loss of money because they create more Excel tools and often add layers of procedures to already overly complex environments. For the second case, a typical example is the \"tool project approach\": when users have a need, let's buy a tool and deploy it. Those two sample lead to the same result: inefficient, error prone and expensive processes, and inefficient, error prone and expensive IT systems. In one word: more recurring costs, less productivity and a future cost of change that increased. Enterprise architecture, by its way of considering the processes and IT systems together enables to perform the proper tradeoffs in projects and to plan a real digital transformation, meaning a process and IT systems joint transformation targeting a better efficiency (cost reduction or productivity enhancement).","title":"Modeling enterprise architecture"},{"location":"articles/mbse-vs-ea/#various-solutions-for-military-projects","text":"Considering the military requirements of table 1 , let us dig into the various frameworks that enable to model military projects.","title":"Various solutions for military projects"},{"location":"articles/mbse-vs-ea/#the-best-of-breed-solutions","text":"If we look at the market and take the best of breed modeling solutions adapted to the requirements of table 1 , we could propose the solutions presented in Table 2. Table 2: Best of breed solutions for military programs # Dimension Solution 1 Product definition SysML systems engineering modeling language 2 Product support and operations Archimate enterprise architecture modeling language 3 Digital transformation Archimate enterprise architecture modeling language 4 Project plan Complex planning tools for programs (set of projects with dependencies) + Archimate with the implementation and migration package As we can see in the table 2, we have 3 ways of modeling the full program: SysML, Archimate and a planning tool. The 3 metamodels not being integrated together, we may have problems at the \"interfaces\" of each domain (see later in the article). SysML proposes 9 kinds of diagrams and Archimate 13 basic viewpoints, what corresponds to approximately 22 types of diagrams. If we had 2 or 3 viewpoints for planning, we would end up with a solution presenting around 25 viewpoints.","title":"The best of breed solutions"},{"location":"articles/mbse-vs-ea/#the-military-frameworks","text":"","title":"The military frameworks"},{"location":"articles/mbse-vs-ea/#dodaf","text":"The military frameworks, for decades, defined a set of artifacts and views to have it all covered. The Figure 3 shows the structure of views of DoDAF version 2. Figure 3: DoDAF viewpoint structure (52 viewpoints)","title":"DoDAF"},{"location":"articles/mbse-vs-ea/#modaf","text":"The Figure 4 shows the structure of views of MoDAF. Figure 4: MoDAF viewpoint structure (46 viewpoints)","title":"MoDAF"},{"location":"articles/mbse-vs-ea/#naf","text":"The Figure 5 shows the structure of views in NAF v4. Figure 5: NAF viewpoint structure (46 viewpoints)","title":"NAF"},{"location":"articles/mbse-vs-ea/#uaf","text":"An attempt of \"unification\" of those three frameworks was done by the OMG with the so-called Unified Architecture Framework ( UAF ). The Figure 6 shows the structure of views in UAF v1. Figure 6: UAF viewpoint structure (+70 viewpoints)","title":"UAF"},{"location":"articles/mbse-vs-ea/#views-and-metamodel","text":"All frameworks propose a metamodel that is often very rich, and also very complex. With time, the metamodels got more abstract which makes their use quite difficult.","title":"Views and metamodel"},{"location":"articles/mbse-vs-ea/#genericity-specialization-and-taxonomy","text":"The core advantage - and the core problem - of those frameworks is that they are generic . With the same semantics (or almost), it is possible to describe a project for a new military aircraft, or a project for a new military device or a project for a new IT application. The fact is that, in every of those projects, some notions can be called the same even if they don't cover the same reality. Let us take the sample of \"service\" (which is very emblematic of those frameworks). Some of those framework describe it as being a \"service in the broadest sense of the term\". An aircraft can provide a service, so is a military device, so is an application. For each of those \"products\", the notion of service will represent a different reality: the aircraft can provide a high level service to the army, the device a service to other devices and the IT application a service to a set of users. The connectivity between those services will depend on the context and on what we are talking about. If we imagine a more complex project such as an aircraft, its ground segment and all support processes, it will be very complex to represent all those various objects and their interconnectivity with generic concepts. To address this problem of \"specialization\" of very general concepts (or meta-concepts), the military frameworks propose the \"taxonomy\" approach. By defining for the various kind of views the appropriate taxonomy, the architects define the content of the views which removes the ambiguity. In this spirit, each project will \"instantiate\" the military architecture framework according to its needs: the taxonomy of artifacts will clarify the meaning of abstract artifacts in the context of the project. This way of working is very common in military projects where a \"guidance conference\" at project start defines the various implementation options that are taken by the project. This way of working can be quite easy in a simple project but can quickly become very complicated when the project is suppose to build several \"products\" of several kinds (which is the case in big military programs). For instance, to be able to create meaningful and useful views in a new aircraft project containing the aircraft, the ground segment and the support system, complex and different conventions will have to be taken for each big \"chunk\" of the program. But, if we specialize each part of the program with a specific meaning of artifacts per program part, why not using domain-specific modeling languages such as presented in the \"best of breed\" part above?","title":"Genericity, specialization and taxonomy"},{"location":"articles/mbse-vs-ea/#bridging-several-semantic-domains-together","text":"The advantage of having an integrated framework is the bridges between various domains. In a best of breed solution, the bridging are almost impossible unless the modeling tool is unifying several modeling languages (see later in the article). Bridges can be very important in the lifecycle of a program, because they can link the two parts of the V-model (as shown in figure 2 ). Samples are numerous and at the heart of many industrial projects: Interlinking product support to product definition enables a better product maturity; Interlinking all phases with an integrated planning is very interesting (product-based planning) especially if the product is an assembly of other subproducts; Interlinking digital transformation to the two branches of the V-model enables to prepare the efficient changes required in the organizations, their processes and their IT systems; Etc. When the modeling language and its metamodel enables to create those connections, the complexity of big programs can be described with many details [ 1 ].","title":"Bridging several semantic domains together"},{"location":"articles/mbse-vs-ea/#a-problem-of-semantics","text":"The problem, as we saw it in figure 2 , is that, under the military frameworks are \"concrete\" semantic domains underlying. We identified 4 semantic areas in table 1 and figure 1 and 2 : Product definition, Product support and operations, Digital transformation, Project management. We can add that the target of military frameworks is to support a certain kind of projects that are belonging to a limited number of categories: full device, partial device and IT systems, more or less. In those conditions, the advantages brought by the chosen framework must be balanced with its drawbacks. Among them we can name: A non natural way of modeling of already standardized semantic domains, which will make the information understanding and sharing quite difficult, even for experienced professionals; A complexity in the use of taxonomies and a risk of heterogeneity of framework interpretation, between various projects but also within the same project; The risk of introducing ambiguities that will have a negative impact on the project; The problem of finding appropriate skills to manipulate the complexity of the framework and its various interpretations; The cost of training people outside the domain-specific standards; The requirement of creating a tool implementing the metamodel of the framework (and maintaining it over time).","title":"A problem of semantics"},{"location":"articles/mbse-vs-ea/#syncretism-or-unification","text":"","title":"Syncretism or unification?"},{"location":"articles/mbse-vs-ea/#evolutions-and-work-arounds","text":"It is important to note that, with time, the military frameworks opened to some modeling standards, while keeping their own core metamodel, which does not appear as fully consistent.","title":"Evolutions and work-arounds"},{"location":"articles/mbse-vs-ea/#using-archimate-for-everything","text":"For instance, NAF v4 indicates that Archimate and UAF can be used for modeling. Clearly, Archimate can be used for product support and digital transformation (lines 2 and 3 of table 1 ) but it seems a bit light to model product definition (even with the physical package introduced in version 3). Concerning the project management Archimate has basic artifacts but that cannot enable the management of a complex program, in the way NAF or other military frameworks picture it. That means that Archimate can be used in NAF for certain kinds of projects, but cannot cover all that NAF intends to cover [ 2 ]. Figure 7 shows the classic use of Archimate which deliverables are processes and IT systems.","title":"Using Archimate for everything"},{"location":"articles/mbse-vs-ea/#using-sysml-for-everything","text":"Another option in the market is to consider that SysML can be used for everything. We propose to look at figure 7 a comparison between Archimate and SysML on an abstract RFLP process, declined respectively in a product definition process for SysML and on a set of processes and IT systems definition process for Archimate. Figure 7: SysML and Archimate comparison As we said earlier, the systems engineering area is introducing now more and more executable domain specific languages (DSL) to be able to simulate, optimize and perform early tradeoffs for system architecture. That is showing that SysML will never be the only systems engineering modeling language. Second of all, if we compare to Archimate, we can see basically that the two modeling languages are not targeting the same problem. As we said in the best of breed solution, each project which scope encompasses a physical product plus processes and IT would definitely need both approaches.","title":"Using SysML for everything"},{"location":"articles/mbse-vs-ea/#using-uaf-for-everything","text":"UAF on the other hand pretends to cover it all. Its metamodel being an extension of SySML, it seems well suited for product definition. As the meta-model integrates many more artifacts, the coverage seems one of the most extensive of the market. The problem of NAF is, for us, the one we already described and that is attached to every military framework: in order to use it concretely, you have to instantiate it with the precise taxonomy of elements you will use. This is the way the military are doing things, but this way may not be adapted for every company.","title":"Using UAF for everything"},{"location":"articles/mbse-vs-ea/#using-a-custom-architecture-framework-for-everything","text":"Some companies are inventing their own modeling frameworks, quite often by sticking various semantically incompatible metamodels together, or by redefining from scratch a large set of artifacts in a brand new metamodel. In 25 years, we never saw this approach work: worse, we always saw this approach end badly. The drawbacks of this approach are numerous: The new framework is generally not documented enough, so architects are interpreting it in a chaotic and non consistent way; This implies that generally diagrams made by one architect are not understandable by another, which generates errors, misunderstanding and inconsistency in the rest of the chain; The new framework knowledge is concentrated in methodology gurus who know better ; They are on the critical path of everything and have, most of the time, no real experience in modeling on large scale projects (or they would not create a new metamodel on their own); They love methodology and metamodels but are not realizing that the quality of a metamodel is its semantic non ambiguity; They generally won't listen to any comment from operational people; The learning curve on the new framework is very long and complex, because there is not enough document, because the gurus are not reachable and because, as time passes, some errors in the metamodel are being corrected, which makes the old models incompatible with the new version of the framework; The new framework is generally based on a tool implementation with the metamodel; Generally, not everyone has access to the tool, which makes the adoption of the framework difficult for many users; There will always be experienced people that saw that the new framework is not working well (generally bringing semantic confusion) and that will keep on using standard methodologies and tools, that proved to be efficient and helpful in projects. This word is very important: helpful. The modeling framework aims to help architects to build better systems, not to glorify the ego of the gurus. The path of creating a new framework with a new metamodel is very dangerous and always ends badly, so we do not recommend it at all. It is a waste of time, energy and brains. That's why standardization groups were created: to put a lot of people with skills and large experience to define frameworks that work in many contexts and projects. You can look at our article on Archimate that explains why Archimate works .","title":"Using a custom architecture framework for everything"},{"location":"articles/mbse-vs-ea/#one-or-many","text":"We have to say that we don't believe in unification, in the single modeling approach that covers any use case, in the \"one size fits all\" universal modeling language. That means we must use a syncretic approach: taking several modeling languages for what they do best. The problem of interlinking models from different modeling languages remains. It can be solved by two approaches: A tool based approach, integrating several metamodels together (with the risk of creating many semantic ambiguities by sticking together metamodels that have semantic overlap); A semantic approach where each modeling universe shares some concepts with other modeling universes, in an interoperability way (semantic web techniques can be very usefull in that area [ 3 ]). Figure 8: Sharing concepts between two modeling languages The second approach is shown in figure 8. Doing the exercise of concept sharing between different modeling languages is very healthy, because it will define what we want to really pilot at the project level. As the modeling languages are domain specific, they will be used at specific moments of the project and so the interfaces between the various modeling activities can be formalized. With the help of the common semantic concepts and relationships, we can define quite easily a maturity process of exchanges between the various poles of expertise of the project during all the phases of its execution.","title":"One or many?"},{"location":"articles/mbse-vs-ea/#conclusion","text":"","title":"Conclusion"},{"location":"articles/mbse-vs-ea/#using-military-frameworks-for-military-contracts","text":"When the military customer requires it, the use of those frameworks is mandatory. Here are pieces of advice for a pertinent use of those frameworks: Identify from the scope of the project what views are really necessary, what they represent, who will look at them and if they are here just to communicate between customer and supplier or if they have an operational impact; That means preparing a lot the guidance conference of the project start; In case of operational impact, try to see if the military framework cannot be mapped to another modeling language, better suited for the the particular views; propose this kind of adaptations in the guidance conference and maintain cautiously the taxonomy views explaining those mappings; Identify in the military frameworks the modeling intentions (artifacts and relationships) that are relevant to the project and have them translated in the language you propose to use; that would create a \"pattern library\" of important topics to keep in the radar, even if the chosen working modeling language is not proposing it; Don't hesitate to use inter-domain views, views located in the round rectangle of figure 8 ; keep track of that in the taxonomy; Be opened to concept duplication in various domains if needed but be sure to trace those choices and to create reconciliation views. In all cases, discuss, debate and negotiate in order to find the best compromise between the interests of the customer and the ones of the supplier. Big projects are successful when people share and work together on complexity.","title":"Using military frameworks for military contracts"},{"location":"articles/mbse-vs-ea/#using-military-frameworks-in-the-industry","text":"Military frameworks are tuned to be used on the customer side: they rely on the hypothesis that the army is the customer and the industrial company (or companies) is (or are) the supplier(s). Table 1 explains the foundations of the military concerns. Military preoccupations are very concrete: features and performance, money, schedule, operational model. Those concerns are not the ones of the supplier company, as shown in Figure 2 in the V-model. The supplier company would target an integration at other levels to be able to perform other tradeoffs than the military, who is often an important customer but not the only one. All military frameworks, even UAF, are the result of the works and experience of the military organizations trying to manage projects of billions of dollars/euros. Those military frameworks are born from the military as a customer constraints. When the set of constraints is different, which is the case for the supplier company as an industrial company, the framework needed will not be the same. The industrial world becoming more and more complex, many industrial companies are searching to define or reuse big modeling frameworks to ensure that every aspect of theirs problems is covered, and that they can do all the panels of their required tradeoffs. Currently, in the market, there seem to be no obvious integrated enterprise framework suiting the full range of their needs. For the industry currently and unfortunately, syncretism of modeling languages seems the only solution.","title":"Using military frameworks in the industry"},{"location":"articles/mbse-vs-ea/#notes","text":"[1] On the other hand, concerning the product lifecycle and specifically the links between engineering and support, some other industry-specific standards are covering the process aspect of such a challenge: the ASD standards - Back to text . [2] See the first NAF to Archimate mapping in the online article of Mark Lankhorst - Back to text . [3] Please see also the extensive works of Nicolas Figay on PLM interoperability and Archimate models interoperability - Back to text . ( November 2019 )","title":"Notes"},{"location":"articles/org-antipatterns/","text":"IT organization anti-patterns An IT organization is a fragile equilibrium that may malfunction due to many causes. As this organization is, most often, at the center of the digital transformation of enterprises, it can be important to identify the various organizational anti-patterns in order to assess the various risks of each of them. In this article, we will take a bias perspective which is based on the following assumptions concerning the role of an IT organization: The capability to operate IT services efficiently for end users; The capability to answer to IT-related requirements; The capability of managing the IT-related assets. The third point is particularly important: The IT department manages a set of assets, being hardware and software...","title":"IT organization anti-patterns"},{"location":"articles/org-antipatterns/#it-organization-anti-patterns","text":"An IT organization is a fragile equilibrium that may malfunction due to many causes. As this organization is, most often, at the center of the digital transformation of enterprises, it can be important to identify the various organizational anti-patterns in order to assess the various risks of each of them. In this article, we will take a bias perspective which is based on the following assumptions concerning the role of an IT organization: The capability to operate IT services efficiently for end users; The capability to answer to IT-related requirements; The capability of managing the IT-related assets. The third point is particularly important: The IT department manages a set of assets, being hardware and software...","title":"IT organization anti-patterns"},{"location":"articles/portfolio/","text":"A Simple Meta-Model for Portfolio Management Image courtesy of freedigitalphotos.net . One of the main problems of companies is to manage many IT projects at the same time. Most often, the companies facing this problem are managing project portfolios. The main drawback of managing several projects in parallel is that is is not easy to: Understand the dependencies of the various projects together, and so the order in which they should be led; Identify and deal with the various scope overlaps between the various projects; Manage the relationship with the business people that want the various projects to happen. This article presents a very simple method to use to address all those points. We will present the original version of the meta-model. An Archimate version of this model will be presented in another article. This method mostly targets enterprise or IT architects, program managers or project portfolio managers. Due to its simplicity, many other profiles can use it. Meta-Model Presentation The first objective is to formalize the client of the IT project. Each organization has several departments in which we have several different customers. Our first artifact type will be Organization . We will assume that every organization can include several other organizations. Each organization will also have a customer name as an attribute. When the projects intentions are declared, the materials that are produced by the various customers inside the same organization will generally be a list of many projects, with a project name and a more or less developed functional description. We will define two artifact types to model that: Project to model the project, Function to model the functionality. In the existing IT systems, we have applications. With the projects to come, some of the current application will get new functions, some new applications will be created and some other will be decommissioned. We need one more artifact type: Application that models the application; if the application is new, we need a way to tag it as \"new\" or \"existing\". The Function artifact type will be used to attach functionality to Application s. For this purpose, we need an attribute on the Function that indicates if the function is already existing, if it is new or if it must evolve. As applications are part of a big part of the IT systems (for instance the financial systems), we will attach the application to an application domain; we will use the Domain artifact type for that purpose. In terms of relationships, we have the following semantics: An Organization can contain other Organization s; An Organization will be the customer of several Project s; Project s are aggregating Function s; Application s are attached to Domain s and Function s are attached to them. The resulting meta-model is presented on the following figure. Methodology Let's suppose we use this model during the budgeting process that usually happens once a year in companies. Budget time is often the time for difficult choices. The following method is easy to use, can neutralize affects and help the top level management to choose between projects and to create a feasible roadmap within the limits of a feasible budget. Step 1: Accept All Project Requests and Identify Functions The first step is consisting in gathering all projects intentions from all organizations. We will analyze organization per organization. For each customer within a particular organization, the project intentions are gathered. The objective is to get the best functional description of the requirements possible and to identify the required functions. Very often, the requirements are a list of features (warning: some of them may already have been implemented without the customer to know about it). In that phase, the customer must not be challenged, but the architect should try his best to understand the functional (and non functional) requirements. To do that, it can be necessary to distinguish between the solution imagined by the customer and the original requirement. As the model is simple, the requirement artifact does not exist in this version. The identification of the required Function s will conclude this phase. Once this phase is complete, we can promise the customer to: Analyze his requirements along with all other requirements and projects and do our best to develop what is required; Provide a detailed feedback on the demands when all demands have been captured. Step 2: Identify Common Functions to Highlight Dependencies Several customers could have defined different projects that are using the same subset of functions. Identifying those functions is very important, this for several reasons: To avoid several projects to develop several times the same function, or worse, variations of the same function; To identify in what projects this function can be required and to scope it carefully; Developing a reusable function will ease the success of many projects whereas not developing it may cause project delays or failure; To add this function early in the roadmap; To take a special care to carefully place this function at the proper spot in the rest of the IT systems (step 3); Indeed, a misplaced function will generate many useless client/server calls, will generate consistency problems and will generate more integration and QA works: Economically, function placement is crucial. Functions, once identified, should be assigned with a number characterizing the number of projects that require them. The highest number will lead the global roadmapping exercise. We will call this number the dependency number . Step 3: Associate Functions to Applications This is a back office step that will be done between IT people, most of the time by architects. It is time to review the main functions of the existing applications to be accurate in the exercise. Functions will be classified into 3 categories: Existing functions : We can plan to go back to the customer to assess if we missed something in the original requirement and if the existing function should not evolve. New functions that can be naturally attached to an existing application : Some of the functions will naturally find their place as an evolution of an existing application; New functions that don't seem to be a natural evolution of the existing applications : However, some new functions have to be studied in order to determine their best spot in the IT systems (or their \"less worst\" spot). The two last steps can give birth to more advanced architecture studies. Covering this topic is not in the scope of this article, but we can insist on leading careful analysis, because function misplacement is a problem that will cause painful and costly problems for years if not for decades. All the functions that are not in existing applications should be gathered and analyzed as a whole to determine the opportunity of developing new applications. Step 4: Analyze the Roadmap of Each Application, Size it Roughly and Compare With The Skills Available Once all functions were assigned to applications, we can size them roughly and have a view of the roadmap of each application. For sure, this roadmap is still theoretical and the pure consequence of the customer's requirements. The Case of Application Maintenance and Evolution All functions of an existing application can be prioritized in the best order of feasibility to create a first credible application roadmap, that is compliant with technical constraints. At this stage, we can also identify the functions that are nice to have compared to the ones that are must haves. This classification, even if it can be debated with the business may lead to some negotiation margins during the year to come. The skills available for each application is important because it will determine the real feasibility of the roadmap per application: We mean IT skills (project manager, product owner, developer, QA) but also business skills. Indeed, in that phase, we can identify the bottlenecks in terms of IT people, when not enough people are skilled to develop the expected application roadmap. Depending on the global priorities of the company, it is possible in that phase to plan people transfer, trainings or recruitments. The Case of New Applications Those projects are always more risky than the previous ones. In some cases, it is obvious for the customer that the new requirements will lead to a new application. In some other cases, it is not. Those cases must be dealt with carefully because the customer may argue that the new requirements are part of an existing application. In those cases, the objective is to avoid function misplacement. However, it is more difficult to create a new application: The project manager has to be found, and she/he has to be able to lead from-scratch projects; The team has to be found, or in the existing people (staff or consultants), or in new comers; The business expert(s) has to be named. Some business people may be a bit worried about the application creation projects because they cause HR troubles and they can face more delays or failures than evolution projects. End of the Phase At the end of the step 4, we should have: All functions identified; All application roadmap sized; All HR requirements. Step 5: Introduction of Dependencies to Create Project Visions To determine which projects to do first in the project portfolio, we need to take the reusable functions with the highest dependency number, in order to schedule them first. Then, the purpose is to play with the constraints to determine the less worst global path considering: The list of projects to develop; The skills available; The potential HR adjustments (people moving from one project to another, new people). Generally, the functions with the highest dependency number will drive the roadmap. That means that a function will be develop if and only if it is in the same project than a function with a high dependency number. That means also that generally a lot of new functions required by the business will not be able to be developed in the coming year, just because they are not part of an \"important\" project. Indeed, this method enables to show to the top level management what is important to the company throughout the functions that are required by many projects (i.e. that have the highest dependency numbers). Once everything is done, the exercise can be redone if the budget is bigger or lower in some parts. Limits of the Method We can argue that the functions with the highest dependency numbers may not be the most crucial functions. That can be true. We can use some tricks in using several levels of functions if needed. Indeed, a function (level 1) required by a certain project could be divided into 3 functions of a smaller granularity (level 2). The level 2 functions could be the functions implemented by the applications. With this model adjustment, we can have a high level view of the function with \"project functions\" (level 1) and an IT view of functions (level 2). The artifact type Function must then be able to aggregate a list of Function s. We let the reader update the meta-model. What Tools Can Help? Yed When this method was used for the first time, Yed was the tool used. A Yed model was realized per organization. The advantage of Yed is its simplicity and its powerful set of graph layout algorithms. The drawback of Yed is when the number of projects is big (we had to deal with the Digital Division and their 130 projects), the model is hardly usable. EMF and Sirius Indeed, when the number of projects is high, a multiple view design tool is required. A possible option is to create and EMF model and to create the edition views with Sirius . You can also use MetaEdit+ that is a powerful metamodeler. Roadmapping With Archimate Archimate is probably the best language to realize this kind of portfolio management. We will explain in a bit more details a possible use of Archimate enterprise architecture modeling language for portfolio roadmapping in another article. Conclusion With this meta-model, it is possible to create and operate a feasible, logic and auditable project portfolio and to have explanations about the constraints and the process of deciding what project should be done in priority. Most often, this kind of tools pacifies the battleground. Even if projects can go wrong during the year to come, with this method, many problems will have been anticipated and will, as per magic, not occur during the year. ( January 2018 )","title":"A Simple Meta-Model for Portfolio Management"},{"location":"articles/portfolio/#a-simple-meta-model-for-portfolio-management","text":"Image courtesy of freedigitalphotos.net . One of the main problems of companies is to manage many IT projects at the same time. Most often, the companies facing this problem are managing project portfolios. The main drawback of managing several projects in parallel is that is is not easy to: Understand the dependencies of the various projects together, and so the order in which they should be led; Identify and deal with the various scope overlaps between the various projects; Manage the relationship with the business people that want the various projects to happen. This article presents a very simple method to use to address all those points. We will present the original version of the meta-model. An Archimate version of this model will be presented in another article. This method mostly targets enterprise or IT architects, program managers or project portfolio managers. Due to its simplicity, many other profiles can use it.","title":"A Simple Meta-Model for Portfolio Management"},{"location":"articles/portfolio/#meta-model-presentation","text":"The first objective is to formalize the client of the IT project. Each organization has several departments in which we have several different customers. Our first artifact type will be Organization . We will assume that every organization can include several other organizations. Each organization will also have a customer name as an attribute. When the projects intentions are declared, the materials that are produced by the various customers inside the same organization will generally be a list of many projects, with a project name and a more or less developed functional description. We will define two artifact types to model that: Project to model the project, Function to model the functionality. In the existing IT systems, we have applications. With the projects to come, some of the current application will get new functions, some new applications will be created and some other will be decommissioned. We need one more artifact type: Application that models the application; if the application is new, we need a way to tag it as \"new\" or \"existing\". The Function artifact type will be used to attach functionality to Application s. For this purpose, we need an attribute on the Function that indicates if the function is already existing, if it is new or if it must evolve. As applications are part of a big part of the IT systems (for instance the financial systems), we will attach the application to an application domain; we will use the Domain artifact type for that purpose. In terms of relationships, we have the following semantics: An Organization can contain other Organization s; An Organization will be the customer of several Project s; Project s are aggregating Function s; Application s are attached to Domain s and Function s are attached to them. The resulting meta-model is presented on the following figure.","title":"Meta-Model Presentation"},{"location":"articles/portfolio/#methodology","text":"Let's suppose we use this model during the budgeting process that usually happens once a year in companies. Budget time is often the time for difficult choices. The following method is easy to use, can neutralize affects and help the top level management to choose between projects and to create a feasible roadmap within the limits of a feasible budget.","title":"Methodology"},{"location":"articles/portfolio/#step-1-accept-all-project-requests-and-identify-functions","text":"The first step is consisting in gathering all projects intentions from all organizations. We will analyze organization per organization. For each customer within a particular organization, the project intentions are gathered. The objective is to get the best functional description of the requirements possible and to identify the required functions. Very often, the requirements are a list of features (warning: some of them may already have been implemented without the customer to know about it). In that phase, the customer must not be challenged, but the architect should try his best to understand the functional (and non functional) requirements. To do that, it can be necessary to distinguish between the solution imagined by the customer and the original requirement. As the model is simple, the requirement artifact does not exist in this version. The identification of the required Function s will conclude this phase. Once this phase is complete, we can promise the customer to: Analyze his requirements along with all other requirements and projects and do our best to develop what is required; Provide a detailed feedback on the demands when all demands have been captured.","title":"Step 1: Accept All Project Requests and Identify Functions"},{"location":"articles/portfolio/#step-2-identify-common-functions-to-highlight-dependencies","text":"Several customers could have defined different projects that are using the same subset of functions. Identifying those functions is very important, this for several reasons: To avoid several projects to develop several times the same function, or worse, variations of the same function; To identify in what projects this function can be required and to scope it carefully; Developing a reusable function will ease the success of many projects whereas not developing it may cause project delays or failure; To add this function early in the roadmap; To take a special care to carefully place this function at the proper spot in the rest of the IT systems (step 3); Indeed, a misplaced function will generate many useless client/server calls, will generate consistency problems and will generate more integration and QA works: Economically, function placement is crucial. Functions, once identified, should be assigned with a number characterizing the number of projects that require them. The highest number will lead the global roadmapping exercise. We will call this number the dependency number .","title":"Step 2: Identify Common Functions to Highlight Dependencies"},{"location":"articles/portfolio/#step-3-associate-functions-to-applications","text":"This is a back office step that will be done between IT people, most of the time by architects. It is time to review the main functions of the existing applications to be accurate in the exercise. Functions will be classified into 3 categories: Existing functions : We can plan to go back to the customer to assess if we missed something in the original requirement and if the existing function should not evolve. New functions that can be naturally attached to an existing application : Some of the functions will naturally find their place as an evolution of an existing application; New functions that don't seem to be a natural evolution of the existing applications : However, some new functions have to be studied in order to determine their best spot in the IT systems (or their \"less worst\" spot). The two last steps can give birth to more advanced architecture studies. Covering this topic is not in the scope of this article, but we can insist on leading careful analysis, because function misplacement is a problem that will cause painful and costly problems for years if not for decades. All the functions that are not in existing applications should be gathered and analyzed as a whole to determine the opportunity of developing new applications.","title":"Step 3: Associate Functions to Applications"},{"location":"articles/portfolio/#step-4-analyze-the-roadmap-of-each-application-size-it-roughly-and-compare-with-the-skills-available","text":"Once all functions were assigned to applications, we can size them roughly and have a view of the roadmap of each application. For sure, this roadmap is still theoretical and the pure consequence of the customer's requirements.","title":"Step 4: Analyze the Roadmap of Each Application, Size it Roughly and Compare With The Skills Available"},{"location":"articles/portfolio/#the-case-of-application-maintenance-and-evolution","text":"All functions of an existing application can be prioritized in the best order of feasibility to create a first credible application roadmap, that is compliant with technical constraints. At this stage, we can also identify the functions that are nice to have compared to the ones that are must haves. This classification, even if it can be debated with the business may lead to some negotiation margins during the year to come. The skills available for each application is important because it will determine the real feasibility of the roadmap per application: We mean IT skills (project manager, product owner, developer, QA) but also business skills. Indeed, in that phase, we can identify the bottlenecks in terms of IT people, when not enough people are skilled to develop the expected application roadmap. Depending on the global priorities of the company, it is possible in that phase to plan people transfer, trainings or recruitments.","title":"The Case of Application Maintenance and Evolution"},{"location":"articles/portfolio/#the-case-of-new-applications","text":"Those projects are always more risky than the previous ones. In some cases, it is obvious for the customer that the new requirements will lead to a new application. In some other cases, it is not. Those cases must be dealt with carefully because the customer may argue that the new requirements are part of an existing application. In those cases, the objective is to avoid function misplacement. However, it is more difficult to create a new application: The project manager has to be found, and she/he has to be able to lead from-scratch projects; The team has to be found, or in the existing people (staff or consultants), or in new comers; The business expert(s) has to be named. Some business people may be a bit worried about the application creation projects because they cause HR troubles and they can face more delays or failures than evolution projects.","title":"The Case of New Applications"},{"location":"articles/portfolio/#end-of-the-phase","text":"At the end of the step 4, we should have: All functions identified; All application roadmap sized; All HR requirements.","title":"End of the Phase"},{"location":"articles/portfolio/#step-5-introduction-of-dependencies-to-create-project-visions","text":"To determine which projects to do first in the project portfolio, we need to take the reusable functions with the highest dependency number, in order to schedule them first. Then, the purpose is to play with the constraints to determine the less worst global path considering: The list of projects to develop; The skills available; The potential HR adjustments (people moving from one project to another, new people). Generally, the functions with the highest dependency number will drive the roadmap. That means that a function will be develop if and only if it is in the same project than a function with a high dependency number. That means also that generally a lot of new functions required by the business will not be able to be developed in the coming year, just because they are not part of an \"important\" project. Indeed, this method enables to show to the top level management what is important to the company throughout the functions that are required by many projects (i.e. that have the highest dependency numbers). Once everything is done, the exercise can be redone if the budget is bigger or lower in some parts.","title":"Step 5: Introduction of Dependencies to Create Project Visions"},{"location":"articles/portfolio/#limits-of-the-method","text":"We can argue that the functions with the highest dependency numbers may not be the most crucial functions. That can be true. We can use some tricks in using several levels of functions if needed. Indeed, a function (level 1) required by a certain project could be divided into 3 functions of a smaller granularity (level 2). The level 2 functions could be the functions implemented by the applications. With this model adjustment, we can have a high level view of the function with \"project functions\" (level 1) and an IT view of functions (level 2). The artifact type Function must then be able to aggregate a list of Function s. We let the reader update the meta-model.","title":"Limits of the Method"},{"location":"articles/portfolio/#what-tools-can-help","text":"","title":"What Tools Can Help?"},{"location":"articles/portfolio/#yed","text":"When this method was used for the first time, Yed was the tool used. A Yed model was realized per organization. The advantage of Yed is its simplicity and its powerful set of graph layout algorithms. The drawback of Yed is when the number of projects is big (we had to deal with the Digital Division and their 130 projects), the model is hardly usable.","title":"Yed"},{"location":"articles/portfolio/#emf-and-sirius","text":"Indeed, when the number of projects is high, a multiple view design tool is required. A possible option is to create and EMF model and to create the edition views with Sirius . You can also use MetaEdit+ that is a powerful metamodeler.","title":"EMF and Sirius"},{"location":"articles/portfolio/#roadmapping-with-archimate","text":"Archimate is probably the best language to realize this kind of portfolio management. We will explain in a bit more details a possible use of Archimate enterprise architecture modeling language for portfolio roadmapping in another article.","title":"Roadmapping With Archimate"},{"location":"articles/portfolio/#conclusion","text":"With this meta-model, it is possible to create and operate a feasible, logic and auditable project portfolio and to have explanations about the constraints and the process of deciding what project should be done in priority. Most often, this kind of tools pacifies the battleground. Even if projects can go wrong during the year to come, with this method, many problems will have been anticipated and will, as per magic, not occur during the year. ( January 2018 )","title":"Conclusion"},{"location":"articles/spreadsheet-and-PLM/","text":"The four core functions showing you need a PLM Photo by Nick Fewings on Unsplash This article is the third one of a series about PLM, after PLM and Graph Data and Configuration management of industrial products in PDM/PLM . This article begins with spreadsheets , numerous, ubiquitous spreadsheets that seem to rule many manufacturing processes, all along the product lifecycle, from engineering to manufacturing and support. Contrary to the tertiary sector where many areas that were using spreadsheets got out of them to use software applications (specific development or off-the-shelf software), it seems difficult, in manufacturing, to get out of spreadsheets. In this article, we will expose some of the reasons why spreadsheets are so hard to get rid of. Part data management (PDM) In our last two articles on PLM, we explained why the manufacturing area had a special vision of data, special comparing to the one of tertiary sector. In manufacturing, most data need to be managed as referential data even before to attach business rules to them. That is why, some decades ago, the industry started to focus on \"part data management\" (PDM) systems, which are managing the part data lifecycles. Once this is taken in charge, it is possible to think about the product and its lifecycle, and enter the Product Lifecycle Management world (PLM). The specific industry needs around data management make it complicated to replace many spreadsheet-based processes because the underlying requirements are more complex than they appear to be in the first place. A large variety of business objects From now, we will suppose that we are taking about business objects and about their lifecycle. The most common industrial business object is the part but it can be many other objects, depending on the business domain, for instance: System engineering and early maintenance engineering : Requirements, operational conditions, probability of failure, system, function, connector, trade-off, logical block, etc. Detailed design : Part, assembly, drawing, 3D, document, standard part, tolerances, change, applicability, compatibility, build, etc. Simulation : Test scenario, test campaign, business object composing the input data or the output data, configurations, etc. Assembly preparation : Routing, instruction, part, elementary action, sequence, balancing information, resource, tool, duration, etc. Assembly : Work order, part, resource, supplier, stock, quantity, tool, station, skills, clocking, warehouse, etc. Maintenance : Maintenance plan, documentation, data module, spare part list, work order, task, frequency, threshold, cycle, tool, skills, duration, etc. Etc. All those objects generally need, as a basis, four core functions to be able to be manipulated correctly and safely within a manufacturing company. 1. Versioning of business objects The versioning function is often done manually by renaming spreadsheets with a version number. But the real need that we can see is a versioning of the business object instances that are contained in the spreadsheet. Versioning of data is the first fundamental function that many spreadsheet users need. Because the business objects that they manipulate evolve in versions, they have to keep track of those versions, even if doing that in a spreadsheet is often a very manual process. The version management looks like the one that is done in the software industry especially because it is often relevant that several versions of the same business object are valid and used, at the same time , throughout the company or even outside. This is totally linked to the fundamentals of the manufacturing process: The engineering division of the company is designing a product in version N , The manufacturing part of the company is generally producing the version N-1 of the product, The support organization is supporting all sold versions starting from N-1 , N-2 , N-3 , etc. The customer of the product will have its product data in a certain version supported by the support organization. So, the general fundamental need on data is to manage several \"living versions\" of data. 2. Change process management When a business object version changes, it is very recommended (and sometimes mandatory due to regulation, such as in aerospace) to track several elements: Why was the change done? By whom? Who validated the change? The first question is becoming more and more important in the industry. Because, if the \"why\" is not captured, future modification to the product may violate some hypothesis done during the design phase. When a product evolves, it can be very useful to track back the reasons of certain modifications, especially the cases that were explored and that were not chosen (see note 1 ). The change process can be done at the business object level but also at a lower level. Indeed, potentially each field of a business object has at least one owner which has a role in the organization. This owner is able to modify the value of the particular field of the particular business object. In a good data governance environment, the various roles are clearly defined. It is important to track who changed what, especially when the business object is complex. The third part of the change process is the process of validation of the change: If there is a change, one or several roles in the organizations are responsible to validate that the change, as it is, can be done. This must also be tracked because some responsibilities are involved, especially to validate that, in a complex environment, the change impacts have correctly been estimated by the various functional disciplines that are concerned. In most evolving spreadsheets, a kind of manual change process is put in place to secure the main reasons of changes and possible the decisions taken by the \"approvers\". This part is often badly done in a spreadsheet environment because it is hard to track if the data manipulated are big and/or complex and/or under the responsibility of several actors. 3. Collaboration As we saw, a business object can be shared by several roles, at different moments of a complex process. Even if only one people is able to modify the spreadsheet at a certain moment, this person will be bound to meet with many people to gather all the information, or to exchange many mails to consolidate information coming from many actors. This Excel-based collaboration process is not generally \"hidden\", in the sense that collaboration is hidden by the \"people+spreadsheet\" paradigm. As long as one person is maintaining the spreadsheet and is responsible for \"gathering information\", the collaboration feature that results from the data gathering process is often not visible. Indeed, the real need is generally to have the involved roles update directly the data in an appropriate place, while pumping versions and explaining the reasons for change. Even if the word \"collaboration\" is quite vague in terms of functionality, it can cover, in the industry, several complex realities: The capability of having many actors working on the same data; The capability of having many actors working on several versions of the same data at the same time; The capability of being notified about the impacts of a data change on linked data; Etc. Collaboration is a feature that must be analyze when an organization wants to get rid of a spreadsheet-based process. Collaboration around business object lifecycle can indicate the presence of complex collaboration processes between teams. The \"concurrent engineering\" is one of them, the idea being that the design phase is done taking into account one or several other phases of the product lifecycle, such as the manufacturing of products or their support. 4. Applicability management Generally, in spreadsheets this functionality is basic and represented by one or several columns with applicability indicators, typically: Applicable to case 1 , Applicable to case 2 , ... Applicable to case N , Applicable to all cases. For sure, those indicators can become quite complex in complex spreadsheets with macros. As we explained in the article about configuration management in PLMs , the manufacturing area needs to manage more or less complex configurations. Most of time, in spreadsheets managing some aspect of the lifecycle of some business objects, there is a notion of applicability management that opens to the management of some configurations. PDM and detailed-design centric PLM Most manufacturing companies use now PDM systems for their core detailed design process. Because many disciplines are collaborating to the design of a complex product (mechanical engineering, electrical engineering, electronics, engines, etc.), the product detailed design process was put in the center of preoccupations in many companies, which is understandable. However, outside the detailed design world, many other domains are often still not covered by PLM software. When the manipulated business objects are requiring the four core functions we just discussed, light PLM engines can provide the four basic functions we just mentioned. We think this phenomenon will generalize in the coming years, as fast as the maturity of the industry to realize that most of the data that are managed in spreadsheets need the four core functions we just detailed. Extending the scope of PLMs to several business domains Modern PLM systems are able to manage many business object lifecycles. In most of them, it is also possible to easily implement your own business objects and to define their custom lifecycle, benefiting from the PLM engine. Considering those capabilities, the question is: Why not integrate many new business objects coming from various business domains inside a single PLM that would be in charge of the management of all the business object lifecycle, on top of the existing detailed design process? This approach exists in the ERP (Enterprise Resource Planning) world and why wouldn't it be possible in the PLM world? This question is a complex one and it is linked to the optimal mapping of several interlinked business object lifecycles. We already brought some elements of response in previous articles such as the real nature of data . We intend to bring more elements in a next article of this series, dedicated to the question. Spreadsheet replacement risks If the industrial area can reuse the same approaches used in the tertiary sector for some processes, there are many cases where the basic solutions of the tertiary sector are not applicable, due to the specific core requirements attached to data we just saw. The most classical cases are to try to replace a spreadsheet by a custom development, typically: A specific web application, A customized content management system (CMS). In the first situation, the focus is made on the data management whereas in the second situation, the focus is made on attached documents. In both cases, if the business objects that are manipulated are requiring the four core functions we have detailed, the development team may progressively discover the extent of the complexity managed manually outside the spreadsheet. This can jeopardize the success of the project. Many projects that were appearing as simple reveal themselves as being very complex. This situation happens regularly when IT service companies took commitments without understanding the full complexity of the project. The difficulty of replacing spreadsheet-based processes We think the need for the four core functions we detailed indicates that there is a need for a PLM engine, even if it is a simple one, and that need cannot be easily satisfied with other more common IT solutions (unless people want to redevelop those four core functions). In the manufacturing business, for instance in the aerospace, many people ask why so many processes are based on spreadsheets. A possible explanation is that, considering the very nature of industrial data, it is often surprisingly complex to replace some manual spreadsheet-based processes. If your business data need the four functions we detailed, you need a PLM. Notes Note 1 : We can note that it appears more and more important to also track the very early choices, or \"design justification\". Quite often, during the early design phase, system engineering methods were applied in order to perform complex trade-offs that lead to a certain product architecture. This can be seen as an \"generalization\" of the change management process, i.e. the capability of going back in the past to find back the original design choices. See also The series of articles about PLM: Article 1: PLM and graph data Article 2: Configuration management of industrial products in PDM/PLM Other articles: The real nature of data (July 2021)","title":"The four core functions showing you need a PLM"},{"location":"articles/spreadsheet-and-PLM/#the-four-core-functions-showing-you-need-a-plm","text":"Photo by Nick Fewings on Unsplash This article is the third one of a series about PLM, after PLM and Graph Data and Configuration management of industrial products in PDM/PLM . This article begins with spreadsheets , numerous, ubiquitous spreadsheets that seem to rule many manufacturing processes, all along the product lifecycle, from engineering to manufacturing and support. Contrary to the tertiary sector where many areas that were using spreadsheets got out of them to use software applications (specific development or off-the-shelf software), it seems difficult, in manufacturing, to get out of spreadsheets. In this article, we will expose some of the reasons why spreadsheets are so hard to get rid of.","title":"The four core functions showing you need a PLM"},{"location":"articles/spreadsheet-and-PLM/#part-data-management-pdm","text":"In our last two articles on PLM, we explained why the manufacturing area had a special vision of data, special comparing to the one of tertiary sector. In manufacturing, most data need to be managed as referential data even before to attach business rules to them. That is why, some decades ago, the industry started to focus on \"part data management\" (PDM) systems, which are managing the part data lifecycles. Once this is taken in charge, it is possible to think about the product and its lifecycle, and enter the Product Lifecycle Management world (PLM). The specific industry needs around data management make it complicated to replace many spreadsheet-based processes because the underlying requirements are more complex than they appear to be in the first place.","title":"Part data management (PDM)"},{"location":"articles/spreadsheet-and-PLM/#a-large-variety-of-business-objects","text":"From now, we will suppose that we are taking about business objects and about their lifecycle. The most common industrial business object is the part but it can be many other objects, depending on the business domain, for instance: System engineering and early maintenance engineering : Requirements, operational conditions, probability of failure, system, function, connector, trade-off, logical block, etc. Detailed design : Part, assembly, drawing, 3D, document, standard part, tolerances, change, applicability, compatibility, build, etc. Simulation : Test scenario, test campaign, business object composing the input data or the output data, configurations, etc. Assembly preparation : Routing, instruction, part, elementary action, sequence, balancing information, resource, tool, duration, etc. Assembly : Work order, part, resource, supplier, stock, quantity, tool, station, skills, clocking, warehouse, etc. Maintenance : Maintenance plan, documentation, data module, spare part list, work order, task, frequency, threshold, cycle, tool, skills, duration, etc. Etc. All those objects generally need, as a basis, four core functions to be able to be manipulated correctly and safely within a manufacturing company.","title":"A large variety of business objects"},{"location":"articles/spreadsheet-and-PLM/#1-versioning-of-business-objects","text":"The versioning function is often done manually by renaming spreadsheets with a version number. But the real need that we can see is a versioning of the business object instances that are contained in the spreadsheet. Versioning of data is the first fundamental function that many spreadsheet users need. Because the business objects that they manipulate evolve in versions, they have to keep track of those versions, even if doing that in a spreadsheet is often a very manual process. The version management looks like the one that is done in the software industry especially because it is often relevant that several versions of the same business object are valid and used, at the same time , throughout the company or even outside. This is totally linked to the fundamentals of the manufacturing process: The engineering division of the company is designing a product in version N , The manufacturing part of the company is generally producing the version N-1 of the product, The support organization is supporting all sold versions starting from N-1 , N-2 , N-3 , etc. The customer of the product will have its product data in a certain version supported by the support organization. So, the general fundamental need on data is to manage several \"living versions\" of data.","title":"1. Versioning of business objects"},{"location":"articles/spreadsheet-and-PLM/#2-change-process-management","text":"When a business object version changes, it is very recommended (and sometimes mandatory due to regulation, such as in aerospace) to track several elements: Why was the change done? By whom? Who validated the change? The first question is becoming more and more important in the industry. Because, if the \"why\" is not captured, future modification to the product may violate some hypothesis done during the design phase. When a product evolves, it can be very useful to track back the reasons of certain modifications, especially the cases that were explored and that were not chosen (see note 1 ). The change process can be done at the business object level but also at a lower level. Indeed, potentially each field of a business object has at least one owner which has a role in the organization. This owner is able to modify the value of the particular field of the particular business object. In a good data governance environment, the various roles are clearly defined. It is important to track who changed what, especially when the business object is complex. The third part of the change process is the process of validation of the change: If there is a change, one or several roles in the organizations are responsible to validate that the change, as it is, can be done. This must also be tracked because some responsibilities are involved, especially to validate that, in a complex environment, the change impacts have correctly been estimated by the various functional disciplines that are concerned. In most evolving spreadsheets, a kind of manual change process is put in place to secure the main reasons of changes and possible the decisions taken by the \"approvers\". This part is often badly done in a spreadsheet environment because it is hard to track if the data manipulated are big and/or complex and/or under the responsibility of several actors.","title":"2. Change process management"},{"location":"articles/spreadsheet-and-PLM/#3-collaboration","text":"As we saw, a business object can be shared by several roles, at different moments of a complex process. Even if only one people is able to modify the spreadsheet at a certain moment, this person will be bound to meet with many people to gather all the information, or to exchange many mails to consolidate information coming from many actors. This Excel-based collaboration process is not generally \"hidden\", in the sense that collaboration is hidden by the \"people+spreadsheet\" paradigm. As long as one person is maintaining the spreadsheet and is responsible for \"gathering information\", the collaboration feature that results from the data gathering process is often not visible. Indeed, the real need is generally to have the involved roles update directly the data in an appropriate place, while pumping versions and explaining the reasons for change. Even if the word \"collaboration\" is quite vague in terms of functionality, it can cover, in the industry, several complex realities: The capability of having many actors working on the same data; The capability of having many actors working on several versions of the same data at the same time; The capability of being notified about the impacts of a data change on linked data; Etc. Collaboration is a feature that must be analyze when an organization wants to get rid of a spreadsheet-based process. Collaboration around business object lifecycle can indicate the presence of complex collaboration processes between teams. The \"concurrent engineering\" is one of them, the idea being that the design phase is done taking into account one or several other phases of the product lifecycle, such as the manufacturing of products or their support.","title":"3. Collaboration"},{"location":"articles/spreadsheet-and-PLM/#4-applicability-management","text":"Generally, in spreadsheets this functionality is basic and represented by one or several columns with applicability indicators, typically: Applicable to case 1 , Applicable to case 2 , ... Applicable to case N , Applicable to all cases. For sure, those indicators can become quite complex in complex spreadsheets with macros. As we explained in the article about configuration management in PLMs , the manufacturing area needs to manage more or less complex configurations. Most of time, in spreadsheets managing some aspect of the lifecycle of some business objects, there is a notion of applicability management that opens to the management of some configurations.","title":"4. Applicability management"},{"location":"articles/spreadsheet-and-PLM/#pdm-and-detailed-design-centric-plm","text":"Most manufacturing companies use now PDM systems for their core detailed design process. Because many disciplines are collaborating to the design of a complex product (mechanical engineering, electrical engineering, electronics, engines, etc.), the product detailed design process was put in the center of preoccupations in many companies, which is understandable. However, outside the detailed design world, many other domains are often still not covered by PLM software. When the manipulated business objects are requiring the four core functions we just discussed, light PLM engines can provide the four basic functions we just mentioned. We think this phenomenon will generalize in the coming years, as fast as the maturity of the industry to realize that most of the data that are managed in spreadsheets need the four core functions we just detailed.","title":"PDM and detailed-design centric PLM"},{"location":"articles/spreadsheet-and-PLM/#extending-the-scope-of-plms-to-several-business-domains","text":"Modern PLM systems are able to manage many business object lifecycles. In most of them, it is also possible to easily implement your own business objects and to define their custom lifecycle, benefiting from the PLM engine. Considering those capabilities, the question is: Why not integrate many new business objects coming from various business domains inside a single PLM that would be in charge of the management of all the business object lifecycle, on top of the existing detailed design process? This approach exists in the ERP (Enterprise Resource Planning) world and why wouldn't it be possible in the PLM world? This question is a complex one and it is linked to the optimal mapping of several interlinked business object lifecycles. We already brought some elements of response in previous articles such as the real nature of data . We intend to bring more elements in a next article of this series, dedicated to the question.","title":"Extending the scope of PLMs to several business domains"},{"location":"articles/spreadsheet-and-PLM/#spreadsheet-replacement-risks","text":"If the industrial area can reuse the same approaches used in the tertiary sector for some processes, there are many cases where the basic solutions of the tertiary sector are not applicable, due to the specific core requirements attached to data we just saw. The most classical cases are to try to replace a spreadsheet by a custom development, typically: A specific web application, A customized content management system (CMS). In the first situation, the focus is made on the data management whereas in the second situation, the focus is made on attached documents. In both cases, if the business objects that are manipulated are requiring the four core functions we have detailed, the development team may progressively discover the extent of the complexity managed manually outside the spreadsheet. This can jeopardize the success of the project. Many projects that were appearing as simple reveal themselves as being very complex. This situation happens regularly when IT service companies took commitments without understanding the full complexity of the project.","title":"Spreadsheet replacement risks"},{"location":"articles/spreadsheet-and-PLM/#the-difficulty-of-replacing-spreadsheet-based-processes","text":"We think the need for the four core functions we detailed indicates that there is a need for a PLM engine, even if it is a simple one, and that need cannot be easily satisfied with other more common IT solutions (unless people want to redevelop those four core functions). In the manufacturing business, for instance in the aerospace, many people ask why so many processes are based on spreadsheets. A possible explanation is that, considering the very nature of industrial data, it is often surprisingly complex to replace some manual spreadsheet-based processes. If your business data need the four functions we detailed, you need a PLM.","title":"The difficulty of replacing spreadsheet-based processes"},{"location":"articles/spreadsheet-and-PLM/#notes","text":"Note 1 : We can note that it appears more and more important to also track the very early choices, or \"design justification\". Quite often, during the early design phase, system engineering methods were applied in order to perform complex trade-offs that lead to a certain product architecture. This can be seen as an \"generalization\" of the change management process, i.e. the capability of going back in the past to find back the original design choices.","title":"Notes"},{"location":"articles/spreadsheet-and-PLM/#see-also","text":"The series of articles about PLM: Article 1: PLM and graph data Article 2: Configuration management of industrial products in PDM/PLM Other articles: The real nature of data (July 2021)","title":"See also"},{"location":"articles/the-v2-vortex/","text":"The V2 Vortex A lot of companies enter, at a certain moment in their life, into the \"V2 Vortex\u201d. In this article, we're going to detail the pattern of the harsh replacement of the V1 of the application by the V2. The Case of Software Companies For a lot of software companies that enter into the V2 Vortex, the history is similar: the V1 of the product was created long ago, for instance in the late 90s or in the early 2000s. It was a success. The company found its market. It has customers. All is great except that in 15 years, technology changed. And so did the competition. In those software companies, we can often see a lack of software culture, and often, business-oriented people surfing on a tool that developed and gathered success. The R D team is generally composed of IT people that preferred implementing new features for customers rather than making the development process change progressively to modernize slowly but surely the product. The problem looks like a pattern: after a large amount of time, even the best buildings get old when they are not maintained. With years, it becomes difficult to implement new features; evolutions are slower to much slower, and the software bugs are more and more complicated to fix [1]. Software is a complex activity because when things go wrong, the more customers, the more troubles, the more time and money spent in support, the less money to invest and the less time to fix. This is the dark side of the software moon, the one that enables selling plenty of times a software that was developed once. Multipliers work both ways. So, one day, the CEO has to face the facts. \"We need a V2\", he says. \u201cWe can invest, I have cash\u201d. That's when the trouble begins. Who Really Does The Maths? Does the company have the right skills? Enough people? The right money? As a consultant, I see that CEOs frequently don't run the maths, or maybe their calculator is bugged, or perhaps they are too optimistic. New trendy technologies make them dream their product can be rewritten in 2 years maximum (it's always 2 years). For sure, they think current technology is much more productive that the old one they keep on using. And new developers can do so many pretty web stuff. The argument I hear also a lot is that the current software was refactored several times and so its global costs is much lower that the R D team multiplied by the age of the software (generally around 10 to 20 years). But, let's do the maths together: let's suppose the R D development team was around 4 FTE during all those years. The gross product weight would be 60 men years (MY) for a team that worked 15 years on a product. Let's suppose 1/3 of the charge is not relevant and is corresponding to the refactorings. This leads to 40 useful MY. Globally, a good developer in Europe costs around 100 k\u20ac a year (with charges). That means that the software company should have approximately 4 M\u20ac available to realize this project. This investment is generally quite risky. 40 MY of effort is possible in two years provided you have a team of 30 people (taking ramp-ups into account). For sure, the CEO of the company (or the CTO) is used to manage only 4 R D people. So he rarely has the (project) management skills for such a big project. Let's suppose he reduces the team and targets 10 people for 4 years, he still has to train them for them to learn the business and to understand the existing product in order to be productive. But where to find the right people? In IT service companies? This could be a bad move: IT service developer usually don't know how to produce good software; but they make it fast. Most CEOs will choose one of those options: Be optimistic and start with his 4 developers using new technologies. Things will probably not progressed as expected. Outsource because a service company can convince the CEO that with 1M\u20ac, you can create 2M\u20ac of software. Generally, the budget is consumed but the software is not produced (and not working). Believe software bandits or software magicians that will promise the CEO that with their code generation MDA approach, they can develop his V2 software in less than 2 years \u2013 provided he pays upfront. Develop a new product with a limited scope, for new customers, hoping to renew the old company success while fearing about the double team and the double recurring costs. But wouldn't it be a company startup within the company? Sell the company when it is still worth something and before it's too late. If we do the maths, we can be worried. Most of those CEOs will potentially try successively several options and fail. We can say they entered into the \"V2 Vortex\", a place where many software company go to die. Like the elephant cemetery, they cannot go back. Soon, they'll run out of cash and be obliged to find a more radical solution. The V2 Vortex in Big Companies In big companies, unfortunately, things are quite similar. Perhaps the application is much bigger (like a mainframe); perhaps it was not developed over 15 years but over 30; perhaps the development team is 100 people instead of 4; but the fact is here: they definitely are into the V2 Vortex. Doing the maths is really frightening on their side. 30 years with 100 FTE leads to 3,000 MY of effort. Let's apply the 1/3 optimistic ratio: the application weights 2,000 MY... For sure, perhaps they don't have the cash problem. But they have tried to replace their systems 3 or 4 times, and all projects failed. Indeed, they never really understood why. This is too scary to look into it. They also tried several options and spent a lot of cash, and the amount spent is at the size of the ambition. They, also, would like to get rid of the problem in 2 years, but that would mean 1,000 people in the project team... Nah, not possible. IT service companies are not a big help with this kind of problem. Those applications are core business ones and they are connected to dozens of systems inside and outside their company. It is quite hard to even grasp the full picture. They are in the V2 Vortex, and despite the fact that they're much bigger than the small software company we spoke about, the people in charge risk their job if they fail. There is No Magic I must say: there is no magic. I wish I could solve their problems with a lot of money but I cannot. However, there is a path - but they never like it. \u201cWhen you have applications with thousands of function points, you have to think about it as an asset\u201d, I say to them. Ok, it is old, they did not maintain it enough, it is bugged, they are locked in ancient design constraints and old technologies, yes, it is the same for everybody. But it is their asset. Most of time, they can still sell it to customers (one day, they won't), or the application is still massively used (because it is core business). It still solves business issues. It still runs everyday for tons of people that can hate it while not being able to live without it. So, as they would do with an old unmaintained asset that they have to keep, they have to modernize it. They cannot let it go. They have to do what they should have done, a long time ago. And it becomes urgent. Rewrite by Parts and Refactor New technologies brought new tools and new ways of managing legacy code. My advice is to upgrade to new tools, to new source management, to continuous build and integration, to pure local development workstations, to automated testing. The old code must absolutely be ported on recent compilers and OS. Surprisingly, after all the previous failures, their development process is often the same old unproductive non agile process. The hardest thing to do is to rewrite the software by parts, which implies code refactoring. The final users of the applications will have to cope with a hybrid product for years, with some modernized parts and some old ones, the time for the team in place to modernize it completely. Enterprise Architecture and Transition Architecture Building The Functional Map, the Map to Other Systems and the Transformation Steps Modernizing a complex software must be planned through the use of enterprise architecture techniques. As the system can be big in terms of functionality and/or can be connected to a lot of other systems, getting a complete map of the functionality and interfaces seems the best starting point. Archimate is the language for the task [2]. With Archimate, it is possible to represent the software progressive evolutions and the impacts on the user's processes. This is particularly important because some application modifications will have a serious impact on processes whereas others won't. In terms of software, it is hard to migrate the core of an old application right away. Generally, the peripheric functions will be the easiest to replace. Basically, those functions often use data that, in the database, are not lined to any other data (leaves of the tree of tables). By migrating function by function, and refactoring continuously the interfaces between the new part of the system and the old part of the system, the old part will become thinner while the new part will grow. Transition Architecture Technically, a transition architecture must be determined. The current one was supposed to be abandonned for a new one, but if we suppose the application will be composed by both technologies at the same time, it is crucial to have a generic way of moving functions from one environment to the other. Identifying what process is creating the data and what processes are consuming the data are important knowledge to get in order to do a smooth migration. The migration should enable to transfer the control of data from the old system to the new one progressively. Software is Made of People The HR can be a problem. Let's face it: it is not so easy to convert people that were used for years to work in a certain paradigm in another. Some people will adapt and some won't. Moreover, developing in that phase of the life of the application requires skills that are generally not present in the original team. Recruitment is generally getting on the critical path. Because software is made of people. If people are closed, the software will be closed; if they are open, it will be open. If their people are messy, the software will be a mess; if they are structured, so will their software. So, to create a great software, you need to hire great people. And, once you got them, you have to cherish them because bright people can be insecure. And you don't want to lose them. This change is hard is many context: In small companies, it can bring a lot of issues due to feelings towards people (generally the original CTO is part of the founders and should be replaced); In big companies, the application manager, even if he took in charge for decades an application, must generally be changed in order for new motivated people to take over and do the work. Tabula Rasa (Rewriting) Versus Transformation Transformation Is Not Popular In the software industry, we generally damage our software assets when maintaining them. Then comes a time when we must transform the big software. Software transformation and refactoring is not very popular, for a lot of reasons: It requires a lot of work on the existing system, to really analyze what it does, how it was designed, and what are his good and bad points; It requires to enter into the fucntional details of the business, which many software engineers are reluctant to do, because they are more interested by technology than by functional aspects; It implies mastering two environments for a long period of time, and one of them may be very old and not very appealing on a r\u00e9sum\u00e9; It implies thinking at the software and IT architecture levels (3 and 4), even in a single application, which is not possible for many people [3]. For all those reasons, most people and managers prefer the \"tabula rasa\" solution. But, if we do the maths, we see that it is very often not an option. The Rewriting Path The rewriting path is not impossible, and it is sometimes the only option. It can be a success (even if many rewriting projects fail) but the conditions are numerous for it to succeed: The budget must be very big, and so must be the business case to get a ROI in a reasonable delay; The rewriting project must take into account the migration from the old system to the new one, the earlier possible in the project; The architecture of the V2 product should respect the semantics of the business, because may old systems do; The team must be great and work closely with the old system one; Evolutions should be limited in the old system; The architecture of the new system must enable parallel developments. Conclusion After decades of projects and auditing, most of the companies I saw facing the V2 vortex should have considered the transformation path rather than the rewriting path. Notes [1] - We will come back on technical debt in other parts of this book. [2] - Archimate resources can be found here: http://www.opengroup.org/subjectareas/enterprise/archimate. [3] - Refer to The Five Levels Of Conceptual Maturity for IT Teams . ( June 2015, corrected November 2017 )","title":"The V2 Vortex"},{"location":"articles/the-v2-vortex/#the-v2-vortex","text":"A lot of companies enter, at a certain moment in their life, into the \"V2 Vortex\u201d. In this article, we're going to detail the pattern of the harsh replacement of the V1 of the application by the V2.","title":"The V2 Vortex"},{"location":"articles/the-v2-vortex/#the-case-of-software-companies","text":"For a lot of software companies that enter into the V2 Vortex, the history is similar: the V1 of the product was created long ago, for instance in the late 90s or in the early 2000s. It was a success. The company found its market. It has customers. All is great except that in 15 years, technology changed. And so did the competition. In those software companies, we can often see a lack of software culture, and often, business-oriented people surfing on a tool that developed and gathered success. The R D team is generally composed of IT people that preferred implementing new features for customers rather than making the development process change progressively to modernize slowly but surely the product. The problem looks like a pattern: after a large amount of time, even the best buildings get old when they are not maintained. With years, it becomes difficult to implement new features; evolutions are slower to much slower, and the software bugs are more and more complicated to fix [1]. Software is a complex activity because when things go wrong, the more customers, the more troubles, the more time and money spent in support, the less money to invest and the less time to fix. This is the dark side of the software moon, the one that enables selling plenty of times a software that was developed once. Multipliers work both ways. So, one day, the CEO has to face the facts. \"We need a V2\", he says. \u201cWe can invest, I have cash\u201d. That's when the trouble begins.","title":"The Case of Software Companies"},{"location":"articles/the-v2-vortex/#who-really-does-the-maths","text":"Does the company have the right skills? Enough people? The right money? As a consultant, I see that CEOs frequently don't run the maths, or maybe their calculator is bugged, or perhaps they are too optimistic. New trendy technologies make them dream their product can be rewritten in 2 years maximum (it's always 2 years). For sure, they think current technology is much more productive that the old one they keep on using. And new developers can do so many pretty web stuff. The argument I hear also a lot is that the current software was refactored several times and so its global costs is much lower that the R D team multiplied by the age of the software (generally around 10 to 20 years). But, let's do the maths together: let's suppose the R D development team was around 4 FTE during all those years. The gross product weight would be 60 men years (MY) for a team that worked 15 years on a product. Let's suppose 1/3 of the charge is not relevant and is corresponding to the refactorings. This leads to 40 useful MY. Globally, a good developer in Europe costs around 100 k\u20ac a year (with charges). That means that the software company should have approximately 4 M\u20ac available to realize this project. This investment is generally quite risky. 40 MY of effort is possible in two years provided you have a team of 30 people (taking ramp-ups into account). For sure, the CEO of the company (or the CTO) is used to manage only 4 R D people. So he rarely has the (project) management skills for such a big project. Let's suppose he reduces the team and targets 10 people for 4 years, he still has to train them for them to learn the business and to understand the existing product in order to be productive. But where to find the right people? In IT service companies? This could be a bad move: IT service developer usually don't know how to produce good software; but they make it fast. Most CEOs will choose one of those options: Be optimistic and start with his 4 developers using new technologies. Things will probably not progressed as expected. Outsource because a service company can convince the CEO that with 1M\u20ac, you can create 2M\u20ac of software. Generally, the budget is consumed but the software is not produced (and not working). Believe software bandits or software magicians that will promise the CEO that with their code generation MDA approach, they can develop his V2 software in less than 2 years \u2013 provided he pays upfront. Develop a new product with a limited scope, for new customers, hoping to renew the old company success while fearing about the double team and the double recurring costs. But wouldn't it be a company startup within the company? Sell the company when it is still worth something and before it's too late. If we do the maths, we can be worried. Most of those CEOs will potentially try successively several options and fail. We can say they entered into the \"V2 Vortex\", a place where many software company go to die. Like the elephant cemetery, they cannot go back. Soon, they'll run out of cash and be obliged to find a more radical solution.","title":"Who Really Does The Maths?"},{"location":"articles/the-v2-vortex/#the-v2-vortex-in-big-companies","text":"In big companies, unfortunately, things are quite similar. Perhaps the application is much bigger (like a mainframe); perhaps it was not developed over 15 years but over 30; perhaps the development team is 100 people instead of 4; but the fact is here: they definitely are into the V2 Vortex. Doing the maths is really frightening on their side. 30 years with 100 FTE leads to 3,000 MY of effort. Let's apply the 1/3 optimistic ratio: the application weights 2,000 MY... For sure, perhaps they don't have the cash problem. But they have tried to replace their systems 3 or 4 times, and all projects failed. Indeed, they never really understood why. This is too scary to look into it. They also tried several options and spent a lot of cash, and the amount spent is at the size of the ambition. They, also, would like to get rid of the problem in 2 years, but that would mean 1,000 people in the project team... Nah, not possible. IT service companies are not a big help with this kind of problem. Those applications are core business ones and they are connected to dozens of systems inside and outside their company. It is quite hard to even grasp the full picture. They are in the V2 Vortex, and despite the fact that they're much bigger than the small software company we spoke about, the people in charge risk their job if they fail.","title":"The V2 Vortex in Big Companies"},{"location":"articles/the-v2-vortex/#there-is-no-magic","text":"I must say: there is no magic. I wish I could solve their problems with a lot of money but I cannot. However, there is a path - but they never like it. \u201cWhen you have applications with thousands of function points, you have to think about it as an asset\u201d, I say to them. Ok, it is old, they did not maintain it enough, it is bugged, they are locked in ancient design constraints and old technologies, yes, it is the same for everybody. But it is their asset. Most of time, they can still sell it to customers (one day, they won't), or the application is still massively used (because it is core business). It still solves business issues. It still runs everyday for tons of people that can hate it while not being able to live without it. So, as they would do with an old unmaintained asset that they have to keep, they have to modernize it. They cannot let it go. They have to do what they should have done, a long time ago. And it becomes urgent.","title":"There is No Magic"},{"location":"articles/the-v2-vortex/#rewrite-by-parts-and-refactor","text":"New technologies brought new tools and new ways of managing legacy code. My advice is to upgrade to new tools, to new source management, to continuous build and integration, to pure local development workstations, to automated testing. The old code must absolutely be ported on recent compilers and OS. Surprisingly, after all the previous failures, their development process is often the same old unproductive non agile process. The hardest thing to do is to rewrite the software by parts, which implies code refactoring. The final users of the applications will have to cope with a hybrid product for years, with some modernized parts and some old ones, the time for the team in place to modernize it completely.","title":"Rewrite by Parts and Refactor"},{"location":"articles/the-v2-vortex/#enterprise-architecture-and-transition-architecture","text":"","title":"Enterprise Architecture and Transition Architecture"},{"location":"articles/the-v2-vortex/#building-the-functional-map-the-map-to-other-systems-and-the-transformation-steps","text":"Modernizing a complex software must be planned through the use of enterprise architecture techniques. As the system can be big in terms of functionality and/or can be connected to a lot of other systems, getting a complete map of the functionality and interfaces seems the best starting point. Archimate is the language for the task [2]. With Archimate, it is possible to represent the software progressive evolutions and the impacts on the user's processes. This is particularly important because some application modifications will have a serious impact on processes whereas others won't. In terms of software, it is hard to migrate the core of an old application right away. Generally, the peripheric functions will be the easiest to replace. Basically, those functions often use data that, in the database, are not lined to any other data (leaves of the tree of tables). By migrating function by function, and refactoring continuously the interfaces between the new part of the system and the old part of the system, the old part will become thinner while the new part will grow.","title":"Building The Functional Map, the Map to Other Systems and the Transformation Steps"},{"location":"articles/the-v2-vortex/#transition-architecture","text":"Technically, a transition architecture must be determined. The current one was supposed to be abandonned for a new one, but if we suppose the application will be composed by both technologies at the same time, it is crucial to have a generic way of moving functions from one environment to the other. Identifying what process is creating the data and what processes are consuming the data are important knowledge to get in order to do a smooth migration. The migration should enable to transfer the control of data from the old system to the new one progressively.","title":"Transition Architecture"},{"location":"articles/the-v2-vortex/#software-is-made-of-people","text":"The HR can be a problem. Let's face it: it is not so easy to convert people that were used for years to work in a certain paradigm in another. Some people will adapt and some won't. Moreover, developing in that phase of the life of the application requires skills that are generally not present in the original team. Recruitment is generally getting on the critical path. Because software is made of people. If people are closed, the software will be closed; if they are open, it will be open. If their people are messy, the software will be a mess; if they are structured, so will their software. So, to create a great software, you need to hire great people. And, once you got them, you have to cherish them because bright people can be insecure. And you don't want to lose them. This change is hard is many context: In small companies, it can bring a lot of issues due to feelings towards people (generally the original CTO is part of the founders and should be replaced); In big companies, the application manager, even if he took in charge for decades an application, must generally be changed in order for new motivated people to take over and do the work.","title":"Software is Made of People"},{"location":"articles/the-v2-vortex/#tabula-rasa-40rewriting41-versus-transformation","text":"","title":"Tabula Rasa (Rewriting) Versus Transformation"},{"location":"articles/the-v2-vortex/#transformation-is-not-popular","text":"In the software industry, we generally damage our software assets when maintaining them. Then comes a time when we must transform the big software. Software transformation and refactoring is not very popular, for a lot of reasons: It requires a lot of work on the existing system, to really analyze what it does, how it was designed, and what are his good and bad points; It requires to enter into the fucntional details of the business, which many software engineers are reluctant to do, because they are more interested by technology than by functional aspects; It implies mastering two environments for a long period of time, and one of them may be very old and not very appealing on a r\u00e9sum\u00e9; It implies thinking at the software and IT architecture levels (3 and 4), even in a single application, which is not possible for many people [3]. For all those reasons, most people and managers prefer the \"tabula rasa\" solution. But, if we do the maths, we see that it is very often not an option.","title":"Transformation Is Not Popular"},{"location":"articles/the-v2-vortex/#the-rewriting-path","text":"The rewriting path is not impossible, and it is sometimes the only option. It can be a success (even if many rewriting projects fail) but the conditions are numerous for it to succeed: The budget must be very big, and so must be the business case to get a ROI in a reasonable delay; The rewriting project must take into account the migration from the old system to the new one, the earlier possible in the project; The architecture of the V2 product should respect the semantics of the business, because may old systems do; The team must be great and work closely with the old system one; Evolutions should be limited in the old system; The architecture of the new system must enable parallel developments.","title":"The Rewriting Path"},{"location":"articles/the-v2-vortex/#conclusion","text":"After decades of projects and auditing, most of the companies I saw facing the V2 vortex should have considered the transformation path rather than the rewriting path.","title":"Conclusion"},{"location":"articles/the-v2-vortex/#notes","text":"[1] - We will come back on technical debt in other parts of this book. [2] - Archimate resources can be found here: http://www.opengroup.org/subjectareas/enterprise/archimate. [3] - Refer to The Five Levels Of Conceptual Maturity for IT Teams . ( June 2015, corrected November 2017 )","title":"Notes"},{"location":"articles/various-stages/","text":"The Various Stages of Digital Transformation Presentation done in Airbus Helicopters for the PMI France Chapter. Click to see the presentation . ( June 2015 )","title":"The Various Stages of Digital Transformation"},{"location":"articles/various-stages/#the-various-stages-of-digital-transformation","text":"Presentation done in Airbus Helicopters for the PMI France Chapter. Click to see the presentation . ( June 2015 )","title":"The Various Stages of Digital Transformation"},{"location":"graph/first-article/","text":"First article on graph-oriented programming In 2013, I started a personal project around directed attributed graph databases. The objective was to find a programming model that would enable the software to be as \"soft\" as the database. First works The concept was defined and setup during 2014 and 2015. Two articles on graph transformations were a crucial inspiration to the project. The AGG Aproach published in the volume 2 of the \"Handbook of Graph Grammars\". AGG was particularly inspiring. Ermel, Claudia, Michael Rudolf, and Gabriele Taentzer. \"The AGG approach: Language and environment.\" Handbook Of Graph Grammars And Computing By Graph Transformation: Volume 2: Applications, Languages and Tools. 1999. 551-603. Another article from the same book was also a source of inspiration, \"The Progres approach\" (which doesn't seem to be available online). Sch\u00fcrr, Andy, Andreas J. Winter, and Albert Z\u00fcndorf. \"The PROGRES approach: Language and environment.\" Handbook Of Graph Grammars And Computing By Graph Transformation: Volume 2: Applications, Languages and Tools. 1999. 487-550. Beginning of 2015, the graph-oriented programming concept was completed, and its capability of solving the useless technical debt was theoretically proven. The graph transformations are playing a core role in graph-oriented programming. Closed-source prototype In 2016, it was prototyped in a company called GraphApps in a closed source approach. I am no longer part of this company. Global article on graph-oriented programming A very first article was written in 2016 to explain all the concepts of this programming model. This article is far from being perfect and didactic but it browses a lot of topics that would deserve special attention (especially in the part concerning graph transformations). The original article can be found here: The graph-oriented programming paradigm . (July 2018)","title":"First article on graph-oriented programming"},{"location":"graph/first-article/#first-article-on-graph-oriented-programming","text":"In 2013, I started a personal project around directed attributed graph databases. The objective was to find a programming model that would enable the software to be as \"soft\" as the database.","title":"First article on graph-oriented programming"},{"location":"graph/first-article/#first-works","text":"The concept was defined and setup during 2014 and 2015. Two articles on graph transformations were a crucial inspiration to the project. The AGG Aproach published in the volume 2 of the \"Handbook of Graph Grammars\". AGG was particularly inspiring. Ermel, Claudia, Michael Rudolf, and Gabriele Taentzer. \"The AGG approach: Language and environment.\" Handbook Of Graph Grammars And Computing By Graph Transformation: Volume 2: Applications, Languages and Tools. 1999. 551-603. Another article from the same book was also a source of inspiration, \"The Progres approach\" (which doesn't seem to be available online). Sch\u00fcrr, Andy, Andreas J. Winter, and Albert Z\u00fcndorf. \"The PROGRES approach: Language and environment.\" Handbook Of Graph Grammars And Computing By Graph Transformation: Volume 2: Applications, Languages and Tools. 1999. 487-550. Beginning of 2015, the graph-oriented programming concept was completed, and its capability of solving the useless technical debt was theoretically proven. The graph transformations are playing a core role in graph-oriented programming.","title":"First works"},{"location":"graph/first-article/#closed-source-prototype","text":"In 2016, it was prototyped in a company called GraphApps in a closed source approach. I am no longer part of this company.","title":"Closed-source prototype"},{"location":"graph/first-article/#global-article-on-graph-oriented-programming","text":"A very first article was written in 2016 to explain all the concepts of this programming model. This article is far from being perfect and didactic but it browses a lot of topics that would deserve special attention (especially in the part concerning graph transformations). The original article can be found here: The graph-oriented programming paradigm . (July 2018)","title":"Global article on graph-oriented programming"},{"location":"graph/staf-icgt2018/","text":"Conference at the STAF/ICGT 2018 in Toulouse The prototyping works of the GraphApps company were stopped in December 2016 for economic reasons, the products (Design and Web Framework) staying closed source. However, I kept on working on the graph-oriented programming topic, in particular in the perspective of solving the technical debt. Marburg University During Fall 2017, some conference calls were organized with Prof. Dr. Gabriele Taentzer . Those discussions were about graph-oriented programming but also about Henshin . Beginning of 2018, Prof. Dr. Taentzer proposed that I was keynote speaker at the STAF/ICGT 2018 in Toulouse, conference organized by Dr. Leen Lambers and Prof. Dr. Jens Weber . I thank all of them for their invitation and support. Introductory papers For the conference, two introductory papers were produced: An abstract of the presentation for the Springer proceedings ; A non published introduction to graph-oriented programming. Slides of the presentation The slides of the presentation can be found hereafter: Introduction to graph-oriented programming A copy of those slides can also be found on the conference website here . What's next? My works on graph-oriented programming will go on, but with probably less time that I had in 2016. (July 2018)","title":"Conference at the STAF/ICGT 2018 in Toulouse"},{"location":"graph/staf-icgt2018/#conference-at-the-staficgt-2018-in-toulouse","text":"The prototyping works of the GraphApps company were stopped in December 2016 for economic reasons, the products (Design and Web Framework) staying closed source. However, I kept on working on the graph-oriented programming topic, in particular in the perspective of solving the technical debt.","title":"Conference at the STAF/ICGT 2018 in Toulouse"},{"location":"graph/staf-icgt2018/#marburg-university","text":"During Fall 2017, some conference calls were organized with Prof. Dr. Gabriele Taentzer . Those discussions were about graph-oriented programming but also about Henshin . Beginning of 2018, Prof. Dr. Taentzer proposed that I was keynote speaker at the STAF/ICGT 2018 in Toulouse, conference organized by Dr. Leen Lambers and Prof. Dr. Jens Weber . I thank all of them for their invitation and support.","title":"Marburg University"},{"location":"graph/staf-icgt2018/#introductory-papers","text":"For the conference, two introductory papers were produced: An abstract of the presentation for the Springer proceedings ; A non published introduction to graph-oriented programming.","title":"Introductory papers"},{"location":"graph/staf-icgt2018/#slides-of-the-presentation","text":"The slides of the presentation can be found hereafter: Introduction to graph-oriented programming A copy of those slides can also be found on the conference website here .","title":"Slides of the presentation"},{"location":"graph/staf-icgt2018/#whats-next","text":"My works on graph-oriented programming will go on, but with probably less time that I had in 2016. (July 2018)","title":"What's next?"},{"location":"research/DSL-for-graph-topology-checks/","text":"A DSL for checking the topology of the graph before transforming it Two kinds of graph transformations (GT) GTs that start with a root node. GTs that aim at making several modifications in the graph (generally using pattern matching expressions). We are focused on the first kind of GT. A DSL to chack the topology It is possible to create a DSL to check the basic applicability of a GT. The DSL can propose a set of primitives to make assertions on the graph, typically seen as starting at a certain root node (so we have an artefact for root node, let's says it is \"root\"). For instance root:A -[:]- :B -[:R]- :C means that the root node which is an instance of A should be connected through whatever relationship to an instance of B which is connected by a relationship of type R to an instance of C. This syntax is inspired by Open Cypher . In this DSL, it is possible to express complex checks like: ASSERT { ( RN:A -[:]- x:B -[:R]- :C ) AND ( ( x:B -[NOT]- :D) OR ( (x:B -[:]- y:E) AND ( y.att12 == 'foo') ) ) } The DSL could not manage: Variables, Attribute value checks. It would be a pure graph structure analysis. But, even if the topology conditions are met, if the DSL is too simple, we have to recode the graph navigation in the GT in order, for instance, to make a check on the instance of C. Advantages and drawbacks This DSL works. However, I am not entirely satisfied about it, in the same way that I am not completely satisfied about the rules description in general. Being a kind of pseudo-code (even if more generic), the DSL: Is a sort of new programming language; The frontier between what we express in this DSL and what must be in the code is vague; The topology checks are often depending on complex business conditions, which means that in real code, you may have topology conditions that you must code manually and that we cannot express really by this DSL. So, the DSL is reduced to a kind of \"helper\" that is addressing many cases in simple applications (or let's say in applications where the rules are simple), but cannot be used as a new \"structural way of coding\" graph transformations. Limitations This limitation opens the problem of theoretical check of GT applicabilities in case of GT forks (see the fork concept .","title":"DSL for graph topology check"},{"location":"research/DSL-for-graph-topology-checks/#a-dsl-for-checking-the-topology-of-the-graph-before-transforming-it","text":"","title":"A DSL for checking the topology of the graph before transforming it"},{"location":"research/DSL-for-graph-topology-checks/#two-kinds-of-graph-transformations-gt","text":"GTs that start with a root node. GTs that aim at making several modifications in the graph (generally using pattern matching expressions). We are focused on the first kind of GT.","title":"Two kinds of graph transformations (GT)"},{"location":"research/DSL-for-graph-topology-checks/#a-dsl-to-chack-the-topology","text":"It is possible to create a DSL to check the basic applicability of a GT. The DSL can propose a set of primitives to make assertions on the graph, typically seen as starting at a certain root node (so we have an artefact for root node, let's says it is \"root\"). For instance root:A -[:]- :B -[:R]- :C means that the root node which is an instance of A should be connected through whatever relationship to an instance of B which is connected by a relationship of type R to an instance of C. This syntax is inspired by Open Cypher . In this DSL, it is possible to express complex checks like: ASSERT { ( RN:A -[:]- x:B -[:R]- :C ) AND ( ( x:B -[NOT]- :D) OR ( (x:B -[:]- y:E) AND ( y.att12 == 'foo') ) ) } The DSL could not manage: Variables, Attribute value checks. It would be a pure graph structure analysis. But, even if the topology conditions are met, if the DSL is too simple, we have to recode the graph navigation in the GT in order, for instance, to make a check on the instance of C.","title":"A DSL to chack the topology"},{"location":"research/DSL-for-graph-topology-checks/#advantages-and-drawbacks","text":"This DSL works. However, I am not entirely satisfied about it, in the same way that I am not completely satisfied about the rules description in general. Being a kind of pseudo-code (even if more generic), the DSL: Is a sort of new programming language; The frontier between what we express in this DSL and what must be in the code is vague; The topology checks are often depending on complex business conditions, which means that in real code, you may have topology conditions that you must code manually and that we cannot express really by this DSL. So, the DSL is reduced to a kind of \"helper\" that is addressing many cases in simple applications (or let's say in applications where the rules are simple), but cannot be used as a new \"structural way of coding\" graph transformations.","title":"Advantages and drawbacks"},{"location":"research/DSL-for-graph-topology-checks/#limitations","text":"This limitation opens the problem of theoretical check of GT applicabilities in case of GT forks (see the fork concept .","title":"Limitations"},{"location":"research/about-ml/","text":"Reflections on artificial neural networks and machine learning Photo by oliver brandt from FreeImages Machine learning based on artificial neural networks attracted much attention and credits those last decades (after a long \"winter of AI\") up to the point that, boosted by marketing, an anti-scientific approach is sold as being the solution to all problems. In this article, we will try to recall the simple mathematical foundation behind artificial neural networks, and why this technology is not reliable, even if it attracks massive investments. We will also analyze the philosophical side of the neural networks. Note: We will focus on supervised learning artificial neural networks and will not talk now about other variants such as unsupervised learning. What is a neural network? Basically, an artificial neural network is: A mathematical function f of several variable; A complex function construction algorithm: Using a representation of neurons and axons that was used to create the function f , Based on a list of inputs of the form (x 1 , x 2 , ..., x n ), matching with outputs of the form (y 1 , y 2 , ..., y p ). Figure 1: A simple artificial neural network The Figure 1 shows a sample of an artificial neural network. There are many techniques to build the function f , hence the variety of machine learning algorithms. An interpolation function Let us go a bit more into the details. Let us note : X k = (x k1 , x k2 , ..., x kn ) And Y m = (y m1 , y m2 , ..., y mp ). Let us suppose we have a set of known inputs and related known outputs, (X k , Y m ) for (k,m) known. Considering a predefined neural network with nodes and edges in layers, we can use an algorithm to define characteristics of nodes and edges so that we define a function f like following: f : D 1 x D 2 x ... X D n R 1 x R 2 x ... x R p (x 1 , x 2 , ..., x n ) (y 1 , y 2 , ..., y p ) To create this function, the algorithm used tried, more of less, to satisfy the following hypothesis: f(X k ) = Y m This process is called \"training the network\". The resulting function f is an interpolation function, defined by an algorithm. Once defined, f is a function of many variables that has the following characteristics: It is probable that f(X k ) = Y m or | f(X k ) - Y m | with \"|\" a norm in the R 1 x R 2 x ... x R p space; The determined global function is not globally mathematically known, which implies it can have singular points, often called \"potential wells\". An interpolation function working in extrapolation mode The idea of neural networks is to assume that f is a continuous function at least in the neighborhood of the known hyper-points X i . That means that: If we take X i so that | X k - X i | Then | f(X k ) - f(X i ) | ' Said differently, any point in the neighborhood of X k will have its image by f in the neighborhood of f(X k ). When f matches this assertion, we can use f as an extrapolation function and consider that its evaluation on X i is valid. The problem is that we have no ways of knowing that f is really continuous in a neighborhood of an hyper-point. Let us consider the Figure 2. Figure 2: Two potential wells in a surface. Image from wired.com The Figure 2 is an artist representation of two singularities in a 3D curve: One where z tends to + , One where z tends to - . That means, for the first singularity, that we have the following situation: For a X i so that | X k - X i | | f(X k ) - f(X i ) | ' The presence of potential wells in neural networks was a well known phenomenon in the 80s, and one of the reasons of the AI winter. Because neural networks were considered as non reliable \"by construction\", those techniques were temporarily abandoned. Mathematically speaking, the situation is quite simple: An interpolation function should not be used as an extrapolation function. New beliefs and arguments: Big data and processing power The development and explosion of machine learning beginning of the 2000s was funded on new beliefs that overpass the previous arguments and mathematical reality. The first idea (which is an engineer's idea) is that if we have a huge number of data (i.e. a lot of couples of the form (X k , Y m )) to feed the algorithm, we will be able to generate many hyper-points in the f hyper-surface. This will imply that the f function will be more \"dense\" in R 1 x R 2 x ... x R p and so we will \"reduce\" the risk of potential well. This idea is, from an engineering standpoint, of good sense, but mathematically, being able to generate one thousand points or one billion points will not change the mathematical reality of potential wells, especially when we talk about functions of many variables. Let's take the example of the image classifier. First of all, we have to create a program to \"encode\" whatever image in the X k form. Then, in supervised learning, we will associate to an image representation X k a value Y m representing the result, for instance a number. Let us suppose the number is 12 and it represents the banana. The f function that will be generated by the algorithm will be the result of \"training\" of many images associated with the result we expect. Then, when a new image will come, we will transform if into a X i , hoping that f(X i ) will be meaningful. Figure 3: The banana disappears. Taken from Brown et al. 2017 The fact is, it is a hope. What is funny is that it works in many cases. And when it doesn't, some various techniques are existing to try to make it work anyway. We can note that, in that process, the encoding function may be quite important. In a certain way, it can hide a structural analysis of the problem, as it is representing the reality. But the banana disappears... Figure 4: The banana disappears. Taken from Brown et al. 2017 Without any surprise, Brown et al. demonstrated in their article Adversarial patch (see Figure 4) that it was possible to mislead a neural network training to recognize some images. It may be even worse, because they claim it is possible to structurally mislead all neural networks that were used as image classifiers. For us, despite the interest of the article, we are only rediscovering the structural mathematical problem of this technique. New beliefs contradicting the scientific method The scientific method is what built the technical foundations of our world. Science is a method and is aiming at being exact or at mastering the margin of error. With machine learning, the scientific is completely violated. Belief 1: Big data and brute force can solve mathematical problems This a way to reformulate the big data belief: If we inject billions of images to create an image classifier with an artificial neural network, we will overcome the mathematical problem. For sure, to be able to do that, we need an enormous processing power. We saw that this approach was not working. Without despising engineering (the author himself is an engineer ;), we see here an engineering reasoning: find all recipes to make it work for the maximum number of cases. Unfortunately, mathematics cannot be solved by brute force. Another issue that we see with big data is that no data set is completely neutral. Data sets are coming from somewhere and they have inherent bias attached to them. This is natural when we consider the real nature of data . Belief 2: Programs can make mistakes like humans The second idea (also an engineer's idea) is to say that better have the neural network than nothing . At least, \"most of the time\", the extrapolation does work and provides relevant results. That is a fundamental regression compared to the objectives that computer science targeted from the beginning of the discipline. Computers, as they were automating human tasks were trustable because they were exact. In a certain sense, we can delegate tasks to computers provided they make no mistake and we can trust them, better than if the tasks were performed by humans. We even created the notion of \"bug\" to name a problem to be fixed in a program, for the computer to to exact, and so reliable. The argumentation is that, as humans make mistakes, it is acceptable that computers do too... If computer make errors in a structural way (which is the case of artificial neural networks), we change the complete computer science paradigm. Computers could be at the source of serious administrative mistakes, medical mistakes, justice mistakes... Even is statistically, those mistakes happen 1% of the time, how to trust computer programs anymore? If we accept to generalize techniques that make errors, we enter into a non certain world, that looks like the Brazil movie. Belief 3: The marvelous ubiquitous set of tools for functionally unskilled people In the hype of machine learning, another element is crucial. It is possible to learn the data crunching techniques separately from any other functional knowledge, and to pretend being able to apply them to whatever functional domain. In a sense, machine learning is the ubiquitous set of tools . More: We don't need to be functionally skilled, meaning in the business area where we do data science . Our skills in machine learning will be enough. We tried to explain, in our article the real nature of data , why data was a complex output product , result of the intersection of a semantic domain and some use cases. With machine learning, we take a reverse approach: Data becomes the input and you can discover things from it with generic tools without even knowing their limitations! We can note that if the analyzed data are the fruit of a highly semantically standardized process (which is the case in many areas where the regulator or standards structured a business domain), machine learning can \"rediscover\" elements of this standardization, alimenting the illusion that it found sense in data - whereas it only uncovered a part of the semantic standardization that help created the data! One interesting question that we can ask considering this article is: Is the second image with the sticker in the \"neighborhood\" of the first one? We can see, asking this question, that the very notion of neighborhood seems not defined in the \"space of images\". It seems only defined in the space of (X i ) which are the results of the encoding of the images. In a certain way, we lack of a mathematical theory to define properly the notion of continuity in our input space. Intuitively the second image is close to the first one in many areas (even identical) but it is radically different in others. So maybe by changing the encoder, we could obtain better results. For instance, an encoder that would encode various zones would train the neural network with a specific structure, fruit of a functional analysis. We begin to see this approach in some articles, especially when machine learning is associated with structured data. Provided the input is structured data and the encoder is \"well defined\", then the machine learning algorithm can create an interpolation function that will rely on encoded structured data as its input. This does not solve the structural problem of mathematical potential wells, but it can enhance this imperfect technology. Note that, to enhance it, we have to get back to a representation of reality that is the fruit of functional analysis and so, we have to come back to standard science: Analysis of reality, creation of a model for reality, testing the model versus the reality, enhancing the model to fit better to reality. This is not intelligence, this is even not science For sure, artificial neural networks are not intelligence, far from it. It is not science, because science always took the same method: First, observe what is; Second, try to model what is, with laws and operational models implementing those laws; Third, iterate and compare the laws of your model and the reality and align your laws on the reality. Neural networks are a complete perversion of the scientific method because: There is no thinking about a law; instead, this is replaced by an algorithm generating a function based on data; The only way to enrich your model when you don't understand what it is doing is to retrain it with all the samples that were in error, hoping that the new model will be better because it was trained on more data; That means you can't enrich your laws but just throw your model and create a new one with more data, without any indication that this new model will be better than the previous one or will not show flows where the previous one didn't have those. Indeed, you don't enhance knowledge, because the create model is not the fruit of intelligence, observation, modeling and tuning; it is the fruit of an algorithm working on data. There is no understanding on what's going on at all. There is no progress of science. On the contrary, by taking output products as input products, we can be in incredibly absurd situations. A good image would be to try to understand how Emacs works on a PC with the database of all electrical currents in a computer. Data are an output product, and they were generated in a very specific semantic context with very specific business rules. Reprendre ici Semantic representation of reality In software and regulation Two main components: Semantic meta-model, enables the structuration of real data Business rules, based on logic Many representations of reality representation is not unique = bridge on subsets Reasoning on data with semantic understanding Rough notes Je n'avais pas vu cette vid\u00e9o avant aujourd'hui. C'est int\u00e9ressant. Des chercheurs qui se mettent \u00e0 penser comme des ing\u00e9nieurs, en couplant le machine learning avec de la logique. J'avoue que \u00e7a ressemble un peu \u00e0 une tentative de surfer sur la vague ML. C'est b\u00eate que la science soit soumise \u00e0 la mode \u00e0 ce point. Si je comprends l'objectif, relativement au monde de la recherche qui est d\u00e9crit, je reste toujours un peu sur ma faim par rapport \u00e0 ce que je vis au quotidien depuis pr\u00e8s de 30 ans. Toute l'industrie au sens large (industrie et tertiaire) lutte depuis les d\u00e9buts de l'IT pour d\u00e9finir des s\u00e9mantiques communes, ou autrement dit des fa\u00e7ons communes de repr\u00e9senter les choses (et quand ce n'est pas possibles des fa\u00e7ons communes d'\u00e9changer les donn\u00e9es). Ainsi, beaucoup de m\u00e9tiers ont \u00e9t\u00e9 \"standardis\u00e9s\" dans leur mod\u00e9lisation, que ce soit au travers de groupes de travail internationaux, ou au travers de progiciels ayant structur\u00e9 la repr\u00e9sentation s\u00e9mantique, ou au travers des r\u00e9gulateurs. Avec le temps, les mod\u00e8les s\u00e9mantiques s'enrichissent. C'est ce que je tente d'expliquer dans https://orey.github.io/papers/articles/data-interop/ . Or, avoir des m\u00e9ta-mod\u00e8les, m\u00eame imparfaits, est immens\u00e9ment plus pertinent que de faire confiance \u00e0 du ML bourrin, s'appuyant qui plus est sur des donn\u00e9es venant d'on ne sait o\u00f9. On peut g\u00e9n\u00e9rer des r\u00e8gles depuis un m\u00e9ta-mod\u00e8le s\u00e9mantique et donc raisonner sur ce m\u00e9ta-mod\u00e8le, et donc raisonner sur les donn\u00e9es qui sont instanci\u00e9es depuis ce m\u00e9ta-mod\u00e8le. Je pense qu'avec un ensemble de r\u00e8gles logiques basiques, on peut arriver \u00e0 des r\u00e9sultats tr\u00e8s int\u00e9ressants. Mais consid\u00e9rer les donn\u00e9es dans l'absolu et balancer un moteur de ML dessus, c'est vraiment le degr\u00e9 z\u00e9ro de la pens\u00e9e s\u00e9mantique :) Note que pas mal de gens sont s\u00e9duits par l'approche dans les entreprises, parce que cela \u00e9vite de r\u00e9fl\u00e9chir. Sans rien comprendre aux donn\u00e9es et aux r\u00e8gles de gestion qui leur sont associ\u00e9es, ils font du ML en esp\u00e9rant \"trouver des choses\"... Evidemment, \u00e7a ne donne rien. Je ne sais pas si ce monsieur est repr\u00e9sentatif, mais la recherche semble vouloir \u00e9tudier les mod\u00e8les de raisonnements \"dans l'absolu\" (ce qui ouvre sur des complexit\u00e9s folles selon les op\u00e9rateurs concern\u00e9s) ou s'appuyer sur du ML qui n'est rien d'autre qu'un algo de construction de fonction d'interpolation, mais qui est stupide s\u00e9mantiquement. C'est un peu les deux extr\u00eames. Et, comme souvent dans ce domaine, quand un exemple concret est pris, les gens regardent des sites web du m\u00eame domaine et d\u00e9terminent des r\u00e8gles a posteriori alors que le m\u00e9tier consid\u00e9r\u00e9 est hyper r\u00e9glement\u00e9 par un m\u00e9ta mod\u00e8le s\u00e9mantique que le r\u00e9gulateur ou l'industrie consid\u00e9r\u00e9e a mis des dizaines d'ann\u00e9es \u00e0 construire. Heureusement que l'on arrive \u00e0 d\u00e9terminer des r\u00e8gles, qui sont pour moi la cons\u00e9quence du mod\u00e8le s\u00e9mantique sous-jacent, et que ces r\u00e8gles fonctionnent ! Car le m\u00e9tier, lui, fonctionne. Je trouve toujours cette fa\u00e7on de faire un peu \u00e9trange. Ce qui serait utile \u00e0 l'industrie, ce serait de partir des mod\u00e8les s\u00e9mantiques et d'introduire des r\u00e8gles de raisonnement ou de transformation sur ces mod\u00e8les. On aurait alors de vraies IA qui travaillent sur des m\u00e9ta-mod\u00e8les s\u00e9mantiques leur permettant de comprendre les donn\u00e9es s\u00e9mantiquement structur\u00e9es. En fait, je trouve que le hype du ML est \u00e0 l'image de notre monde. Sans que les gens ne s'en rendent compte, ils ont r\u00e9duit leurs ambitions qui sont devenues minuscules, ou sont invers\u00e9es par rapport \u00e0 ce qu'il faudrait faire (il y a quelque chose de diabolique l\u00e0-dedans ;) :On ne cherche plus \u00e0 comprendre, on cherche des r\u00e9sultats, sans savoir ce qu'on cherche. On cherche des r\u00e8gles sur des donn\u00e9es qui sont la fin d'un processus, alors que ces donn\u00e9es ont \u00e9t\u00e9 cr\u00e9\u00e9es selon un processus s\u00e9mantiquement riche avec des r\u00e8gles connues en amont !!! C'est comme tenter de comprendre fonctionne les apps Android en mesurant les courants de tous les fils internes du t\u00e9l\u00e9phone.En fait, on ne travaille pas, on ne fait pas d'efforts, on fait des trucs qui n'ont aucun sens, tellement on a peur de r\u00e9fl\u00e9chir... ou d'apprendre. On ne veut plus faire de la science. La science, c'est un mod\u00e8le qui repr\u00e9sente la r\u00e9alit\u00e9 et qui nous permet de comprendre les lois de la nature. La logique est dans la science, mais le ML est dans la pens\u00e9e magique. On divinise un outil \u00e0 deux balles, le \"r\u00e9seau de neurones\", qui est un algo de construction d'une fonction d'interpolation que l'on utilise en mode extrapolation. Et on s'\u00e9tonne que la banane disparaisse en mettant un sticker ! Cela montre juste que l'on ne ma\u00eetrise rien de la fonction multidimensionnelle g\u00e9n\u00e9r\u00e9e. Ce serait rigolo si cela ne traduisait pas une profonde incompr\u00e9hension des math\u00e9matiques les plus basiques. Philosophiquement, le ML est un vrai aveu d'\u00e9chec \u00e0 donner du sens au monde, en prenant les choses par la fin, par la data, par le produit fini, par l'infrastructure informatique. C'est ne pas comprendre la cha\u00eene des causes et des cons\u00e9quences : c'est commencer par la cons\u00e9quence et la consid\u00e9rer comme une cause. Pas \u00e9tonnant que Google mette aux nues cette technique. C'est \u00e0 leur image : bourrin et tr\u00e8s b\u00eate :) L\u00e0 o\u00f9 le monsieur me fait un peu peur (mais c'est sans doute juste une posture pour obtenir des cr\u00e9dits de recherche), c'est qu'il donne du cr\u00e9dit au ML. Je ne comprends pas comment un logicien peut donner du cr\u00e9dit \u00e0 cela. Pour moi, l'histoire de l'informatique recherche un autre objectif que toute cette tendance : elle cherche \u00e0 faire marcher des choses, \u00e0 informatiser des m\u00e9tiers et donc \u00e0 les conna\u00eetre, les mod\u00e9liser, les comprendre intimement que ce soit dans la structure de leur connaissance ou dans leurs r\u00e8gles de gestion (bas\u00e9es sur des r\u00e8gles logiques).","title":"Reflections on artificial neural networks and machine learning"},{"location":"research/about-ml/#reflections-on-artificial-neural-networks-and-machine-learning","text":"Photo by oliver brandt from FreeImages Machine learning based on artificial neural networks attracted much attention and credits those last decades (after a long \"winter of AI\") up to the point that, boosted by marketing, an anti-scientific approach is sold as being the solution to all problems. In this article, we will try to recall the simple mathematical foundation behind artificial neural networks, and why this technology is not reliable, even if it attracks massive investments. We will also analyze the philosophical side of the neural networks. Note: We will focus on supervised learning artificial neural networks and will not talk now about other variants such as unsupervised learning.","title":"Reflections on artificial neural networks and machine learning"},{"location":"research/about-ml/#what-is-a-neural-network","text":"Basically, an artificial neural network is: A mathematical function f of several variable; A complex function construction algorithm: Using a representation of neurons and axons that was used to create the function f , Based on a list of inputs of the form (x 1 , x 2 , ..., x n ), matching with outputs of the form (y 1 , y 2 , ..., y p ). Figure 1: A simple artificial neural network The Figure 1 shows a sample of an artificial neural network. There are many techniques to build the function f , hence the variety of machine learning algorithms.","title":"What is a neural network?"},{"location":"research/about-ml/#an-interpolation-function","text":"Let us go a bit more into the details. Let us note : X k = (x k1 , x k2 , ..., x kn ) And Y m = (y m1 , y m2 , ..., y mp ). Let us suppose we have a set of known inputs and related known outputs, (X k , Y m ) for (k,m) known. Considering a predefined neural network with nodes and edges in layers, we can use an algorithm to define characteristics of nodes and edges so that we define a function f like following: f : D 1 x D 2 x ... X D n R 1 x R 2 x ... x R p (x 1 , x 2 , ..., x n ) (y 1 , y 2 , ..., y p ) To create this function, the algorithm used tried, more of less, to satisfy the following hypothesis: f(X k ) = Y m This process is called \"training the network\". The resulting function f is an interpolation function, defined by an algorithm. Once defined, f is a function of many variables that has the following characteristics: It is probable that f(X k ) = Y m or | f(X k ) - Y m | with \"|\" a norm in the R 1 x R 2 x ... x R p space; The determined global function is not globally mathematically known, which implies it can have singular points, often called \"potential wells\".","title":"An interpolation function"},{"location":"research/about-ml/#an-interpolation-function-working-in-extrapolation-mode","text":"The idea of neural networks is to assume that f is a continuous function at least in the neighborhood of the known hyper-points X i . That means that: If we take X i so that | X k - X i | Then | f(X k ) - f(X i ) | ' Said differently, any point in the neighborhood of X k will have its image by f in the neighborhood of f(X k ). When f matches this assertion, we can use f as an extrapolation function and consider that its evaluation on X i is valid. The problem is that we have no ways of knowing that f is really continuous in a neighborhood of an hyper-point. Let us consider the Figure 2. Figure 2: Two potential wells in a surface. Image from wired.com The Figure 2 is an artist representation of two singularities in a 3D curve: One where z tends to + , One where z tends to - . That means, for the first singularity, that we have the following situation: For a X i so that | X k - X i | | f(X k ) - f(X i ) | ' The presence of potential wells in neural networks was a well known phenomenon in the 80s, and one of the reasons of the AI winter. Because neural networks were considered as non reliable \"by construction\", those techniques were temporarily abandoned. Mathematically speaking, the situation is quite simple: An interpolation function should not be used as an extrapolation function.","title":"An interpolation function working in extrapolation mode"},{"location":"research/about-ml/#new-beliefs-and-arguments-big-data-and-processing-power","text":"The development and explosion of machine learning beginning of the 2000s was funded on new beliefs that overpass the previous arguments and mathematical reality. The first idea (which is an engineer's idea) is that if we have a huge number of data (i.e. a lot of couples of the form (X k , Y m )) to feed the algorithm, we will be able to generate many hyper-points in the f hyper-surface. This will imply that the f function will be more \"dense\" in R 1 x R 2 x ... x R p and so we will \"reduce\" the risk of potential well. This idea is, from an engineering standpoint, of good sense, but mathematically, being able to generate one thousand points or one billion points will not change the mathematical reality of potential wells, especially when we talk about functions of many variables. Let's take the example of the image classifier. First of all, we have to create a program to \"encode\" whatever image in the X k form. Then, in supervised learning, we will associate to an image representation X k a value Y m representing the result, for instance a number. Let us suppose the number is 12 and it represents the banana. The f function that will be generated by the algorithm will be the result of \"training\" of many images associated with the result we expect. Then, when a new image will come, we will transform if into a X i , hoping that f(X i ) will be meaningful. Figure 3: The banana disappears. Taken from Brown et al. 2017 The fact is, it is a hope. What is funny is that it works in many cases. And when it doesn't, some various techniques are existing to try to make it work anyway. We can note that, in that process, the encoding function may be quite important. In a certain way, it can hide a structural analysis of the problem, as it is representing the reality.","title":"New beliefs and arguments: Big data and processing power"},{"location":"research/about-ml/#but-the-banana-disappears","text":"Figure 4: The banana disappears. Taken from Brown et al. 2017 Without any surprise, Brown et al. demonstrated in their article Adversarial patch (see Figure 4) that it was possible to mislead a neural network training to recognize some images. It may be even worse, because they claim it is possible to structurally mislead all neural networks that were used as image classifiers. For us, despite the interest of the article, we are only rediscovering the structural mathematical problem of this technique.","title":"But the banana disappears..."},{"location":"research/about-ml/#new-beliefs-contradicting-the-scientific-method","text":"The scientific method is what built the technical foundations of our world. Science is a method and is aiming at being exact or at mastering the margin of error. With machine learning, the scientific is completely violated.","title":"New beliefs contradicting the scientific method"},{"location":"research/about-ml/#belief-1-big-data-and-brute-force-can-solve-mathematical-problems","text":"This a way to reformulate the big data belief: If we inject billions of images to create an image classifier with an artificial neural network, we will overcome the mathematical problem. For sure, to be able to do that, we need an enormous processing power. We saw that this approach was not working. Without despising engineering (the author himself is an engineer ;), we see here an engineering reasoning: find all recipes to make it work for the maximum number of cases. Unfortunately, mathematics cannot be solved by brute force. Another issue that we see with big data is that no data set is completely neutral. Data sets are coming from somewhere and they have inherent bias attached to them. This is natural when we consider the real nature of data .","title":"Belief 1: Big data and brute force can solve mathematical problems"},{"location":"research/about-ml/#belief-2-programs-can-make-mistakes-like-humans","text":"The second idea (also an engineer's idea) is to say that better have the neural network than nothing . At least, \"most of the time\", the extrapolation does work and provides relevant results. That is a fundamental regression compared to the objectives that computer science targeted from the beginning of the discipline. Computers, as they were automating human tasks were trustable because they were exact. In a certain sense, we can delegate tasks to computers provided they make no mistake and we can trust them, better than if the tasks were performed by humans. We even created the notion of \"bug\" to name a problem to be fixed in a program, for the computer to to exact, and so reliable. The argumentation is that, as humans make mistakes, it is acceptable that computers do too... If computer make errors in a structural way (which is the case of artificial neural networks), we change the complete computer science paradigm. Computers could be at the source of serious administrative mistakes, medical mistakes, justice mistakes... Even is statistically, those mistakes happen 1% of the time, how to trust computer programs anymore? If we accept to generalize techniques that make errors, we enter into a non certain world, that looks like the Brazil movie.","title":"Belief 2: Programs can make mistakes like humans"},{"location":"research/about-ml/#belief-3-the-marvelous-ubiquitous-set-of-tools-for-functionally-unskilled-people","text":"In the hype of machine learning, another element is crucial. It is possible to learn the data crunching techniques separately from any other functional knowledge, and to pretend being able to apply them to whatever functional domain. In a sense, machine learning is the ubiquitous set of tools . More: We don't need to be functionally skilled, meaning in the business area where we do data science . Our skills in machine learning will be enough. We tried to explain, in our article the real nature of data , why data was a complex output product , result of the intersection of a semantic domain and some use cases. With machine learning, we take a reverse approach: Data becomes the input and you can discover things from it with generic tools without even knowing their limitations! We can note that if the analyzed data are the fruit of a highly semantically standardized process (which is the case in many areas where the regulator or standards structured a business domain), machine learning can \"rediscover\" elements of this standardization, alimenting the illusion that it found sense in data - whereas it only uncovered a part of the semantic standardization that help created the data! One interesting question that we can ask considering this article is: Is the second image with the sticker in the \"neighborhood\" of the first one? We can see, asking this question, that the very notion of neighborhood seems not defined in the \"space of images\". It seems only defined in the space of (X i ) which are the results of the encoding of the images. In a certain way, we lack of a mathematical theory to define properly the notion of continuity in our input space. Intuitively the second image is close to the first one in many areas (even identical) but it is radically different in others. So maybe by changing the encoder, we could obtain better results. For instance, an encoder that would encode various zones would train the neural network with a specific structure, fruit of a functional analysis. We begin to see this approach in some articles, especially when machine learning is associated with structured data. Provided the input is structured data and the encoder is \"well defined\", then the machine learning algorithm can create an interpolation function that will rely on encoded structured data as its input. This does not solve the structural problem of mathematical potential wells, but it can enhance this imperfect technology. Note that, to enhance it, we have to get back to a representation of reality that is the fruit of functional analysis and so, we have to come back to standard science: Analysis of reality, creation of a model for reality, testing the model versus the reality, enhancing the model to fit better to reality.","title":"Belief 3: The marvelous ubiquitous set of tools for functionally unskilled people"},{"location":"research/about-ml/#this-is-not-intelligence-this-is-even-not-science","text":"For sure, artificial neural networks are not intelligence, far from it. It is not science, because science always took the same method: First, observe what is; Second, try to model what is, with laws and operational models implementing those laws; Third, iterate and compare the laws of your model and the reality and align your laws on the reality. Neural networks are a complete perversion of the scientific method because: There is no thinking about a law; instead, this is replaced by an algorithm generating a function based on data; The only way to enrich your model when you don't understand what it is doing is to retrain it with all the samples that were in error, hoping that the new model will be better because it was trained on more data; That means you can't enrich your laws but just throw your model and create a new one with more data, without any indication that this new model will be better than the previous one or will not show flows where the previous one didn't have those. Indeed, you don't enhance knowledge, because the create model is not the fruit of intelligence, observation, modeling and tuning; it is the fruit of an algorithm working on data. There is no understanding on what's going on at all. There is no progress of science. On the contrary, by taking output products as input products, we can be in incredibly absurd situations. A good image would be to try to understand how Emacs works on a PC with the database of all electrical currents in a computer. Data are an output product, and they were generated in a very specific semantic context with very specific business rules. Reprendre ici","title":"This is not intelligence, this is even not science"},{"location":"research/about-ml/#semantic-representation-of-reality","text":"In software and regulation Two main components: Semantic meta-model, enables the structuration of real data Business rules, based on logic","title":"Semantic representation of reality"},{"location":"research/about-ml/#many-representations-of-reality","text":"representation is not unique = bridge on subsets","title":"Many representations of reality"},{"location":"research/about-ml/#reasoning-on-data-with-semantic-understanding","text":"","title":"Reasoning on data with semantic understanding"},{"location":"research/about-ml/#rough-notes","text":"Je n'avais pas vu cette vid\u00e9o avant aujourd'hui. C'est int\u00e9ressant. Des chercheurs qui se mettent \u00e0 penser comme des ing\u00e9nieurs, en couplant le machine learning avec de la logique. J'avoue que \u00e7a ressemble un peu \u00e0 une tentative de surfer sur la vague ML. C'est b\u00eate que la science soit soumise \u00e0 la mode \u00e0 ce point. Si je comprends l'objectif, relativement au monde de la recherche qui est d\u00e9crit, je reste toujours un peu sur ma faim par rapport \u00e0 ce que je vis au quotidien depuis pr\u00e8s de 30 ans. Toute l'industrie au sens large (industrie et tertiaire) lutte depuis les d\u00e9buts de l'IT pour d\u00e9finir des s\u00e9mantiques communes, ou autrement dit des fa\u00e7ons communes de repr\u00e9senter les choses (et quand ce n'est pas possibles des fa\u00e7ons communes d'\u00e9changer les donn\u00e9es). Ainsi, beaucoup de m\u00e9tiers ont \u00e9t\u00e9 \"standardis\u00e9s\" dans leur mod\u00e9lisation, que ce soit au travers de groupes de travail internationaux, ou au travers de progiciels ayant structur\u00e9 la repr\u00e9sentation s\u00e9mantique, ou au travers des r\u00e9gulateurs. Avec le temps, les mod\u00e8les s\u00e9mantiques s'enrichissent. C'est ce que je tente d'expliquer dans https://orey.github.io/papers/articles/data-interop/ . Or, avoir des m\u00e9ta-mod\u00e8les, m\u00eame imparfaits, est immens\u00e9ment plus pertinent que de faire confiance \u00e0 du ML bourrin, s'appuyant qui plus est sur des donn\u00e9es venant d'on ne sait o\u00f9. On peut g\u00e9n\u00e9rer des r\u00e8gles depuis un m\u00e9ta-mod\u00e8le s\u00e9mantique et donc raisonner sur ce m\u00e9ta-mod\u00e8le, et donc raisonner sur les donn\u00e9es qui sont instanci\u00e9es depuis ce m\u00e9ta-mod\u00e8le. Je pense qu'avec un ensemble de r\u00e8gles logiques basiques, on peut arriver \u00e0 des r\u00e9sultats tr\u00e8s int\u00e9ressants. Mais consid\u00e9rer les donn\u00e9es dans l'absolu et balancer un moteur de ML dessus, c'est vraiment le degr\u00e9 z\u00e9ro de la pens\u00e9e s\u00e9mantique :) Note que pas mal de gens sont s\u00e9duits par l'approche dans les entreprises, parce que cela \u00e9vite de r\u00e9fl\u00e9chir. Sans rien comprendre aux donn\u00e9es et aux r\u00e8gles de gestion qui leur sont associ\u00e9es, ils font du ML en esp\u00e9rant \"trouver des choses\"... Evidemment, \u00e7a ne donne rien. Je ne sais pas si ce monsieur est repr\u00e9sentatif, mais la recherche semble vouloir \u00e9tudier les mod\u00e8les de raisonnements \"dans l'absolu\" (ce qui ouvre sur des complexit\u00e9s folles selon les op\u00e9rateurs concern\u00e9s) ou s'appuyer sur du ML qui n'est rien d'autre qu'un algo de construction de fonction d'interpolation, mais qui est stupide s\u00e9mantiquement. C'est un peu les deux extr\u00eames. Et, comme souvent dans ce domaine, quand un exemple concret est pris, les gens regardent des sites web du m\u00eame domaine et d\u00e9terminent des r\u00e8gles a posteriori alors que le m\u00e9tier consid\u00e9r\u00e9 est hyper r\u00e9glement\u00e9 par un m\u00e9ta mod\u00e8le s\u00e9mantique que le r\u00e9gulateur ou l'industrie consid\u00e9r\u00e9e a mis des dizaines d'ann\u00e9es \u00e0 construire. Heureusement que l'on arrive \u00e0 d\u00e9terminer des r\u00e8gles, qui sont pour moi la cons\u00e9quence du mod\u00e8le s\u00e9mantique sous-jacent, et que ces r\u00e8gles fonctionnent ! Car le m\u00e9tier, lui, fonctionne. Je trouve toujours cette fa\u00e7on de faire un peu \u00e9trange. Ce qui serait utile \u00e0 l'industrie, ce serait de partir des mod\u00e8les s\u00e9mantiques et d'introduire des r\u00e8gles de raisonnement ou de transformation sur ces mod\u00e8les. On aurait alors de vraies IA qui travaillent sur des m\u00e9ta-mod\u00e8les s\u00e9mantiques leur permettant de comprendre les donn\u00e9es s\u00e9mantiquement structur\u00e9es. En fait, je trouve que le hype du ML est \u00e0 l'image de notre monde. Sans que les gens ne s'en rendent compte, ils ont r\u00e9duit leurs ambitions qui sont devenues minuscules, ou sont invers\u00e9es par rapport \u00e0 ce qu'il faudrait faire (il y a quelque chose de diabolique l\u00e0-dedans ;) :On ne cherche plus \u00e0 comprendre, on cherche des r\u00e9sultats, sans savoir ce qu'on cherche. On cherche des r\u00e8gles sur des donn\u00e9es qui sont la fin d'un processus, alors que ces donn\u00e9es ont \u00e9t\u00e9 cr\u00e9\u00e9es selon un processus s\u00e9mantiquement riche avec des r\u00e8gles connues en amont !!! C'est comme tenter de comprendre fonctionne les apps Android en mesurant les courants de tous les fils internes du t\u00e9l\u00e9phone.En fait, on ne travaille pas, on ne fait pas d'efforts, on fait des trucs qui n'ont aucun sens, tellement on a peur de r\u00e9fl\u00e9chir... ou d'apprendre. On ne veut plus faire de la science. La science, c'est un mod\u00e8le qui repr\u00e9sente la r\u00e9alit\u00e9 et qui nous permet de comprendre les lois de la nature. La logique est dans la science, mais le ML est dans la pens\u00e9e magique. On divinise un outil \u00e0 deux balles, le \"r\u00e9seau de neurones\", qui est un algo de construction d'une fonction d'interpolation que l'on utilise en mode extrapolation. Et on s'\u00e9tonne que la banane disparaisse en mettant un sticker ! Cela montre juste que l'on ne ma\u00eetrise rien de la fonction multidimensionnelle g\u00e9n\u00e9r\u00e9e. Ce serait rigolo si cela ne traduisait pas une profonde incompr\u00e9hension des math\u00e9matiques les plus basiques. Philosophiquement, le ML est un vrai aveu d'\u00e9chec \u00e0 donner du sens au monde, en prenant les choses par la fin, par la data, par le produit fini, par l'infrastructure informatique. C'est ne pas comprendre la cha\u00eene des causes et des cons\u00e9quences : c'est commencer par la cons\u00e9quence et la consid\u00e9rer comme une cause. Pas \u00e9tonnant que Google mette aux nues cette technique. C'est \u00e0 leur image : bourrin et tr\u00e8s b\u00eate :) L\u00e0 o\u00f9 le monsieur me fait un peu peur (mais c'est sans doute juste une posture pour obtenir des cr\u00e9dits de recherche), c'est qu'il donne du cr\u00e9dit au ML. Je ne comprends pas comment un logicien peut donner du cr\u00e9dit \u00e0 cela. Pour moi, l'histoire de l'informatique recherche un autre objectif que toute cette tendance : elle cherche \u00e0 faire marcher des choses, \u00e0 informatiser des m\u00e9tiers et donc \u00e0 les conna\u00eetre, les mod\u00e9liser, les comprendre intimement que ce soit dans la structure de leur connaissance ou dans leurs r\u00e8gles de gestion (bas\u00e9es sur des r\u00e8gles logiques).","title":"Rough notes"},{"location":"research/basic-graph-transformations/","text":"Basic graph transformations This page is written in the context of multi-labeled nodes as they are implemented in standard graph databases. Node structure Basic Python structure features: Id with hash and eq function Type Domain Attributes are in a dictionary (real implementation of the multi-labelled graph, knowing that it is also the way Neo represents it) Questions: Link to a grammar or not Position semantics (in the case for instance we unserialize elements in a CSV file in which the line number is important relatively to the previous or the next one) Relationship Attributes dictionary also To be thought through: a graph based on arrays of neighborhood may be sufficient Graph Node dictionary - Neighbors dictionary - Dictionary of attribute links Unitary GTs GT have two modes: Destructive mode Copy mode Gt1: filter Remove void attributes/columns Gt2: a column is a type Change the type and remove the attribute Gt3: foreign key column Remove attribute and create a link Gt4: references hidden in a label Remove attribute and create a link Gt5 : references hidden towards a concept that does not exist Gt6 : link in the past PREVIOUS GT Interface graph, rootnode, options = GT(graph, rootnode, options) rootnode is in the graph options = {key : value, etc.} For instance: Gt1 (graph, None, { \"pattern\" : \"01001110\"}) Gt1 (graph, root, { \"depth\" : 3, \"pattern\" : \"0100110\"}) GT can forward parameters such as: CloneGt(None, root, params) Gt6(None, root, params) - None, root, params clonegt(Gt6) - None, root, params By forwarding the rest, we can compose GTs. Edited: better interface graph2, rootnode2 = GT1(graph1, rootnode1, options1) graph3, rootnode3 = GT2(*GT1(graph1, rootnode1, options1), option2) This enables to set the options at each stage. For sure, depending on the case, the graph or the rootnode may be None . Flexible interface: Graph, None = Enables to address all nodes issues Graph, root = Considers the graph from the root, meaning, we can request any node or relationship in the graph in complement than accessing the graph through the root node None, root = Or we consider a GT only touching the node, or the GT will access the graph through the root node only. Questions Deal with subgraphs?","title":"Basic graph transformations"},{"location":"research/basic-graph-transformations/#basic-graph-transformations","text":"This page is written in the context of multi-labeled nodes as they are implemented in standard graph databases.","title":"Basic graph transformations"},{"location":"research/basic-graph-transformations/#node-structure","text":"Basic Python structure features: Id with hash and eq function Type Domain Attributes are in a dictionary (real implementation of the multi-labelled graph, knowing that it is also the way Neo represents it) Questions: Link to a grammar or not Position semantics (in the case for instance we unserialize elements in a CSV file in which the line number is important relatively to the previous or the next one)","title":"Node structure"},{"location":"research/basic-graph-transformations/#relationship","text":"Attributes dictionary also To be thought through: a graph based on arrays of neighborhood may be sufficient","title":"Relationship"},{"location":"research/basic-graph-transformations/#graph","text":"Node dictionary - Neighbors dictionary - Dictionary of attribute links","title":"Graph"},{"location":"research/basic-graph-transformations/#unitary-gts","text":"GT have two modes: Destructive mode Copy mode Gt1: filter Remove void attributes/columns Gt2: a column is a type Change the type and remove the attribute Gt3: foreign key column Remove attribute and create a link Gt4: references hidden in a label Remove attribute and create a link Gt5 : references hidden towards a concept that does not exist Gt6 : link in the past PREVIOUS","title":"Unitary GTs"},{"location":"research/basic-graph-transformations/#gt-interface","text":"graph, rootnode, options = GT(graph, rootnode, options) rootnode is in the graph options = {key : value, etc.} For instance: Gt1 (graph, None, { \"pattern\" : \"01001110\"}) Gt1 (graph, root, { \"depth\" : 3, \"pattern\" : \"0100110\"}) GT can forward parameters such as: CloneGt(None, root, params) Gt6(None, root, params) - None, root, params clonegt(Gt6) - None, root, params By forwarding the rest, we can compose GTs. Edited: better interface graph2, rootnode2 = GT1(graph1, rootnode1, options1) graph3, rootnode3 = GT2(*GT1(graph1, rootnode1, options1), option2) This enables to set the options at each stage. For sure, depending on the case, the graph or the rootnode may be None . Flexible interface: Graph, None = Enables to address all nodes issues Graph, root = Considers the graph from the root, meaning, we can request any node or relationship in the graph in complement than accessing the graph through the root node None, root = Or we consider a GT only touching the node, or the GT will access the graph through the root node only.","title":"GT Interface"},{"location":"research/basic-graph-transformations/#questions","text":"Deal with subgraphs?","title":"Questions"},{"location":"research/basic-semantic-graph-transformations/","text":"Basic semantic graph transformations This page is following Basic graph transformations but with a semantic graph perspective. Rich/poor level of information A single triple is a \"poor piece of information\", but other triples with the same subjects can build a rich set of information. Poor piece of information: s p o . Rich piece of information: s p o ; a S . o a O . Time management Rich piece of information: adding time information: s p_23DEC2018 o ; a S . o a O . p_23DEC2018 a P . P a Time_Predicate . This is practical because s P o . can be deduced easily even if s p_23DEC2018 o . is more precise. The statement P a Time-Predicate . indicates that P is a time-enabled predicate. Version management We can have a variation of what we saw with version tagging. s p_V2 o ; a S . o a O . p_V2 a P . P a Version_Predicate . Managing life-cycle Case of subject modification and history keeping. Step 1. We have: s1 p o . Step 2: s1 becomes s2. We have: s1 p o . s2 previous s1 . s2 p o . // Rewiring This is ambiguous because the 3rd statement was made after the first. Let's use a time predicate. s1 p_12DEC2018 o . p_12DEC2018 a P . P a Time_Predicate . s2 previous s1 . s2 p_23DEC2018 o . // Rewiring p_23DEC2018 a P . We can look at the graph at various moments. Indeed, we still have: s1 P o . s2 previous s1 . s2 P o . // Rewiring But we also encoded a more precise information. We could think about removing s1 p o . after s2 is created but as the semantic web is a cumulative system, this does not seem very interesting. Shortcuts Let's consider the pattern: q = p(1) o p(2) o ... o p(n) with p(i) a set of predicates. s q a . if x(n) p(n) a . and x(n-1) p(n-1) x(n) . and .. and s p(1) x(2) . q id just a new \"predicate name\". This can be very useful to present the same reality in another perspective/point of view. Filters If we have: s p o ; q a . We can define a subgraph by \"removing\" the q predicate: graph(s , depth=1) \\ {q} = s p o . Classical inferences The use of classical inferences is very important also. Temporal inferences If we have: s p(t1) a . s p(t2) b . p(t1) a P . p(t2) a P . P a Time_Predicate . and a and b were not existing before the predicates were created, we could deduce: a before b . Not sure it is useful. Simpler like that: a2 previous a1 . a3 previous a2 . = a3 previous a1 . Some predicates can have special transitivity features. Special predicates Transitive predicates: a r b . and b r c . = a r c . Commutative predicates: a r b . = b r a . Semantic inversion (very used in Owl): p is the inverse of r , so if a r b . = b p a . To do Re-analyze the list notion in the RDF standard Re-analyze the Gruf temporal display Formalization of grammar in table parsing = grammar being a semantic graph List graph transformations in Sparql-like","title":"Basic semantic graph transformations"},{"location":"research/basic-semantic-graph-transformations/#basic-semantic-graph-transformations","text":"This page is following Basic graph transformations but with a semantic graph perspective.","title":"Basic semantic graph transformations"},{"location":"research/basic-semantic-graph-transformations/#richpoor-level-of-information","text":"A single triple is a \"poor piece of information\", but other triples with the same subjects can build a rich set of information. Poor piece of information: s p o . Rich piece of information: s p o ; a S . o a O .","title":"Rich/poor level of information"},{"location":"research/basic-semantic-graph-transformations/#time-management","text":"Rich piece of information: adding time information: s p_23DEC2018 o ; a S . o a O . p_23DEC2018 a P . P a Time_Predicate . This is practical because s P o . can be deduced easily even if s p_23DEC2018 o . is more precise. The statement P a Time-Predicate . indicates that P is a time-enabled predicate.","title":"Time management"},{"location":"research/basic-semantic-graph-transformations/#version-management","text":"We can have a variation of what we saw with version tagging. s p_V2 o ; a S . o a O . p_V2 a P . P a Version_Predicate .","title":"Version management"},{"location":"research/basic-semantic-graph-transformations/#managing-life-cycle","text":"Case of subject modification and history keeping. Step 1. We have: s1 p o . Step 2: s1 becomes s2. We have: s1 p o . s2 previous s1 . s2 p o . // Rewiring This is ambiguous because the 3rd statement was made after the first. Let's use a time predicate. s1 p_12DEC2018 o . p_12DEC2018 a P . P a Time_Predicate . s2 previous s1 . s2 p_23DEC2018 o . // Rewiring p_23DEC2018 a P . We can look at the graph at various moments. Indeed, we still have: s1 P o . s2 previous s1 . s2 P o . // Rewiring But we also encoded a more precise information. We could think about removing s1 p o . after s2 is created but as the semantic web is a cumulative system, this does not seem very interesting.","title":"Managing life-cycle"},{"location":"research/basic-semantic-graph-transformations/#shortcuts","text":"Let's consider the pattern: q = p(1) o p(2) o ... o p(n) with p(i) a set of predicates. s q a . if x(n) p(n) a . and x(n-1) p(n-1) x(n) . and .. and s p(1) x(2) . q id just a new \"predicate name\". This can be very useful to present the same reality in another perspective/point of view.","title":"Shortcuts"},{"location":"research/basic-semantic-graph-transformations/#filters","text":"If we have: s p o ; q a . We can define a subgraph by \"removing\" the q predicate: graph(s , depth=1) \\ {q} = s p o .","title":"Filters"},{"location":"research/basic-semantic-graph-transformations/#classical-inferences","text":"The use of classical inferences is very important also.","title":"Classical inferences"},{"location":"research/basic-semantic-graph-transformations/#temporal-inferences","text":"If we have: s p(t1) a . s p(t2) b . p(t1) a P . p(t2) a P . P a Time_Predicate . and a and b were not existing before the predicates were created, we could deduce: a before b . Not sure it is useful. Simpler like that: a2 previous a1 . a3 previous a2 . = a3 previous a1 . Some predicates can have special transitivity features.","title":"Temporal inferences"},{"location":"research/basic-semantic-graph-transformations/#special-predicates","text":"Transitive predicates: a r b . and b r c . = a r c . Commutative predicates: a r b . = b r a . Semantic inversion (very used in Owl): p is the inverse of r , so if a r b . = b p a .","title":"Special predicates"},{"location":"research/basic-semantic-graph-transformations/#to-do","text":"Re-analyze the list notion in the RDF standard Re-analyze the Gruf temporal display Formalization of grammar in table parsing = grammar being a semantic graph List graph transformations in Sparql-like","title":"To do"},{"location":"research/data-mig/","text":"Semantic data migration project First research elements (2018-2020) Why? Arguments for using the semantic graph technology to complex data conversion: It is relatively easy to turn whatever table into sets of triples. The semantic of each column must be named and reused as a \"semantic dictionary\" between the multiples sources. This step is much easier to accomplish than in standard data lakes where the complete big definitions must be set in one single movement. The design actions, that were at the heart of the problems in GraphApps , are less important and can be limited to a correct understanding of the data. The semantic databases (For instance AllegroGraph or Apache Jena) are working in an incremental way: if a triple already exists, the attempt to import it again will do nothing, which \"by design\" eliminates redundant information. Sparql enables easy graph transformations, first of all to visualize data (Gruff on AllegroGraph is a good triplestore visualization tool), and then to transform them. Timed of life-cycled data Data versions (being time-based versions or life-cycle versions) can be managed with a link timestamp or a link version stamp. The timestamp relation will have to be a rdfs:SubClassOf the theoretical link type. Basic semantic graph transformation (in work) In the context of graph transformations , the page Basic semantic graph transformations aims at defining a set of basic graph transformations. Industry data The page Graphs and semantic data in industry talks about the opportunity of using RDF approaches to convert efficiently industrial data. Presentation to CNRS Madics conference in Rennes in 2019 The aerospace industry is managing aircraft products with PLM (product lifecycle management) software. Most of the time, the PLM software used are very customized and very old. Moreover in the day to day life, people are using a mix of various PLM systems and Excel spreadsheets. In order to change the business practices for the 3 core businesses of the aerospace companies (design office, manufacturing, support and services), the PLM backbones must evolve on two dimensions: first they must gather all data spread across various tools per business, and second they must integrate the three businesses together (digital continuity) in order to perform efficient concurrent engineering and gain on lead times. New end to end PLM backbones are available on the market but the migration to those products is slowed down by the lack of method and tools to properly migrate the data. The main constraint of PLM data migration is to be able to keep all the semantic links of past data into the new system, because the aircraft certification is tied to those links and data. At the intersection of all those constraints, the semantic web technologies (RDF/RDFS, Triplestores, SPARQL-based rules, etc.) can help Airbus to convert its core industrial data to migrate them into new generations of PLM systems. The presentation exposes a concrete industrial case and explain the status of the works and the tool chain involved in those works. Presentation A systematic migration process (2020-) The 6 steps of transformation The process of data migration is linked to the transformation of ontologies. The migration process that we target is the following: Definition of the hidden source ontology : In the data as they are structured today, a source ontology is hidden. The objective of this step is to exhibit the source ontology, and to create a triplestore with the existing data based on the source ontology. The source ontology must have its own namespace. Convert the source data in the source ontology referential. Definition of the target ontology : We want to convert data in a target format which is based on a target ontology. This ontology must be formalized. It must have its own namespace. Definition of the graph transformation required to transform the source ontology in the target ontology : This graph transformation should be defined with Basic semantic graph transformations , and can also use certain abstract domain ontologies. Derive from the ontology graph representation, a data graph transformation that enables to transform the data from the source ontology in the target ontology. Convert source data into target data . Data representation is relative This process is based on the facts that: The same reality can be modeled differently, There is a path between the various modeling spaces when it comes to the same concept representations. See also: The real nature of data . (Last update: August 2021)","title":"Semantic data migration project page"},{"location":"research/data-mig/#semantic-data-migration-project","text":"","title":"Semantic data migration project"},{"location":"research/data-mig/#first-research-elements-2018-2020","text":"","title":"First research elements (2018-2020)"},{"location":"research/data-mig/#why","text":"Arguments for using the semantic graph technology to complex data conversion: It is relatively easy to turn whatever table into sets of triples. The semantic of each column must be named and reused as a \"semantic dictionary\" between the multiples sources. This step is much easier to accomplish than in standard data lakes where the complete big definitions must be set in one single movement. The design actions, that were at the heart of the problems in GraphApps , are less important and can be limited to a correct understanding of the data. The semantic databases (For instance AllegroGraph or Apache Jena) are working in an incremental way: if a triple already exists, the attempt to import it again will do nothing, which \"by design\" eliminates redundant information. Sparql enables easy graph transformations, first of all to visualize data (Gruff on AllegroGraph is a good triplestore visualization tool), and then to transform them.","title":"Why?"},{"location":"research/data-mig/#timed-of-life-cycled-data","text":"Data versions (being time-based versions or life-cycle versions) can be managed with a link timestamp or a link version stamp. The timestamp relation will have to be a rdfs:SubClassOf the theoretical link type.","title":"Timed of life-cycled data"},{"location":"research/data-mig/#basic-semantic-graph-transformation-in-work","text":"In the context of graph transformations , the page Basic semantic graph transformations aims at defining a set of basic graph transformations.","title":"Basic semantic graph transformation (in work)"},{"location":"research/data-mig/#industry-data","text":"The page Graphs and semantic data in industry talks about the opportunity of using RDF approaches to convert efficiently industrial data.","title":"Industry data"},{"location":"research/data-mig/#presentation-to-cnrs-madics-conference-in-rennes-in-2019","text":"The aerospace industry is managing aircraft products with PLM (product lifecycle management) software. Most of the time, the PLM software used are very customized and very old. Moreover in the day to day life, people are using a mix of various PLM systems and Excel spreadsheets. In order to change the business practices for the 3 core businesses of the aerospace companies (design office, manufacturing, support and services), the PLM backbones must evolve on two dimensions: first they must gather all data spread across various tools per business, and second they must integrate the three businesses together (digital continuity) in order to perform efficient concurrent engineering and gain on lead times. New end to end PLM backbones are available on the market but the migration to those products is slowed down by the lack of method and tools to properly migrate the data. The main constraint of PLM data migration is to be able to keep all the semantic links of past data into the new system, because the aircraft certification is tied to those links and data. At the intersection of all those constraints, the semantic web technologies (RDF/RDFS, Triplestores, SPARQL-based rules, etc.) can help Airbus to convert its core industrial data to migrate them into new generations of PLM systems. The presentation exposes a concrete industrial case and explain the status of the works and the tool chain involved in those works. Presentation","title":"Presentation to CNRS Madics conference in Rennes in 2019"},{"location":"research/data-mig/#a-systematic-migration-process-2020-","text":"","title":"A systematic migration process (2020-)"},{"location":"research/data-mig/#the-6-steps-of-transformation","text":"The process of data migration is linked to the transformation of ontologies. The migration process that we target is the following: Definition of the hidden source ontology : In the data as they are structured today, a source ontology is hidden. The objective of this step is to exhibit the source ontology, and to create a triplestore with the existing data based on the source ontology. The source ontology must have its own namespace. Convert the source data in the source ontology referential. Definition of the target ontology : We want to convert data in a target format which is based on a target ontology. This ontology must be formalized. It must have its own namespace. Definition of the graph transformation required to transform the source ontology in the target ontology : This graph transformation should be defined with Basic semantic graph transformations , and can also use certain abstract domain ontologies. Derive from the ontology graph representation, a data graph transformation that enables to transform the data from the source ontology in the target ontology. Convert source data into target data .","title":"The 6 steps of transformation"},{"location":"research/data-mig/#data-representation-is-relative","text":"This process is based on the facts that: The same reality can be modeled differently, There is a path between the various modeling spaces when it comes to the same concept representations. See also: The real nature of data . (Last update: August 2021)","title":"Data representation is relative"},{"location":"research/grammar-graph-transformation/","text":"Grammar graph transformation Graphs in general, and semantic graphs in particular, can be used to migrate very efficiently data from one format to another. Grammar By grammar, we mean several possible things: A description of the schema of a RDBMS, especially of tables, specifying the type of each table and the types of each columns. In that case, the grammar corresponds to the information contained in the database schema. The grammar that we can have to generate triples from a structured table format source. See the page on industry data for a sample. In that page, the description of the types of all data in a particular semantic space can be called the \"grammar\" of the semantic space. Transforming grammars with reusable basic graph transformations Grammars can be expressed in graphs. Let's take the very simple example of the following RDMS schema (see Figure 1). Figure 1: Simple grammar for a table This grammar can be expressed in basic RDF. T1 a Table . T1 attribute C1 . C1 SubClassOf Type1 . ... Tj attribute Cj . Cj SubClassOf Typej . ... Tn attribute Cn . Cn SubClassOf Typen . Basic RDBMS schema transformation in a multi-attributes graph database The following transformations are basic RDBMS schema transformations. Note that graph transformations are not commutative and their order is important. Transforming foreign key into a relationship Let's suppose Ck is a foreign key of Table1 to Table2 , referencing column Kl . # Before T1 attribute Ck . Ck SubClassOf T2 . T2 attribute Kl . Kl SubClassOf Typel . # Grammar graph transformation 1 Transform_foreign_key { # 1. Link the two table colums with a particular semantic predicate # 2. Remove Ck attribute from T1 } # After T1 foreign_key T2 . T2 attribute Kl . Kl SubClassOf Typel . The predicate foreign_key will enable real data transformation later. Simple class split and simple class merge Those graph transformations are exploiting the 1-1 relationship. In case of split, semantically the table T aggregates 2 concepts that are implicitly linked together. It is possible to reorder the attributes in that case. Let's consider a table T with 4 attributes C1 , C2 , C3 and C4 . The graph transformation will create a new resource T' and use a new semantic predicate (here linked_to ) so that: # Before T attribute C1 . T attribute C2 . T attribute C3 . T attribute C4 . # Grammar graph transformation 2 Split_class { # 1. Create the new resource type for T' (here a Table but could be some other semantic class) # 2. Link to source T # 3. Get attributes of T # 4. Remove attributes from T } # After T attribute C1 . T attribute C2 . T linked_to T' . T' a Table . T' attribute C3 . T' attribute C4 . Symmetrically, we have the merge. # Before T attribute C1 . T attribute C2 . T' attribute C3 . T' attribute C4 . # Grammar graph transformation 3 Merge_classes { # 1. Get attributes of T' to wire them in T # 2. Remove attributes from T' # 3. Remove T' } # After T attribute C1 . T attribute C2 . T attribute C3 . T attribute C4 . For sure, if T' is removed, we have to ensure before that, in the semantic space, the type is not required elsewhere. Complex class split One problem of table design is that we have sometimes different types hiding in the same table. Those cases are frequent in RDBMS and can be due to: A compromise between the object oriented approach and the RDMS (see Figure 2); Ease of code reuse for some different concepts. Note: Those practices generate a lot of technical debt and may paralyze the evolution of a software. Figure 2: Many types hiding in a table The class split will exhibit new concepts Type1 , Type2 and Type3 , all those concepts not having anymore the attribute of type indication ( C2 in Figure 2). The transformation will have to manage the various values in C2 and, depending on those values, choose what attributes to move from one resource to another. Non destructiveness and changing the semantic domain If we don't want to be destructive on the original grammar, we might want to contextualize the grammar to a semantic space describing the source world (here the RDBMS world). This could be done by using a specific URI prefix, like rdbms . The consequence is that we need basic transformations to transform rdbms:Table artifact into graph:Concept artifact, and rdms:attribute into graph:attribute . Those transformations are like \"translations\". Figure 3: Various levels of graph transformations In that case, we will have global (inter-semantic domain) graph transformations for translations, and intra grammar semantic space (graph) for the various transformations we just saw before. Using the time approach described in the basic semantic graph transformation page , we could imagine chaining the graph transformations for the global transformation of the RDBMS model into a richer, \"re-semantized\", version of the model. From that chain of graph transformations, we can generate code to transform the real data from the grammar transformation process. Complementary readings: Basic semantic graph transformations Basic graph transformations","title":"Grammar of graph transformation"},{"location":"research/grammar-graph-transformation/#grammar-graph-transformation","text":"Graphs in general, and semantic graphs in particular, can be used to migrate very efficiently data from one format to another.","title":"Grammar graph transformation"},{"location":"research/grammar-graph-transformation/#grammar","text":"By grammar, we mean several possible things: A description of the schema of a RDBMS, especially of tables, specifying the type of each table and the types of each columns. In that case, the grammar corresponds to the information contained in the database schema. The grammar that we can have to generate triples from a structured table format source. See the page on industry data for a sample. In that page, the description of the types of all data in a particular semantic space can be called the \"grammar\" of the semantic space.","title":"Grammar"},{"location":"research/grammar-graph-transformation/#transforming-grammars-with-reusable-basic-graph-transformations","text":"Grammars can be expressed in graphs. Let's take the very simple example of the following RDMS schema (see Figure 1). Figure 1: Simple grammar for a table This grammar can be expressed in basic RDF. T1 a Table . T1 attribute C1 . C1 SubClassOf Type1 . ... Tj attribute Cj . Cj SubClassOf Typej . ... Tn attribute Cn . Cn SubClassOf Typen .","title":"Transforming grammars with reusable basic graph transformations"},{"location":"research/grammar-graph-transformation/#basic-rdbms-schema-transformation-in-a-multi-attributes-graph-database","text":"The following transformations are basic RDBMS schema transformations. Note that graph transformations are not commutative and their order is important.","title":"Basic RDBMS schema transformation in a multi-attributes graph database"},{"location":"research/grammar-graph-transformation/#transforming-foreign-key-into-a-relationship","text":"Let's suppose Ck is a foreign key of Table1 to Table2 , referencing column Kl . # Before T1 attribute Ck . Ck SubClassOf T2 . T2 attribute Kl . Kl SubClassOf Typel . # Grammar graph transformation 1 Transform_foreign_key { # 1. Link the two table colums with a particular semantic predicate # 2. Remove Ck attribute from T1 } # After T1 foreign_key T2 . T2 attribute Kl . Kl SubClassOf Typel . The predicate foreign_key will enable real data transformation later.","title":"Transforming foreign key into a relationship"},{"location":"research/grammar-graph-transformation/#simple-class-split-and-simple-class-merge","text":"Those graph transformations are exploiting the 1-1 relationship. In case of split, semantically the table T aggregates 2 concepts that are implicitly linked together. It is possible to reorder the attributes in that case. Let's consider a table T with 4 attributes C1 , C2 , C3 and C4 . The graph transformation will create a new resource T' and use a new semantic predicate (here linked_to ) so that: # Before T attribute C1 . T attribute C2 . T attribute C3 . T attribute C4 . # Grammar graph transformation 2 Split_class { # 1. Create the new resource type for T' (here a Table but could be some other semantic class) # 2. Link to source T # 3. Get attributes of T # 4. Remove attributes from T } # After T attribute C1 . T attribute C2 . T linked_to T' . T' a Table . T' attribute C3 . T' attribute C4 . Symmetrically, we have the merge. # Before T attribute C1 . T attribute C2 . T' attribute C3 . T' attribute C4 . # Grammar graph transformation 3 Merge_classes { # 1. Get attributes of T' to wire them in T # 2. Remove attributes from T' # 3. Remove T' } # After T attribute C1 . T attribute C2 . T attribute C3 . T attribute C4 . For sure, if T' is removed, we have to ensure before that, in the semantic space, the type is not required elsewhere.","title":"Simple class split and simple class merge"},{"location":"research/grammar-graph-transformation/#complex-class-split","text":"One problem of table design is that we have sometimes different types hiding in the same table. Those cases are frequent in RDBMS and can be due to: A compromise between the object oriented approach and the RDMS (see Figure 2); Ease of code reuse for some different concepts. Note: Those practices generate a lot of technical debt and may paralyze the evolution of a software. Figure 2: Many types hiding in a table The class split will exhibit new concepts Type1 , Type2 and Type3 , all those concepts not having anymore the attribute of type indication ( C2 in Figure 2). The transformation will have to manage the various values in C2 and, depending on those values, choose what attributes to move from one resource to another.","title":"Complex class split"},{"location":"research/grammar-graph-transformation/#non-destructiveness-and-changing-the-semantic-domain","text":"If we don't want to be destructive on the original grammar, we might want to contextualize the grammar to a semantic space describing the source world (here the RDBMS world). This could be done by using a specific URI prefix, like rdbms . The consequence is that we need basic transformations to transform rdbms:Table artifact into graph:Concept artifact, and rdms:attribute into graph:attribute . Those transformations are like \"translations\". Figure 3: Various levels of graph transformations In that case, we will have global (inter-semantic domain) graph transformations for translations, and intra grammar semantic space (graph) for the various transformations we just saw before. Using the time approach described in the basic semantic graph transformation page , we could imagine chaining the graph transformations for the global transformation of the RDBMS model into a richer, \"re-semantized\", version of the model. From that chain of graph transformations, we can generate code to transform the real data from the grammar transformation process. Complementary readings: Basic semantic graph transformations Basic graph transformations","title":"Non destructiveness and changing the semantic domain"},{"location":"research/graph-interop/","text":"A general interoperability model using graph transformations This article proposes a general model of interoperability between systems using the concept of graph transformations . Photo by kliverap from FreeImages Introduction In the real nature of data , we discussed many important dimensions of data: Data can be considered as the \"intersection\" between the semantic domain and the point of view defined by the actor and the use case; Data are organized in a relative way, meaning that there are several ways to represent the same reality; Data is an \"output product\", result of a process and business rules that were applied before, and considering it as a \"entry product\" may lead to ridiculous anti-scientific approaches.","title":"A general interoperability model using graph transformations"},{"location":"research/graph-interop/#a-general-interoperability-model-using-graph-transformations","text":"This article proposes a general model of interoperability between systems using the concept of graph transformations . Photo by kliverap from FreeImages","title":"A general interoperability model using graph transformations"},{"location":"research/graph-interop/#introduction","text":"In the real nature of data , we discussed many important dimensions of data: Data can be considered as the \"intersection\" between the semantic domain and the point of view defined by the actor and the use case; Data are organized in a relative way, meaning that there are several ways to represent the same reality; Data is an \"output product\", result of a process and business rules that were applied before, and considering it as a \"entry product\" may lead to ridiculous anti-scientific approaches.","title":"Introduction"},{"location":"research/graph-oriented-pl/","text":"Graph-Oriented programming language This document is gathering some reflections about what the syntax of a programming language could be. Constructs (graph local lg local-graph) (graph remote rg remote-graph url user pwd)","title":"Graph-oriented programming language"},{"location":"research/graph-oriented-pl/#graph-oriented-programming-language","text":"This document is gathering some reflections about what the syntax of a programming language could be.","title":"Graph-Oriented programming language"},{"location":"research/graph-oriented-pl/#constructs","text":"(graph local lg local-graph) (graph remote rg remote-graph url user pwd)","title":"Constructs"},{"location":"research/graph-transfo/","text":"Graph transformations Graph transformation are mathematical objects that transform graphs. Definition of an labeled directed graph A graph is a mathematical object that contains nodes attached by edges. It is noted: G = {N, E} where N is a set of nodes, and E a set of pair of nodes n N An labeled graph is a graph where nodes and edges can have labels. Because we are in IT, we will consider that each node and edge has at least one label that is its unique identifier named id . In all the pages of this site, we will often consider that the graph label is an attribute with a value, what could be represented as follows, for a node n N and for an edge e E: n{id:value, attribute1:value1, ..., attributen,valuen} e{id:value, sourcenode, destination node, attribute1:value1, ..., attributen,valuen} Note that the edges are directed in the graphs we are considering which means that they have a source and a destination. That enables to exhibit the notions of: Incoming relationships to a node: All edges pointing to that node Outgoing relationships from a node: All edges have the node as their source Graphs can be included in each other. At the level of attributed directed graphs, the inclusion can be defined as follows. Let A = {N 1 , E 1 } and B = {N 2 , E 2 } two graphs, we will note: B A if and only if n N 2 , n N 1 , and e E 2 , e E 1 Note: That means that all labels are the same. Definition of a semantic graph A semantic graph is defined with RDF triples manipulating and linking resources (URI). We will often think about resources as bearing 3 kinds of information: Their domain Their type Their identifier For instance, the following URI http://example.com/2021/07/product#item12345 can be split in: Domain: http://example.com/2021/07/ Type: product Id: item12345 Some cases are more complex but we will address them as they come. In a semantic graph, URIs can be used as nodes and edges, which make the semantic graph not a graph actually, and makes it complicated to manipulate without some specific hypothesis on an IT standpoint. Definition of a graph transformation Let g be a graph transformation. g is define as follows: g: D R, D and R being sets of graphs, potentially reduced to a single graph For G D: g(G) = G' (1) If G G', then g is said to be non destructive . If G G', then g is said to be destructive . Note: If some labels were modified, then g is qualified as destructive even if the node or edge is preserved. Non destructive graph transformations preserve the full input graph and all the labels attached to its nodes and edges. We can call this process graph enrichment . It is used in the process of data migration . That leads us to highlight the two kinds of graph transformations: Enrichment Modify , which has two subtypes: Modify destructive Modify non destructive This split is quite important in IT where, implicitly, most graph transformation are in the modify mode to keep the same graph in memory. But, for operations that need to track all previous steps, we can use enrichment graph transformations. Naturally, graph transformations can be composed: Let f: D R and g: R P g o f is a graph transformation from D P This defines: The basis of graph transformation reusability; The possible existence of a \"graph transformation base\" of basic graph transformations. See after. Definition of a root node We will define a root node as a node of G that is preserved by g at least in its existence (maybe its labels were modified, all except its id). Using a root note r instead of the graph itself, we can write by extension of (1): g(r) = r with the first r G and the second r G' Note: Qualifying \"root node\" a node of G is relative and can only be done in the context of a graph transformation. This enables to create graph transformations with an homogeneous IT signature that enables cascades of function calls. Indeed, let's suppose r is a root node for g 1 , g 2 and g p , we can define h the composition of all graph transformations g i by: h = g p o g p-1 o ... o g 1 if h(r) = g p (g p-1 (... (g 1 (r)...)) = r But warning, at each step, r is representing a different graph. Limit conditions In the IT context, some graph transformations may be not applicable to a certain graph because some conditions are not met. Instead of being an error, we can enrich a bit the semantics by defining the NOT_APPLICABLE keyword. For f a graph transformation that is not applicable to a graph G, we can note: f: D R G NOT_APPLICABLE Even if it is only another way of expressing that G D, it can be very useful in IT because D is never really defined if f contains business rules. To make the model homogeneous, we can postulate, in order to have the composition of graph transformation work, that: g, graph transformation, g(NOT_APPLICABLE) = NOT_APPLICABLE Basic set of graph transformations See the special pages: Basic graph transformations Basic semantic graph transformations Applicability of graph transformations See: DSL for graph topology check Graph transformation applicability Note: to be declined in Sparql. Grammar for graph transformation In the context of the page on industry data , please have a look at the grammar of graph transformation . (Last update: July 2021)","title":"Graph transformations"},{"location":"research/graph-transfo/#graph-transformations","text":"Graph transformation are mathematical objects that transform graphs.","title":"Graph transformations"},{"location":"research/graph-transfo/#definition-of-an-labeled-directed-graph","text":"A graph is a mathematical object that contains nodes attached by edges. It is noted: G = {N, E} where N is a set of nodes, and E a set of pair of nodes n N An labeled graph is a graph where nodes and edges can have labels. Because we are in IT, we will consider that each node and edge has at least one label that is its unique identifier named id . In all the pages of this site, we will often consider that the graph label is an attribute with a value, what could be represented as follows, for a node n N and for an edge e E: n{id:value, attribute1:value1, ..., attributen,valuen} e{id:value, sourcenode, destination node, attribute1:value1, ..., attributen,valuen} Note that the edges are directed in the graphs we are considering which means that they have a source and a destination. That enables to exhibit the notions of: Incoming relationships to a node: All edges pointing to that node Outgoing relationships from a node: All edges have the node as their source Graphs can be included in each other. At the level of attributed directed graphs, the inclusion can be defined as follows. Let A = {N 1 , E 1 } and B = {N 2 , E 2 } two graphs, we will note: B A if and only if n N 2 , n N 1 , and e E 2 , e E 1 Note: That means that all labels are the same.","title":"Definition of an labeled directed graph"},{"location":"research/graph-transfo/#definition-of-a-semantic-graph","text":"A semantic graph is defined with RDF triples manipulating and linking resources (URI). We will often think about resources as bearing 3 kinds of information: Their domain Their type Their identifier For instance, the following URI http://example.com/2021/07/product#item12345 can be split in: Domain: http://example.com/2021/07/ Type: product Id: item12345 Some cases are more complex but we will address them as they come. In a semantic graph, URIs can be used as nodes and edges, which make the semantic graph not a graph actually, and makes it complicated to manipulate without some specific hypothesis on an IT standpoint.","title":"Definition of a semantic graph"},{"location":"research/graph-transfo/#definition-of-a-graph-transformation","text":"Let g be a graph transformation. g is define as follows: g: D R, D and R being sets of graphs, potentially reduced to a single graph For G D: g(G) = G' (1) If G G', then g is said to be non destructive . If G G', then g is said to be destructive . Note: If some labels were modified, then g is qualified as destructive even if the node or edge is preserved. Non destructive graph transformations preserve the full input graph and all the labels attached to its nodes and edges. We can call this process graph enrichment . It is used in the process of data migration . That leads us to highlight the two kinds of graph transformations: Enrichment Modify , which has two subtypes: Modify destructive Modify non destructive This split is quite important in IT where, implicitly, most graph transformation are in the modify mode to keep the same graph in memory. But, for operations that need to track all previous steps, we can use enrichment graph transformations. Naturally, graph transformations can be composed: Let f: D R and g: R P g o f is a graph transformation from D P This defines: The basis of graph transformation reusability; The possible existence of a \"graph transformation base\" of basic graph transformations. See after.","title":"Definition of a graph transformation"},{"location":"research/graph-transfo/#definition-of-a-root-node","text":"We will define a root node as a node of G that is preserved by g at least in its existence (maybe its labels were modified, all except its id). Using a root note r instead of the graph itself, we can write by extension of (1): g(r) = r with the first r G and the second r G' Note: Qualifying \"root node\" a node of G is relative and can only be done in the context of a graph transformation. This enables to create graph transformations with an homogeneous IT signature that enables cascades of function calls. Indeed, let's suppose r is a root node for g 1 , g 2 and g p , we can define h the composition of all graph transformations g i by: h = g p o g p-1 o ... o g 1 if h(r) = g p (g p-1 (... (g 1 (r)...)) = r But warning, at each step, r is representing a different graph.","title":"Definition of a root node"},{"location":"research/graph-transfo/#limit-conditions","text":"In the IT context, some graph transformations may be not applicable to a certain graph because some conditions are not met. Instead of being an error, we can enrich a bit the semantics by defining the NOT_APPLICABLE keyword. For f a graph transformation that is not applicable to a graph G, we can note: f: D R G NOT_APPLICABLE Even if it is only another way of expressing that G D, it can be very useful in IT because D is never really defined if f contains business rules. To make the model homogeneous, we can postulate, in order to have the composition of graph transformation work, that: g, graph transformation, g(NOT_APPLICABLE) = NOT_APPLICABLE","title":"Limit conditions"},{"location":"research/graph-transfo/#basic-set-of-graph-transformations","text":"See the special pages: Basic graph transformations Basic semantic graph transformations","title":"Basic set of graph transformations"},{"location":"research/graph-transfo/#applicability-of-graph-transformations","text":"See: DSL for graph topology check Graph transformation applicability Note: to be declined in Sparql.","title":"Applicability of graph transformations"},{"location":"research/graph-transfo/#grammar-for-graph-transformation","text":"In the context of the page on industry data , please have a look at the grammar of graph transformation . (Last update: July 2021)","title":"Grammar for graph transformation"},{"location":"research/graph-transformation-applicability/","text":"Graph transformation applicability The note about the DSL opens the problem of GT applicability. This is a theoretical problem but rather interesting. We are in the context of GTs that are attached to root nodes. Dictionary of applicable GTs per type It is possible to record in a dictionary the list of GTs applicable from a certain root node type. Once recorded, we can find the rules that enable to determine if each of the GT, that is potentially applicable is really applicable. Position of the applicability code When a GT is created and then forked, we can consider we have 2 versions of the GT: GTv1 and GTv2 for instance. GTv1 will be at the beginning applicable forever until it is forked. There seem to be 3 cases: The forking of the GT is necessary when there is a topological change, and in that case, the GTv2 will not apply to the samle topology than GTv1 . In a way, the applicability information is encoded into the graph topology itself. There are unfortunately many cases where GTv2 exists because the rule changes but not the topology. We identified this case as being difficult, because there is no real standard rule to decide if we had to fork or not. If we did, the 2 versions of the GT may apply at the same time, if we consider their topology checks only (that are the same in that case). The 2 cases are the following: We encode the applicability knowledge into the GT, Or we encode the applicability knowledge outside the GT. The first case will require a possible modification of the GTv1 when GTv2 appears, because it will need to chack its version or a certain date of applicability from which it becomes not applicable. In the second case, we can imagine a controler that knows what rule to apply considering, for instance, the age of data we are looking for. In that case, that would be, for instance, the age of the root node that would be a good first indicator to determine what version is applicable. Heterogeneous encoding of knowledge The core problem of the graph-oriented programming works was the placement of the rules in transient structures representing the graph. Here also, we have the same problem. It is not easy to encode a certain knowledge right inside the various artifacts that we have. Quite often, this is a design choice, which implies that we cannot find a rule that answers to all cases. Knowledge adequate/optimized/relevant encoding is very complex. Maybe the GT should be analyzed in semantic graphs that are simpler than directed multi-labeled graphs.","title":"Graph transformation applicability"},{"location":"research/graph-transformation-applicability/#graph-transformation-applicability","text":"The note about the DSL opens the problem of GT applicability. This is a theoretical problem but rather interesting. We are in the context of GTs that are attached to root nodes.","title":"Graph transformation applicability"},{"location":"research/graph-transformation-applicability/#dictionary-of-applicable-gts-per-type","text":"It is possible to record in a dictionary the list of GTs applicable from a certain root node type. Once recorded, we can find the rules that enable to determine if each of the GT, that is potentially applicable is really applicable.","title":"Dictionary of applicable GTs per type"},{"location":"research/graph-transformation-applicability/#position-of-the-applicability-code","text":"When a GT is created and then forked, we can consider we have 2 versions of the GT: GTv1 and GTv2 for instance. GTv1 will be at the beginning applicable forever until it is forked. There seem to be 3 cases: The forking of the GT is necessary when there is a topological change, and in that case, the GTv2 will not apply to the samle topology than GTv1 . In a way, the applicability information is encoded into the graph topology itself. There are unfortunately many cases where GTv2 exists because the rule changes but not the topology. We identified this case as being difficult, because there is no real standard rule to decide if we had to fork or not. If we did, the 2 versions of the GT may apply at the same time, if we consider their topology checks only (that are the same in that case). The 2 cases are the following: We encode the applicability knowledge into the GT, Or we encode the applicability knowledge outside the GT. The first case will require a possible modification of the GTv1 when GTv2 appears, because it will need to chack its version or a certain date of applicability from which it becomes not applicable. In the second case, we can imagine a controler that knows what rule to apply considering, for instance, the age of data we are looking for. In that case, that would be, for instance, the age of the root node that would be a good first indicator to determine what version is applicable.","title":"Position of the applicability code"},{"location":"research/graph-transformation-applicability/#heterogeneous-encoding-of-knowledge","text":"The core problem of the graph-oriented programming works was the placement of the rules in transient structures representing the graph. Here also, we have the same problem. It is not easy to encode a certain knowledge right inside the various artifacts that we have. Quite often, this is a design choice, which implies that we cannot find a rule that answers to all cases. Knowledge adequate/optimized/relevant encoding is very complex. Maybe the GT should be analyzed in semantic graphs that are simpler than directed multi-labeled graphs.","title":"Heterogeneous encoding of knowledge"},{"location":"research/graphapps/","text":"GraphApps project What is GraphApps? GraphApps is a research project (first phase: 2013-2018) that aims to use graphs and graph transformations in the context of software engineering, like application building and data migration. The first phase of GraphApps was focused on the use of attributed directed graphs in software engineering. Graph-oriented programming and technical debt You can read the following articles: First article on graph oriented programming Slides from the ICGT 2018 conference The code that was produced during this period is under copyright and so, unfortunately, this code is not available. Important points Those works brought several results. Object-oriented and RDBMS-based software engineering generates a lot of couplings (structural and temporal). Those couplings are, for us, at the center of the technical debt problem. The way we represent knowledge in current software engineering is largely sub-optimal , and we believe that the technical debt is a problem created by bad engineering practices and tools (OOP/RDBMS) and not attached to the business semantics. A new programming model called \"graph-oriented programming\" enables to limit the technical debt to its minimal expression (semantic couplings). This programming model is using: Graph databases (attributed directed graph databases); Graph transformations to model business logic. This programming model is an intermediate between object-orientation and functional programming. Shortly said, it takes the best out of the two programming models while being totally consistent because based on the business semantics and not on technical considerations. See also The research page . (June 2020)","title":"GraphApps project page"},{"location":"research/graphapps/#graphapps-project","text":"","title":"GraphApps project"},{"location":"research/graphapps/#what-is-graphapps","text":"GraphApps is a research project (first phase: 2013-2018) that aims to use graphs and graph transformations in the context of software engineering, like application building and data migration. The first phase of GraphApps was focused on the use of attributed directed graphs in software engineering.","title":"What is GraphApps?"},{"location":"research/graphapps/#graph-oriented-programming-and-technical-debt","text":"You can read the following articles: First article on graph oriented programming Slides from the ICGT 2018 conference The code that was produced during this period is under copyright and so, unfortunately, this code is not available.","title":"Graph-oriented programming and technical debt"},{"location":"research/graphapps/#important-points","text":"Those works brought several results. Object-oriented and RDBMS-based software engineering generates a lot of couplings (structural and temporal). Those couplings are, for us, at the center of the technical debt problem. The way we represent knowledge in current software engineering is largely sub-optimal , and we believe that the technical debt is a problem created by bad engineering practices and tools (OOP/RDBMS) and not attached to the business semantics. A new programming model called \"graph-oriented programming\" enables to limit the technical debt to its minimal expression (semantic couplings). This programming model is using: Graph databases (attributed directed graph databases); Graph transformations to model business logic. This programming model is an intermediate between object-orientation and functional programming. Shortly said, it takes the best out of the two programming models while being totally consistent because based on the business semantics and not on technical considerations.","title":"Important points"},{"location":"research/graphapps/#see-also","text":"The research page . (June 2020)","title":"See also"},{"location":"research/index-research/","text":"Personal Research Main Page Main Ideas All research projects are founded on main ideas. Here are the main ideas that are underlying to the GraphApps project. Idea 1 : In the software engineering world, our ways of representing knowledge are bad, inefficient and non evolutive , being at the programming level or at the data level. This fact generates massive costs and pain in evolution of software or evolution of data storage. In one formula: Software is far from beware \"soft\". Idea 2 : Graphs enable better knowledge representation , both static and within the evolution dynamics of programs and data. For instance, graphs can be used to address the technical debt problem (see here and here ) or to address complex data migration (see here ). Idea 3 : Graph transformations are at the heart of new software engineering paradigms , such as graph-oriented programming (see here and here ) or semantic data migration (see here ). For sure, a new generation of tools is required to be efficient with those paradigms (design, programming languages, data storage). Idea 4 : With a real evolutive software and data, the total worldwide software effort could be divided by more than 2, probably 3 , which means: An enormous worldwide gain of productivity; A capability of evolving the business processes faster than ever before; A convergence of the design processes of software creation, software evolution and refactoring; A different mindset based on graph representation of knowledge and graph transformations for business rules. The Graph Transformations at the center See the Graph transformations page. Industrial Projects 2 main industrial projects were developed based on the 4 ideas and are implementing concepts described in the graph transformation page: GraphApps project project (2013-2018) Semantic data migration project (2018-now) Open-source projects Here is a list: The graph repo: tests on graph transformations (Python project). The graphappsjs repo, which is targeting the same objective but analyzes the link of the programming language in that problem (JS project). The csv2rdf repo: able to turn a CSV file into RDF with a graph transformation grammar (Python project). The rdfviz repo proposes to visualize rdf graph data (Python project). The ontovisu repo aims at enabling the neighborhood-based visual navigation in RDF graph data (JS project). More UML to RDF considerations - Updated June 2020 Reflections on industry data Towards a graph-oriented programming language? Resources (June 2020)","title":"Index"},{"location":"research/index-research/#personal-research-main-page","text":"","title":"Personal Research Main Page"},{"location":"research/index-research/#main-ideas","text":"All research projects are founded on main ideas. Here are the main ideas that are underlying to the GraphApps project. Idea 1 : In the software engineering world, our ways of representing knowledge are bad, inefficient and non evolutive , being at the programming level or at the data level. This fact generates massive costs and pain in evolution of software or evolution of data storage. In one formula: Software is far from beware \"soft\". Idea 2 : Graphs enable better knowledge representation , both static and within the evolution dynamics of programs and data. For instance, graphs can be used to address the technical debt problem (see here and here ) or to address complex data migration (see here ). Idea 3 : Graph transformations are at the heart of new software engineering paradigms , such as graph-oriented programming (see here and here ) or semantic data migration (see here ). For sure, a new generation of tools is required to be efficient with those paradigms (design, programming languages, data storage). Idea 4 : With a real evolutive software and data, the total worldwide software effort could be divided by more than 2, probably 3 , which means: An enormous worldwide gain of productivity; A capability of evolving the business processes faster than ever before; A convergence of the design processes of software creation, software evolution and refactoring; A different mindset based on graph representation of knowledge and graph transformations for business rules.","title":"Main Ideas"},{"location":"research/index-research/#the-graph-transformations-at-the-center","text":"See the Graph transformations page.","title":"The Graph Transformations at the center"},{"location":"research/index-research/#industrial-projects","text":"2 main industrial projects were developed based on the 4 ideas and are implementing concepts described in the graph transformation page: GraphApps project project (2013-2018) Semantic data migration project (2018-now)","title":"Industrial Projects"},{"location":"research/index-research/#open-source-projects","text":"Here is a list: The graph repo: tests on graph transformations (Python project). The graphappsjs repo, which is targeting the same objective but analyzes the link of the programming language in that problem (JS project). The csv2rdf repo: able to turn a CSV file into RDF with a graph transformation grammar (Python project). The rdfviz repo proposes to visualize rdf graph data (Python project). The ontovisu repo aims at enabling the neighborhood-based visual navigation in RDF graph data (JS project).","title":"Open-source projects"},{"location":"research/index-research/#more","text":"UML to RDF considerations - Updated June 2020 Reflections on industry data Towards a graph-oriented programming language? Resources (June 2020)","title":"More"},{"location":"research/industry-data/","text":"Reflections on industry data The Graal of the single data model In the IT world, the idea of interoperability seems OK for a long time. It is largely accepted that many systems have to collaborate around the same data and that a standard neutral shared data format is enabling: Every system to do manage its own business; All system to share a minimal set of semantic data. In the industry, things are a bit more complicated for the following reasons: Data have complex life-cycles; Data are interconnected to each other; Data may be transformed in order to be used (for instance in simulation). For sure, data outside the industry have also complex lifecycles, but there are specific applications designed to manage them with their own business rules. Due to the fact that the core element of industrial data was the \"part\", standard software like PDMs manipulated those parts and made everyone believe that it was easy to have a single representation of things based on parts. The Requirements, Functional, Logical and Physical (RFLP) model The RFLP model, largely accepted now in the industry, showed that many artifacts were required to be able to create and manage the lifecycle of a product, each of them being part of a certain level of representation and being interconnected to another level. This approach is already largely known in enterprise architecture where different levels of representation of the enterprise enable complex multi-level modelings (cf. Archimate). The semantic graphs at the center of the game At the center of those multiple representation views are the semantic graphs. Those graphs can be seen as a two level semantic universe (see Fig. 1): Each semantic domain owns its own set of artifacts that are interconnected through various kinds of links (predicates); The domains are interconnected together by other kind of predicates (inter-domain predicates). For sure, those predicates are touching source and destination nodes from several semantic domains but enable to attach the various semantic domains through a set of semantic links. Figure 1: Semantic spaces in industry data Graph transformations The formalism of graph transformations enables to transform the data inside a semantic space but also outside (see Fig. 2). Figure 2: Graph transformations in semantic spaces (Last update: June 2020)","title":"Reflections on industry data"},{"location":"research/industry-data/#reflections-on-industry-data","text":"","title":"Reflections on industry data"},{"location":"research/industry-data/#the-graal-of-the-single-data-model","text":"In the IT world, the idea of interoperability seems OK for a long time. It is largely accepted that many systems have to collaborate around the same data and that a standard neutral shared data format is enabling: Every system to do manage its own business; All system to share a minimal set of semantic data. In the industry, things are a bit more complicated for the following reasons: Data have complex life-cycles; Data are interconnected to each other; Data may be transformed in order to be used (for instance in simulation). For sure, data outside the industry have also complex lifecycles, but there are specific applications designed to manage them with their own business rules. Due to the fact that the core element of industrial data was the \"part\", standard software like PDMs manipulated those parts and made everyone believe that it was easy to have a single representation of things based on parts.","title":"The Graal of the single data model"},{"location":"research/industry-data/#the-requirements-functional-logical-and-physical-rflp-model","text":"The RFLP model, largely accepted now in the industry, showed that many artifacts were required to be able to create and manage the lifecycle of a product, each of them being part of a certain level of representation and being interconnected to another level. This approach is already largely known in enterprise architecture where different levels of representation of the enterprise enable complex multi-level modelings (cf. Archimate).","title":"The Requirements, Functional, Logical and Physical (RFLP) model"},{"location":"research/industry-data/#the-semantic-graphs-at-the-center-of-the-game","text":"At the center of those multiple representation views are the semantic graphs. Those graphs can be seen as a two level semantic universe (see Fig. 1): Each semantic domain owns its own set of artifacts that are interconnected through various kinds of links (predicates); The domains are interconnected together by other kind of predicates (inter-domain predicates). For sure, those predicates are touching source and destination nodes from several semantic domains but enable to attach the various semantic domains through a set of semantic links. Figure 1: Semantic spaces in industry data","title":"The semantic graphs at the center of the game"},{"location":"research/industry-data/#graph-transformations","text":"The formalism of graph transformations enables to transform the data inside a semantic space but also outside (see Fig. 2). Figure 2: Graph transformations in semantic spaces (Last update: June 2020)","title":"Graph transformations"},{"location":"research/rdf-design-patterns/","text":"UML to RDF considerations References In a W3C article, Semantic Web Best Practices , some indications are provided to compare the semantic web approach with the object-oriented approach. Those indications are very interesting but they are not complete to serve as a guide for software engineers trying to work with semantic web technologies. This page aims at gathering interesting elements about that topic. Basic correspondence between UML and rdf/rdfs Consider the following figure. Several points must be noted. Types are instances of rdf:Property and rdfs:Class Basically, in most existing (RDFS) ontologies, when a new class is created, it is declared as being a rdfs:Class , and when a new property type is created, it is declared as being a rdf:Property . We have this situation in the diagram for exs:ClassA , exs:ClassB , exs:ClassC and for exs:attribute1 and exs:attribute2 . That means that new types, being class types or property types, are considered as \"instances\" of the \"meta-types\" rdfs:Class and rdf:Property . First translation In the UML diagram, ClassB derives from ClassA . That implies: exs:ClassB rdfs:subClassOf exs:ClassA as an obvious translation of the UML fact. We can stay like that and have a first UML to RDF translation like follows. @prefix rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# . @prefix rdfs: http://www.w3.org/2000/01/rdf-schema# . @prefix owl: http://www.w3.org/2002/07/owl# . @prefix exs: https://orey.github.io/sample-schema.ttl# . exs:ClassA a rdfs:Class. exs:ClassB a rdfs:Class ; rdfs:subClassOf exs:ClassA . exs:ClassC a rdfs:Class. exs:attribute1 a rdf:Property ; rdfs:domain exs:ClassA ; rdfs:range rdf:langString . exs:attribute2 a rdf:Property ; rdfs:domain exs:ClassA ; rdfs:range exs:ClassC . Second translation This first translation may be considered as having a slight problem. All new classes could be subClassOf another class, in order to enable reasoning based on sub-classes. We could enrich a bit our first translation by adding some lines. @prefix rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# . @prefix rdfs: http://www.w3.org/2000/01/rdf-schema# . @prefix owl: http://www.w3.org/2002/07/owl# . @prefix exs: https://orey.github.io/sample-schema.ttl# . exs:ClassA a rdfs:Class ; rdfs:subClassOf rdfs:Class . = NEW exs:ClassB a rdfs:Class ; rdfs:subClassOf exs:ClassA . exs:ClassC a rdfs:Class ; rdfs:subClassOf rdfs:Class . = NEW exs:attribute1 a rdf:Property; rdfs:subPropertyOf rdf:Property ; = NEW rdfs:domain exs:ClassA ; rdfs:range rdf:langString . exs:attribute2 a rdf:Property ; rdfs:subPropertyOf rdf:Property ; = NEW rdfs:domain exs:ClassA ; rdfs:range exs:ClassC . Semantically, the a indicates the real nature of the entity and the rdfs:subClass/PropertyOf its type hierarchy. 2 kinds of attribute types Attribute types are not homogeneous. For a given attribute exs:att , exs:att rdfs:range rdfs:Class may indicate that we are talking about a link or relationship between two objects, whereas exs:att rdfs:range rdfs:Literal may indicate that we have a real attribute with a literal value. That is one UML and object-orientation deepest problem: the aggregation (and composition) relationships are represented at the same level than the attributes pointing to literal values (see paper on graph-oriented programming ). The semantic web enables us to get over this crucial limitation by defining the attribute outside of the class itself. But, if we want to be effective, we would need a specific root attribute type for the attribute types having a range in the rdfs:Literal world, and a root attribute type for the attribute types that point to objects. That leads us to the third translation below. @prefix rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# . @prefix rdfs: http://www.w3.org/2000/01/rdf-schema# . @prefix owl: http://www.w3.org/2002/07/owl# . @prefix dc: http://purl.org/dc/elements/1.1/ . @prefix ga: https://orey.github.io/graphapps-V2# . @prefix exs: https://orey.github.io/sample-schema.ttl# . exs:ClassA a rdfs:Class ; rdfs:subClassOf ga:Class . = NEW exs:ClassB a rdfs:Class ; rdfs:subClassOf exs:ClassA . exs:ClassC a rdfs:Class ; rdfs:subClassOf ga:Class . = NEW exs:attribute1 a rdf:Property ; rdfs:subPropertyOf ga:attribute ; = NEW rdfs:domain exs:ClassA ; rdfs:range rdf:langString . exs:attribute2 a rdf:Property ; rdfs:subPropertyOf ga:link ; = NEW rdfs:domain exs:ClassA ; rdfs:range exs:ClassC . The root attribute type for attribute types having a range in rdfs:Literal is ga:attribute , and the root attribute type for attribute types that point to classes is ga:link . If we want to be sure that classes are realm classes, and because rdfs:Class maybe sometimes too large, we defined ga:Class as the root type for all classes. That provides the following RDFS-based ontology: @prefix rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# . @prefix rdfs: http://www.w3.org/2000/01/rdf-schema# . @prefix owl: http://www.w3.org/2002/07/owl# . @prefix dc: http://purl.org/dc/elements/1.1/ . @prefix ga: https://orey.github.io/graphapps-V2# . https://orey.github.io/graphapps-V2# a owl:Ontology ; dc:title GraphApps ontology for building semantic applications ; dc:date 2020-06-21 ; dc:description This ontology describe the minimal set of constructs required to build applications in the spirit of the GraphApps works. . ga:Class a rdfs:Class ; rdfs:subClassOf rdfs:Class ; rdfs:isDefinedBy https://orey.github.io/graphapps-V2# ; rdfs:label Class ; rdfs:comment Root class of the GraphApps classes. . ga:attribute a rdf:Property ; rdfs:subPropertyOf rdf:Property ; rdfs:isDefinedBy https://orey.github.io/graphapps-V2# ; rdfs:label attribute ; rdfs:comment Root property of final data attributes. ; rdfs:domain ga:Class ; rdfs:range rdfs:Literal . ga:link a rdf:Property ; rdfs:subPropertyOf rdf:Property ; rdfs:isDefinedBy https://orey.github.io/graphapps-V2# ; rdfs:label link ; rdfs:comment Root property of link between ga:Class. ; rdfs:domain ga:Class ; rdfs:range rdfs:Class . ga:id a ga:attribute ; rdfs:isDefinedBy https://orey.github.io/graphapps-V2# ; rdfs:label id ; rdfs:comment Instance of attribute type to define a unique id for ga:Class instances. ; rdfs:domain ga:Class ; rdfs:range rdfs:Literal . Such a grammar is enabling several things: Translate UML concepts in semantic web concepts; Distinguish between topological links between concepts, and values of attributes; Reason on less ambiguous concepts; Benefit from the RDF/RDFS definition of attributes out of classes; Enable graph transformations based on a set of non ambiguous concepts. The grammar can be found here . (June 2020)","title":"UML to RDF considerations"},{"location":"research/rdf-design-patterns/#uml-to-rdf-considerations","text":"","title":"UML to RDF considerations"},{"location":"research/rdf-design-patterns/#references","text":"In a W3C article, Semantic Web Best Practices , some indications are provided to compare the semantic web approach with the object-oriented approach. Those indications are very interesting but they are not complete to serve as a guide for software engineers trying to work with semantic web technologies. This page aims at gathering interesting elements about that topic.","title":"References"},{"location":"research/rdf-design-patterns/#basic-correspondence-between-uml-and-rdfrdfs","text":"Consider the following figure. Several points must be noted.","title":"Basic correspondence between UML and rdf/rdfs"},{"location":"research/rdf-design-patterns/#types-are-instances-of-rdfproperty-and-rdfsclass","text":"Basically, in most existing (RDFS) ontologies, when a new class is created, it is declared as being a rdfs:Class , and when a new property type is created, it is declared as being a rdf:Property . We have this situation in the diagram for exs:ClassA , exs:ClassB , exs:ClassC and for exs:attribute1 and exs:attribute2 . That means that new types, being class types or property types, are considered as \"instances\" of the \"meta-types\" rdfs:Class and rdf:Property .","title":"Types are instances of rdf:Property and rdfs:Class"},{"location":"research/rdf-design-patterns/#first-translation","text":"In the UML diagram, ClassB derives from ClassA . That implies: exs:ClassB rdfs:subClassOf exs:ClassA as an obvious translation of the UML fact. We can stay like that and have a first UML to RDF translation like follows. @prefix rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# . @prefix rdfs: http://www.w3.org/2000/01/rdf-schema# . @prefix owl: http://www.w3.org/2002/07/owl# . @prefix exs: https://orey.github.io/sample-schema.ttl# . exs:ClassA a rdfs:Class. exs:ClassB a rdfs:Class ; rdfs:subClassOf exs:ClassA . exs:ClassC a rdfs:Class. exs:attribute1 a rdf:Property ; rdfs:domain exs:ClassA ; rdfs:range rdf:langString . exs:attribute2 a rdf:Property ; rdfs:domain exs:ClassA ; rdfs:range exs:ClassC .","title":"First translation"},{"location":"research/rdf-design-patterns/#second-translation","text":"This first translation may be considered as having a slight problem. All new classes could be subClassOf another class, in order to enable reasoning based on sub-classes. We could enrich a bit our first translation by adding some lines. @prefix rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# . @prefix rdfs: http://www.w3.org/2000/01/rdf-schema# . @prefix owl: http://www.w3.org/2002/07/owl# . @prefix exs: https://orey.github.io/sample-schema.ttl# . exs:ClassA a rdfs:Class ; rdfs:subClassOf rdfs:Class . = NEW exs:ClassB a rdfs:Class ; rdfs:subClassOf exs:ClassA . exs:ClassC a rdfs:Class ; rdfs:subClassOf rdfs:Class . = NEW exs:attribute1 a rdf:Property; rdfs:subPropertyOf rdf:Property ; = NEW rdfs:domain exs:ClassA ; rdfs:range rdf:langString . exs:attribute2 a rdf:Property ; rdfs:subPropertyOf rdf:Property ; = NEW rdfs:domain exs:ClassA ; rdfs:range exs:ClassC . Semantically, the a indicates the real nature of the entity and the rdfs:subClass/PropertyOf its type hierarchy.","title":"Second translation"},{"location":"research/rdf-design-patterns/#2-kinds-of-attribute-types","text":"Attribute types are not homogeneous. For a given attribute exs:att , exs:att rdfs:range rdfs:Class may indicate that we are talking about a link or relationship between two objects, whereas exs:att rdfs:range rdfs:Literal may indicate that we have a real attribute with a literal value. That is one UML and object-orientation deepest problem: the aggregation (and composition) relationships are represented at the same level than the attributes pointing to literal values (see paper on graph-oriented programming ). The semantic web enables us to get over this crucial limitation by defining the attribute outside of the class itself. But, if we want to be effective, we would need a specific root attribute type for the attribute types having a range in the rdfs:Literal world, and a root attribute type for the attribute types that point to objects. That leads us to the third translation below. @prefix rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# . @prefix rdfs: http://www.w3.org/2000/01/rdf-schema# . @prefix owl: http://www.w3.org/2002/07/owl# . @prefix dc: http://purl.org/dc/elements/1.1/ . @prefix ga: https://orey.github.io/graphapps-V2# . @prefix exs: https://orey.github.io/sample-schema.ttl# . exs:ClassA a rdfs:Class ; rdfs:subClassOf ga:Class . = NEW exs:ClassB a rdfs:Class ; rdfs:subClassOf exs:ClassA . exs:ClassC a rdfs:Class ; rdfs:subClassOf ga:Class . = NEW exs:attribute1 a rdf:Property ; rdfs:subPropertyOf ga:attribute ; = NEW rdfs:domain exs:ClassA ; rdfs:range rdf:langString . exs:attribute2 a rdf:Property ; rdfs:subPropertyOf ga:link ; = NEW rdfs:domain exs:ClassA ; rdfs:range exs:ClassC . The root attribute type for attribute types having a range in rdfs:Literal is ga:attribute , and the root attribute type for attribute types that point to classes is ga:link . If we want to be sure that classes are realm classes, and because rdfs:Class maybe sometimes too large, we defined ga:Class as the root type for all classes. That provides the following RDFS-based ontology: @prefix rdf: http://www.w3.org/1999/02/22-rdf-syntax-ns# . @prefix rdfs: http://www.w3.org/2000/01/rdf-schema# . @prefix owl: http://www.w3.org/2002/07/owl# . @prefix dc: http://purl.org/dc/elements/1.1/ . @prefix ga: https://orey.github.io/graphapps-V2# . https://orey.github.io/graphapps-V2# a owl:Ontology ; dc:title GraphApps ontology for building semantic applications ; dc:date 2020-06-21 ; dc:description This ontology describe the minimal set of constructs required to build applications in the spirit of the GraphApps works. . ga:Class a rdfs:Class ; rdfs:subClassOf rdfs:Class ; rdfs:isDefinedBy https://orey.github.io/graphapps-V2# ; rdfs:label Class ; rdfs:comment Root class of the GraphApps classes. . ga:attribute a rdf:Property ; rdfs:subPropertyOf rdf:Property ; rdfs:isDefinedBy https://orey.github.io/graphapps-V2# ; rdfs:label attribute ; rdfs:comment Root property of final data attributes. ; rdfs:domain ga:Class ; rdfs:range rdfs:Literal . ga:link a rdf:Property ; rdfs:subPropertyOf rdf:Property ; rdfs:isDefinedBy https://orey.github.io/graphapps-V2# ; rdfs:label link ; rdfs:comment Root property of link between ga:Class. ; rdfs:domain ga:Class ; rdfs:range rdfs:Class . ga:id a ga:attribute ; rdfs:isDefinedBy https://orey.github.io/graphapps-V2# ; rdfs:label id ; rdfs:comment Instance of attribute type to define a unique id for ga:Class instances. ; rdfs:domain ga:Class ; rdfs:range rdfs:Literal . Such a grammar is enabling several things: Translate UML concepts in semantic web concepts; Distinguish between topological links between concepts, and values of attributes; Reason on less ambiguous concepts; Benefit from the RDF/RDFS definition of attributes out of classes; Enable graph transformations based on a set of non ambiguous concepts. The grammar can be found here . (June 2020)","title":"2 kinds of attribute types"},{"location":"research/resources/","text":"Resources Back to index Technical debt The well-known curve of software technical debt. Maintenance costs The rule of 60/60 (Robert L. Glass): Between 40% and 80% of software costs are maintenance costs (average 60%); Around 60% of maintenance costs are enhancements. Source: Frequently forgotten fundamental facts about software engineering - alternate . This figure seems rather optimistic and the interval may more be 60-80% for maintenance costs. (Same source) Back to index (Last update: June 2020)","title":"Resources"},{"location":"research/resources/#resources","text":"Back to index","title":"Resources"},{"location":"research/resources/#technical-debt","text":"The well-known curve of software technical debt.","title":"Technical debt"},{"location":"research/resources/#maintenance-costs","text":"The rule of 60/60 (Robert L. Glass): Between 40% and 80% of software costs are maintenance costs (average 60%); Around 60% of maintenance costs are enhancements. Source: Frequently forgotten fundamental facts about software engineering - alternate . This figure seems rather optimistic and the interval may more be 60-80% for maintenance costs. (Same source) Back to index (Last update: June 2020)","title":"Maintenance costs"}]}