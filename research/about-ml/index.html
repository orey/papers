<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        
        <meta name="author" content="Olivier Rey">
        <link rel="canonical" href="https://orey.github.io/papers/research/about-ml/">
        <link rel="shortcut icon" href="../../img/favicon.ico">
        <title>About artificial neural networks and machine learning - Papers</title>
        <link href="../../css/bootstrap-custom.min.css" rel="stylesheet">
        <link href="../../css/font-awesome.min.css" rel="stylesheet">
        <link href="../../css/base.css" rel="stylesheet">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css">
        <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
        <!--[if lt IE 9]>
            <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
            <script src="https://oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
        <![endif]-->

        <script src="../../js/jquery-1.10.2.min.js" defer></script>
        <script src="../../js/bootstrap-3.0.3.min.js" defer></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
        <script>hljs.initHighlightingOnLoad();</script> 
    </head>

    <body>

        <div class="navbar navbar-default navbar-fixed-top" role="navigation">
            <div class="container">

                <!-- Collapsed navigation -->
                <div class="navbar-header">
                    <!-- Expander button -->
                    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-collapse">
                        <span class="sr-only">Toggle navigation</span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                        <span class="icon-bar"></span>
                    </button>
                    <a class="navbar-brand" href="../..">Papers</a>
                </div>

                <!-- Expanded navigation -->
                <div class="navbar-collapse collapse">
                        <!-- Main navigation -->
                        <ul class="nav navbar-nav">
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Home <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../..">Index</a>
</li>
                                    
<li >
    <a href="../../about/about/">Author</a>
</li>
                                    
<li >
    <a href="../../about/LICENSE/">License</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">PLM <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../articles/about-plm/">PLM and graph data</a>
</li>
                                    
<li >
    <a href="../../articles/conf-mgt/">Configuration management of industrial products in PDM/PLM</a>
</li>
                                    
<li >
    <a href="../../articles/spreadsheet-and-PLM/">The four core functions showing you need a PLM</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Modeling <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../articles/portfolio/">A Simple Meta-Model for Portfolio Management</a>
</li>
                                    
<li >
    <a href="../../articles/archimate-intro/">An Introduction to The Archimate Revolution</a>
</li>
                                    
<li >
    <a href="../../articles/archimate-recipes/">Archimate Recipes</a>
</li>
                                    
<li >
    <a href="../../articles/mbse-vs-ea/">Military frameworks, systems engineering and enterprise architecture</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown active">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Data <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li class="active">
    <a href="./">About artificial neural networks and machine learning</a>
</li>
                                    
<li >
    <a href="../../articles/about-rest/">Considerations About Rest And Web Services</a>
</li>
                                    
<li >
    <a href="../../articles/graphql-web-services/">GraphQL And Classic Web Services</a>
</li>
                                    
<li >
    <a href="../../articles/data-interop/">The real nature of data</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">IT <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../../articles/five-levels/">The Five Levels of Conceptual Maturity for IT Teams</a>
</li>
                                    
<li >
    <a href="../../articles/the-v2-vortex/">The V2 Vortex</a>
</li>
                                    
<li >
    <a href="../../articles/various-stages/">The Various Stages of Digital Transformation</a>
</li>
                                </ul>
                            </li>
                            <li class="dropdown">
                                <a href="#" class="dropdown-toggle" data-toggle="dropdown">Research <b class="caret"></b></a>
                                <ul class="dropdown-menu">
                                    
<li >
    <a href="../index-research/">Index</a>
</li>
                                    
  <li class="dropdown-submenu">
    <a href="#">Graph-oriented programming</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../../graph/first-article/">First article on graph-oriented programming</a>
</li>
            
<li >
    <a href="../../graph/staf-icgt2018/">Conference at the STAF/ICGT 2018 in Toulouse</a>
</li>
            
<li >
    <a href="../graphapps/">GraphApps project page</a>
</li>
            
<li >
    <a href="../graph-oriented-pl/">Towards a graph-oriented programming language?</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">Semantic Web</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../data-mig/">Aerospace data migration</a>
</li>
            
<li >
    <a href="../data-mig/">Semantic data migration project page</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">Graph transformations</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../graph-transfo/">Graph transformations</a>
</li>
            
<li >
    <a href="../basic-graph-transformations/">Basic graph transformations</a>
</li>
            
<li >
    <a href="../basic-semantic-graph-transformations/">Basic semantic graph transformations</a>
</li>
            
<li >
    <a href="../DSL-for-graph-topology-checks/">DSL for graph topology check</a>
</li>
            
<li >
    <a href="../graph-transformation-applicability/">Graph transformation applicability</a>
</li>
            
<li >
    <a href="../grammar-graph-transformation/">Grammar of graph transformation</a>
</li>
            
<li >
    <a href="../graph-oriented-pl/">Graph-oriented programming language</a>
</li>
    </ul>
  </li>
                                    
  <li class="dropdown-submenu">
    <a href="#">Various</a>
    <ul class="dropdown-menu">
            
<li >
    <a href="../industry-data/">Reflections on industry data</a>
</li>
            
<li >
    <a href="../rdf-design-patterns/">UML to RDF considerations</a>
</li>
            
<li >
    <a href="../resources/">Resources</a>
</li>
    </ul>
  </li>
                                </ul>
                            </li>
                        </ul>

                    <ul class="nav navbar-nav navbar-right">
                        <li>
                            <a href="#" data-toggle="modal" data-target="#mkdocs_search_modal">
                                <i class="fa fa-search"></i> Search
                            </a>
                        </li>
                            <li >
                                <a rel="next" href="../../articles/mbse-vs-ea/">
                                    <i class="fa fa-arrow-left"></i> Previous
                                </a>
                            </li>
                            <li >
                                <a rel="prev" href="../../articles/about-rest/">
                                    Next <i class="fa fa-arrow-right"></i>
                                </a>
                            </li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="container">
                <div class="col-md-3"><div class="bs-sidebar hidden-print affix well" role="complementary">
    <ul class="nav bs-sidenav">
        <li class="main active"><a href="#about-artificial-neural-networks-and-machine-learning">About artificial neural networks and machine learning</a></li>
            <li><a href="#what-is-an-artificial-neural-network">What is an artificial neural network?</a></li>
            <li><a href="#characteristics-of-a-standard-artificial-neural-network">Characteristics of a standard artificial neural network</a></li>
            <li><a href="#an-interpolation-function">An interpolation function</a></li>
            <li><a href="#an-interpolation-function-working-in-extrapolation-mode">An interpolation function working in extrapolation mode</a></li>
            <li><a href="#big-data-and-processing-power">Big data and processing power</a></li>
            <li><a href="#but-the-banana-disappears">But the banana disappears...</a></li>
            <li><a href="#new-beliefs-brought-by-the-machine-learning-era">New beliefs brought by the machine learning era</a></li>
            <li><a href="#belief-1-big-data-and-brute-force-can-solve-mathematical-problems">Belief 1: Big data and brute force can solve mathematical problems</a></li>
            <li><a href="#belief-2-programs-can-make-mistakes-like-humans">Belief 2: Programs can make mistakes like humans</a></li>
            <li><a href="#belief-3-the-marvelous-ubiquitous-set-of-tools-for-functionally-unskilled-people">Belief 3: The marvelous ubiquitous set of tools for functionally unskilled people</a></li>
            <li><a href="#belief-4-finding-meaning-in-data-without-knowing-the-business-domain">Belief 4: Finding meaning in data without knowing the business domain</a></li>
            <li><a href="#belief-5-the-average-wins-and-thats-ok">Belief 5: The average wins and that's OK</a></li>
            <li><a href="#philosophical-aspects-on-science">Philosophical aspects on science</a></li>
            <li><a href="#conclusion">Conclusion</a></li>
            <li><a href="#the-alternate-model-reasoning-on-semantic-data">The alternate model: Reasoning on semantic data</a></li>
            <li><a href="#note-on-image-encoder">Note on image encoder</a></li>
            <li><a href="#bibliography">Bibliography</a></li>
    </ul>
</div></div>
                <div class="col-md-9" role="main">

<h1 id="about-artificial-neural-networks-and-machine-learning">About artificial neural networks and machine learning</h1>
<p><img alt="Photo of a toy robot from the 50s" src="../../images/robot.jpg" /></p>
<p><em>Photo by <a href="https://freeimages.com/photographer/ollinger-36628">oliver brandt</a> from <a href="https://freeimages.com">FreeImages</a></em></p>
<p>Machine learning based on artificial neural networks attracted much attention and credits those last decades (after a long "winter of AI") up to the point where, boosted by marketing, an anti-scientific approach is sold as being the solution to all problems.</p>
<p>In this article, we will try to recall the simple mathematical reality behind artificial neural networks, and why this technology is not reliable, even if it attracks massive investments. We will also analyze the philosophical side of the neural networks.</p>
<p>Note: We will focus on supervised learning artificial neural networks and  will not talk now about other variants such as unsupervised learning.</p>
<h2 id="what-is-an-artificial-neural-network">What is an artificial neural network?</h2>
<p>Basically, an artificial neural network is:</p>
<ul>
<li>A mathematical function <code>f</code> of several variable;</li>
<li>A complex function construction algorithm:<ul>
<li>Using a representation of neurons and axons that was used to create the function <code>f</code>,</li>
<li>Based on a list of inputs of the form (x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>), matching with outputs of the form (y<sub>1</sub>, y<sub>2</sub>, ..., y<sub>p</sub>).</li>
</ul>
</li>
</ul>
<p><img alt="Image of a neural network" src="../../yed/ann.png" /></p>
<p><em>Figure 1: A simple artificial neural network</em></p>
<p>The Figure 1 shows a sample of an artificial neural network.</p>
<p>There are many techniques to build the function <code>f</code>, hence the variety of machine learning algorithms.</p>
<h2 id="characteristics-of-a-standard-artificial-neural-network">Characteristics of a standard artificial neural network</h2>
<p><img alt="Detailed view of a neuron" src="../../yed/ann-detail.png" /></p>
<p><em>Figure 2: Detailed view of a neuron</em></p>
<p>A neural network is a specific kind of directed labeled graph with the following characteristics:</p>
<ul>
<li>It has "layers".<ul>
<li>Every node on every layer (except the first one named "entry layer") is the <em>target</em> of nodes from the previous layer.</li>
<li>Except for the last layer or "output layer", each node of each layer is the source of edges going to the next layer.</li>
<li>Generally neural networks are represented from left to right.</li>
</ul>
</li>
<li>Every edge has a label that is called the "weight".</li>
<li>Every node has an internal function called "activation function" that is a function of the linear combination of inputs.</li>
</ul>
<p>For instance, in the Figure 2, the node <code>N</code> has an activation function <code>f</code> that is a function of &Sigma; = (X<sub>1</sub> x w<sub>1</sub>) + (X<sub>2</sub> x w<sub>2</sub>) + ... + (X<sub>5</sub> x w<sub>5</sub>). That function is often name &sigma; and we can note:</p>
<blockquote>
<p>&sigma; = f(&Sigma; + b), typically &sigma; = 1 / (1 - e<sup>-(&Sigma; + b)</sup>) (sigmoid or "logistic function")</p>
</blockquote>
<p>If the set of nodes tagged X<sub>i</sub> are not the input layer, that means that <strong><em>*</em></strong><strong>*</strong>*</p>
<h2 id="an-interpolation-function">An interpolation function</h2>
<p>Let us go a bit more into the details. Let us note :</p>
<blockquote>
<p>X<sub>k</sub> = (x<sub>k1</sub>, x<sub>k2</sub>, ..., x<sub>kn</sub>)</p>
<p>And Y<sub>m</sub> = (y<sub>m1</sub>, y<sub>m2</sub>, ..., y<sub>mp</sub>).</p>
</blockquote>
<p>Let us suppose we have a set of known inputs and related known outputs, (X<sub>k</sub>, Y<sub>m</sub>) for (k,m) known.</p>
<p>Considering a predefined neural network with nodes and edges in layers, we can use an algorithm to define characteristics of nodes and edges so that we define a function <code>f</code> like following:</p>
<blockquote>
<p>f : D<sub>1</sub> x D<sub>2</sub> x ... X D<sub>n</sub> &rarr; R<sub>1</sub> x R<sub>2</sub> x ... x R<sub>p</sub></p>
<p>(x<sub>1</sub>, x<sub>2</sub>, ..., x<sub>n</sub>) &rarr; (y<sub>1</sub>, y<sub>2</sub>, ..., y<sub>p</sub>)</p>
</blockquote>
<p>To create this function, the algorithm used tried, more of less, to satisfy the following hypothesis:</p>
<blockquote>
<p>f(X<sub>k</sub>) = Y<sub>m</sub></p>
</blockquote>
<p>This process is called "training the network".</p>
<p>The resulting function <code>f</code> is an <em>interpolation</em> function, defined by an algorithm. Once defined, <code>f</code> is a function of many variables that has the following characteristics:</p>
<ul>
<li>It is probable that f(X<sub>k</sub>) = Y<sub>m</sub> or | f(X<sub>k</sub>) - Y<sub>m</sub> | &lt; &epsi; with "|" a norm in the R<sub>1</sub> x R<sub>2</sub> x ... x R<sub>p</sub> space;</li>
<li>The determined global function is not globally mathematically known, which implies it can have singular points, often called "potential wells".</li>
</ul>
<h2 id="an-interpolation-function-working-in-extrapolation-mode">An interpolation function working in extrapolation mode</h2>
<p>The idea of neural networks is to assume that <code>f</code> is a continuous function at least in the neighborhood of the known hyper-points X<sub>i</sub>. That means that:</p>
<blockquote>
<p>If we take X<sub>i</sub> so that | X<sub>k</sub> - X<sub>i</sub> | &lt; &epsi;</p>
<p>Then | f(X<sub>k</sub>) - f(X<sub>i</sub>) | &lt; &epsi;'</p>
</blockquote>
<p>Said differently, any point in the neighborhood of X<sub>k</sub> will have its image by <code>f</code> in the neighborhood of f(X<sub>k</sub>).</p>
<p>When <code>f</code> matches this assertion, we can use <code>f</code> as an extrapolation function and consider that its evaluation on X<sub>i</sub> is valid.</p>
<p>The problem is that we have no ways of <em>knowing</em> that <code>f</code> is really continuous in a neighborhood of an hyper-point.</p>
<p>Let us consider the Figure 2.</p>
<p><img alt="Alt text" src="../../images/potential-well.gif" /></p>
<p><em>Figure 2: Two potential wells in a surface. Image from <a href="https://www.wired.com/wp-content/uploads/2014/06/63-plaster-logarithmics.gif">wired.com</a></em></p>
<p>The Figure 2 is an artist representation of two singularities in a 3D curve:</p>
<ul>
<li>One where <code>z</code> tends to +&infin;,</li>
<li>One where <code>z</code> tends to -&infin;.</li>
</ul>
<p>That means, for the first singularity, that we have the following situation:</p>
<blockquote>
<p>For a X<sub>i</sub> so that | X<sub>k</sub> - X<sub>i</sub> | &lt; &epsi;</p>
<p>| f(X<sub>k</sub>) - f(X<sub>i</sub>) | &gt;&gt; &epsi;'</p>
</blockquote>
<p>The presence of potential wells in neural networks was a well known phenomenon in the 80s, and one of the reasons of the AI winter. Because neural networks were considered as non reliable "by construction", those techniques were temporarily abandoned.</p>
<p>Mathematically speaking, the situation is quite simple: An interpolation function should not be used as an extrapolation function.</p>
<h2 id="big-data-and-processing-power">Big data and processing power</h2>
<p>The development and explosion of machine learning beginning of the 2000s was funded on new beliefs that overpass the previous arguments and mathematical reality.</p>
<p>The first idea (which is an engineer's idea) is that if we have a huge number of data (i.e. a lot of couples of the form (X<sub>k</sub>, Y<sub>m</sub>)) to feed the algorithm, we will be able to generate many hyper-points in the <code>f</code> hyper-surface.</p>
<p>This will imply that the <code>f</code> function will be more "dense" in R<sub>1</sub> x R<sub>2</sub> x ... x R<sub>p</sub> and so we will "reduce" the risk of potential well.</p>
<p>This idea is, from an engineering standpoint, of good sense, but mathematically, being able to generate one thousand points or one billion points will not change the mathematical reality of potential wells, especially when we talk about functions of many variables.</p>
<p>Let's take the example of the image classifier. First of all, we have to create a program to "encode" whatever image in the X<sub>k</sub> form. Then, in supervised learning, we will associate to an image representation X<sub>k</sub> a value Y<sub>m</sub> representing the result, for instance a number. Let us suppose the number is <code>12</code> and it represents the banana.</p>
<p>The <code>f</code> function that will be generated by the algorithm will be the result of "training" of many images associated with the result we expect. Then, when a new image will come, we will transform if into a X<sub>i</sub>, <em>hoping</em> that f(X<sub>i</sub>) will be meaningful.</p>
<p><img alt="Image: The chain of image recognition" src="../../yed/encoding.png" /></p>
<p><a name="Figure3"></a><em>Figure 3: The chain of image recognition</em></p>
<p>The fact is, it is a hope. What is funny is that <em>it works</em> in many cases. And when it doesn't, some various techniques are existing to try to make it work anyway.</p>
<p>We can note that, in that process, the encoding function may be quite important. In a certain way, it can hide a structural analysis of the problem, as it is representing the reality.</p>
<h2 id="but-the-banana-disappears">But the banana disappears...</h2>
<p><img alt="Image: The banana experiment" src="../../images/banana.png" /></p>
<p><a name="Figure4"></a>*<em>Figure 4: The banana disappears. Taken from <a href="https://arxiv.org/pdf/1712.09665.pdf">Brown et al. 2017</a></em></p>
<p>Without any surprise, Brown et al. demonstrated in their <a href="https://arxiv.org/pdf/1712.09665.pdf">article</a> <em>Adversarial patch</em> (see Figure 4) that it was possible to mislead a neural network training to recognize some images. It may be even worse, because they claim it is possible to structurally mislead all neural networks that were used as image classifiers.</p>
<p>For us, despite the interest of the article, we are only rediscovering the structural mathematical problem of this technique.</p>
<p>See also our <a href="#note1">note on encoder</a>.</p>
<h2 id="new-beliefs-brought-by-the-machine-learning-era">New beliefs brought by the machine learning era</h2>
<p>The machine learning trend brought new beliefs among IT people, some of them being totally the opposite of the usual computer science paradigm. We will list some of them.</p>
<h2 id="belief-1-big-data-and-brute-force-can-solve-mathematical-problems">Belief 1: Big data and brute force can solve mathematical problems</h2>
<p>This a way to reformulate the big data belief: If we inject billions of images to create an image classifier with an artificial neural network, we will overcome the mathematical problem. For sure, to be able to do that, we need an enormous processing power.</p>
<p>We saw that this approach was not working.</p>
<p>Without despising engineering (the author himself is an engineer ;), we see here an engineering reasoning: find all recipes to <em>make it work</em> for the maximum number of cases. Unfortunately, mathematics cannot be overcome by brute force.</p>
<p>Another issue that we see with big data is that no data set is completely neutral. Data sets are coming from somewhere and they have inherent bias attached to them. This is natural when we consider <a href="../../articles/data-interop/">the real nature of data</a>. data is an output product that is the result of the intersection of a semantic field, business rules and use cases.</p>
<h2 id="belief-2-programs-can-make-mistakes-like-humans">Belief 2: Programs can make mistakes like humans</h2>
<p>The second idea (also an engineer's idea) is to say that <em>better have the neural network than nothing</em>. At least, "most of the time", the extrapolation does work and provides relevant results.</p>
<p>That is a fundamental regression compared to the objectives that computer science targeted from the beginning of the discipline. Computers, as they were automating human tasks were trustable because they were exact. In a certain sense, we can delegate tasks to computers <em>provided</em> they make no mistake and we can trust them, better than if the tasks were performed by humans.</p>
<p>We even created the notion of "bug" to name a problem to be fixed in a program, for the computer to to exact, and so reliable.</p>
<p>The argumentation is that, as humans make mistakes, it is acceptable that computers do too...</p>
<p>If computer make errors in a structural way (which is the case of artificial neural networks), we change the complete computer science paradigm.</p>
<p>Computers could be at the source of serious administrative mistakes, medical mistakes, justice mistakes... Even is statistically, those mistakes happen 1% of the time, how to trust computer programs anymore?</p>
<p>If we accept to generalize techniques that make errors, we enter into a non certain world, that looks like the Brazil movie.</p>
<h2 id="belief-3-the-marvelous-ubiquitous-set-of-tools-for-functionally-unskilled-people">Belief 3: The marvelous ubiquitous set of tools for functionally unskilled people</h2>
<p>In the hype of machine learning, another element is crucial. It is possible to learn the data crunching techniques <em>separately</em> from any other functional knowledge, and to pretend being able to apply them to whatever functional domain. In a sense, machine learning is the <em>ubiquitous set of tools</em>.</p>
<p>More: We don't need to be functionally skilled, meaning <em>in the business area where we do data science</em>. Our skills in machine learning will be enough.</p>
<h2 id="belief-4-finding-meaning-in-data-without-knowing-the-business-domain">Belief 4: Finding meaning in data without knowing the business domain</h2>
<p>We tried to explain, in our article <a href="../../articles/data-interop/">the real nature of data</a>, why data was a complex <em>output product</em>, result of the intersection of a semantic domain, some business rules and some use cases.</p>
<p>With machine learning, we take a reverse approach: Data becomes the <em>input</em> and you are suppose to be able to "discover" things from it, with generic tools, without, most of the time realizing those tools have limitations.</p>
<p>In the case where the analyzed data are the fruit of a highly semantically standardized business domain (which is the case in many areas where the regulator or standards structured a particular business domain like accounting, aerospace, travel, etc.), machine learning can "rediscover" elements of this underlying structure, feeding the illusion that it found sense in data.</p>
<p>For sure, the reasoning is bad: Data was, at the beginning, the fruit of a structured and standardized business, and machine learning will just find back some dimensions already present in the data in the first place. Generally, those discovery will established very challenge-able correlations that can be analyzed as relevant or not, only by the ones knowing the business domain.</p>
<h2 id="belief-5-the-average-wins-and-thats-ok">Belief 5: The average wins and that's OK</h2>
<p>If we need billions of data to determine an interpolation function, we will create an interpolation function that will tend to favor the average data. And the fact is, some phenomena are not correctly represented with averages, but more with signal having a certain diversity. The more data, the more the non average data (particular cases, legitimous singular points) will be erased in the resulting interpolation function.</p>
<p>The belief is in the fact that seeing the world through average is OK, whereas most of the time, the diversity is the real nature of the world.</p>
<h2 id="philosophical-aspects-on-science">Philosophical aspects on science</h2>
<p>The scientific method is what built the technical foundations of our world. Science is a method and is aiming at being exact or at mastering the margin of error.</p>
<p>Science always took the same approach:</p>
<ul>
<li>First, we observe the reality as it is;</li>
<li>Second, we try to model what is, with laws and operational models implementing those laws;</li>
<li>Third, we iterate, comparing the laws of our model to the reality and aligning our laws to it. In that process, we are able to measure the differences between our laws and the reality, and put some quantifier in them.</li>
</ul>
<p>Neural networks are a complete perversion of the scientific method because:</p>
<ul>
<li>To create the magic interpolation function, machine learning takes as "reality" a set of data, which are output products as we saw.</li>
<li>There is no intelligent construction of a operational model based on laws. Instead, a generic approach is taken to determine an operative function, without thinking of a law or without understanding why the function should be as it is. In a certain way, the model is "hidden" inside the interpolation function.</li>
<li>The iterative process is not perfecting a model, because this model is hidden. Indeed, if we want to reconsider the model, we have to re-do the full process of training (interpolation function determination) with a bigger data set, data set containing the new samples that probably were badly treated or recognized in the first attempts.</li>
<li>When the model is wrong, there is no real possibility of estimation: How often is it wrong? To what extent? We cannot say. And, as the process of creating the interpolation function is based on data, who knows if introducing new data will not create a completely different interpolation function that does not have the same qualities or the same drawbacks that the one previously generated. In a certain way, the model being data-driven, there is no possibility of enhancing its fundamental laws.</li>
</ul>
<p>So, machine learning is like Canada Dry: It looks like science, it says it is science, it claims many usages in many domains, which is true and really frightening, but machine learning is absolutely not science.</p>
<p>There is no understanding on what's going on at all. There is no progress of science. On the contrary, by taking output products as input products, we can be in incredibly absurd situations.</p>
<p>A good image would be to try to understand how Emacs works on a PC with all the data of all electrical currents in a computer. Data are an output product, and they were generated in a very specific semantic context with very specific business rules.</p>
<p>In a way, machine learning is nearer from the magical thinking than from science.</p>
<h2 id="conclusion">Conclusion</h2>
<p>Machine learning is a technique that must be used with much caution for all the points that we have detailed in this article.</p>
<p>The especially worrying phenomenon is the generalization of machine learning in various areas of our daily life, without us to be aware that some people changed the fundamental paradigm of computer science that tasks can be delegated to machines provided machines do it perfectly. Here, we are delegating tasks to machine that will structurally make mistakes without a lot of people to even realize the change.</p>
<h2 id="the-alternate-model-reasoning-on-semantic-data">The alternate model: Reasoning on semantic data</h2>
<p>Data being output products, they embed a lot of semantic prerequisites, in their structure and in their values. It would be probably much more efficient to work on the semantic modeling of data, to interpret a certain set of data based on their semantic content and then reason on those data with logical paradigms.</p>
<hr />
<h2 id="note-on-image-encoder">Note on image encoder</h2>
<p><a name="note1"></a>One interesting question that we can ask considering the <a href="https://arxiv.org/pdf/1712.09665.pdf">article of Brown et al.</a> is: Is the second image with the sticker in the "neighborhood" of the first one?</p>
<p>We can see, asking this question, that the very notion of "neighborhood" seems not defined in the "space of images". This is normal because we are entering the world of mathematics and function building <em>only after the encoding of the image</em> (see <a href="#Figure3">Figure 3</a>). Indeed, the notion of neighborhood is only defined in the space of (X<sub>i</sub>) which are the results of the encoding of the images. We need to define a mathematical notion of topology in the space of images.</p>
<p>This problem is currently addressed by various techniques of image labelling, decomposing the image in parts.</p>
<p>We see that, in order to make the function more relevant, we tend to add complex pre-processing based on business rules and possibly complex post-processing based also on business rules. The neural network part is decreased at the benefit of more traditional approaches.</p>
<h2 id="bibliography">Bibliography</h2>
<ul>
<li>(1997) An introduction to neural networks - Gurney. <a href="https://www.inf.ed.ac.uk/teaching/courses/nlu/assets/reading/Gurney_et_al.pdf">Link</a>.</li>
<li>(2018) Adversarial Patch - Brown et al. - <a href="https://arxiv.org/pdf/1712.09665.pdf">Link</a>.</li>
<li>(2018) Introduction to representation theory - Etingof et al. <a href="http://www-math.mit.edu/~etingof/repbookcor1.pdf">Link</a>.</li>
<li>(2019) Thesis - Mathematical Analysis of Neural Networks - Leidinger. <a href="http://www-m15.ma.tum.de/foswiki/pub/M15/Allgemeines/PublicationsEN/Thesis_ALeidinger_final.pdf">Link</a>.</li>
<li>(2020) Topology of Deep Neural Networks - Naitzat et al. <a href="http://www.stat.uchicago.edu/~lekheng/work/topdeep.pdf">Link</a>.</li>
<li>(2021) The Representation Theory of Neural Networks - Armanta et al. <a href="https://arxiv.org/pdf/2007.12213.pdf">Link</a>.</li>
</ul>
<p><em>(Last update: August 2021)</em></p></div>
        </div>

        <footer class="col-md-12">
            <hr>
                <p>Copyleft Olivier Rey 2017-2021 - Site licensed under the GNU FDL V3</p>
            <p>Documentation built with <a href="https://www.mkdocs.org/">MkDocs</a>.</p>
        </footer>
        <script>
            var base_url = "../..",
                shortcuts = {"search": 83, "next": 78, "help": 191, "previous": 80};
        </script>
        <script src="../../js/base.js" defer></script>
        <script src="../../search/main.js" defer></script>

        <div class="modal" id="mkdocs_search_modal" tabindex="-1" role="dialog" aria-labelledby="Search Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Search</h4>
            </div>
            <div class="modal-body">
                <p>
                    From here you can search these documents. Enter
                    your search terms below.
                </p>
                <form role="form">
                    <div class="form-group">
                        <input type="text" class="form-control" placeholder="Search..." id="mkdocs-search-query" title="Type search term here">
                    </div>
                </form>
                <div id="mkdocs-search-results"></div>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div><div class="modal" id="mkdocs_keyboard_modal" tabindex="-1" role="dialog" aria-labelledby="Keyboard Shortcuts Modal" aria-hidden="true">
    <div class="modal-dialog">
        <div class="modal-content">
            <div class="modal-header">
                <button type="button" class="close" data-dismiss="modal"><span aria-hidden="true">&times;</span><span class="sr-only">Close</span></button>
                <h4 class="modal-title" id="exampleModalLabel">Keyboard Shortcuts</h4>
            </div>
            <div class="modal-body">
              <table class="table">
                <thead>
                  <tr>
                    <th style="width: 20%;">Keys</th>
                    <th>Action</th>
                  </tr>
                </thead>
                <tbody>
                  <tr>
                    <td class="help shortcut"><kbd>?</kbd></td>
                    <td>Open this help</td>
                  </tr>
                  <tr>
                    <td class="next shortcut"><kbd>n</kbd></td>
                    <td>Next page</td>
                  </tr>
                  <tr>
                    <td class="prev shortcut"><kbd>p</kbd></td>
                    <td>Previous page</td>
                  </tr>
                  <tr>
                    <td class="search shortcut"><kbd>s</kbd></td>
                    <td>Search</td>
                  </tr>
                </tbody>
              </table>
            </div>
            <div class="modal-footer">
            </div>
        </div>
    </div>
</div>

    </body>
</html>
